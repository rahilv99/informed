title,abstract,text,score,authors,url,source_type
Natural Language Understanding and Inference with MLLM in Visual Question Answering: A Survey,"Visual Question Answering (VQA) is a challenge task that combines natural language processing and computer vision techniques and gradually becomes a benchmark test task in multimodal large language models (MLLMs). The goal of our survey is to provide an overview of the development of VQA and a detailed description of the latest models with high timeliness. This survey gives an up-to-date synthesis of natural language understanding of images and text, as well as the knowledge reasoning module based on image-question information on the core VQA tasks. In addition, we elaborate on recent advances in extracting and fusing modal information with vision-language pretraining models and multimodal large language models in VQA. We also exhaustively review the progress of knowledge reasoning in VQA by detailing the extraction of internal knowledge and the introduction of external knowledge. Finally, we present the datasets of VQA and different evaluation metrics and discuss possible directions for future work.","Natural Language Understanding and Inference with MLLM in Visual Question
Answering: A Survey
JIAYI KUANGâˆ—and JINGYOU XIEâˆ—,Sun Yat-sen University, China
HAOHAO LUOâ€ ,Sun Yat-sen University, China
RONGHAO LIâ€ ,Sun Yat-sen University, China
ZHE XUâ€ ,Sun Yat-sen University, China
XIANFENG CHENG, Sun Yat-sen University, China
YINGHUI LI, Tsinghua University, China
XIKA LIN, Department of Computer Science, Worcester Polytechnic Institute, USA
YING SHENâ€¡,Sun Yat-Sen University, China
Visual Question Answering (VQA) is a challenge task that combines natural language processing and computer vision techniques
and gradually becomes a benchmark test task in multimodal large language models (MLLMs). The goal of our survey is to provide
an overview of the development of VQA and a detailed description of the latest models with high timeliness. This survey gives an
up-to-date synthesis of natural language understanding of images and text, as well as the knowledge reasoning module based on
image-question information on the core VQA tasks. In addition, we elaborate on recent advances in extracting and fusing modal
information with vision-language pretraining models and multimodal large language models in VQA. We also exhaustively review the
progress of knowledge reasoning in VQA by detailing the extraction of internal knowledge and the introduction of external knowledge.
Finally, we present the datasets of VQA and different evaluation metrics and discuss possible directions for future work.
CCS Concepts: â€¢Computing methodologies â†’Natural language processing ;Computer vision .
Additional Key Words and Phrases: visual question answering, multimodal representation and reasoning, neural networks
ACM Reference Format:
Jiayi Kuang, Jingyou Xie, Haohao Luo, Ronghao Li, Zhe Xu, Xianfeng Cheng, Yinghui Li, Xika Lin, and Ying Shen. 2018. Natural
Language Understanding and Inference with MLLM in Visual Question Answering: A Survey. 1, 1 (November 2018), 47 pages.
https://doi.org/XXXXXXX.XXXXXXX
âˆ—Both authors contributed equally to this research.
â€ Equal contribution.
â€¡Corresponding author.
Authorsâ€™ addresses: Jiayi Kuang, kuangjy6@mail2.sysu.edu.cn; Jingyou Xie, xiejy73@mail2.sysu.edu.cn, Sun Yat-sen University, Shenzhen, China; Haohao
Luo, luohh5@mail2.sysu.edu.cn, Sun Yat-sen University, Shenzhen, China; Ronghao Li, lirh56@mail2.sysu.edu.cn, Sun Yat-sen University, Shenzhen,
China; Zhe Xu, xuzh226@mail2.sysu.edu.cn, Sun Yat-sen University, Shenzhen, China; Xianfeng Cheng, chengxf6@mail2.sysu.edu.cn, Sun Yat-sen
University, Shenzhen, China; Yinghui Li, liyinghu20@mails.tsinghua.edu.cn, Tsinghua University, Shenzhen, China; Xika Lin, xikalin@gmail.com,
Department of Computer Science, Worcester Polytechnic Institute, Worcester, MA, USA; Ying Shen, sheny76@mail.sysu.edu.cn, Sun Yat-Sen University,
Shenzhen, China.
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not
made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components
of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on
servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org.
Â©2018 Copyright held by the owner/author(s). Publication rights licensed to ACM.
Manuscript submitted to ACM
Manuscript submitted to ACM 1arXiv:2411.17558v1  [cs.CL]  26 Nov 20242 Kuang and Xie, et al.
Image Text Video Audio
Data Understanding Inference
What is the person doing?
A. Eating.      B. Sleeping.
C. Dancing.   D. Singing.Image Encoder Text Encoder Audio EncoderLSTM LSTM
ConcatConcat
Multi -head self -attAdd & NormFeed ForwardAdd & Norm
Image EncoderImage -Text Alignment
Text QueryInstruction 
tuningPrompt In-context 
LearningThink Step by step
Step 1: Think xxxx
Step 2: Consider 
xxxxx
â€¦â€¦
Step n: Check xxxxLLM Control CenterAction
Agent AgentMemory
SPARQL queries
KB query tripletOne-hop Multi -hop
Entity -based Extraction Feature -based ExtractionConventional
SPARQL,
Dense Passage 
Retrieval (DPR),
â€¦â€¦Memory,
Graph,
Implicit,â€¦
GNN -based Transformer Deep learningChain -of-thoughtLLM-aided & Multi -agent
Agent
LLM
Conventional Deep Learning Multimodal Large Language ModelConventional Deep Learning Multimodal Large Language Model
Fig. 1. The data used in VQA tasks, with the conclusion of the understanding and inference methods from conventional models to
multimodal large language models.
1 INTRODUCTION
1.1 What is Visual Question Answering?
Visual Question Answering (VQA) has emerged as a significant task in the intersection of computer vision and natural
language processing (NLP). The goal of VQA is to predict an answer ğ´to a question ğ‘„based on visual information ğ‘‰,
formalized as ğ´=ğ‘“(ğ‘„,ğ‘‰), whereğ‘“represents the model function [ 23]. Visual inputs may include images of landscapes,
people, or videos, and questions can range from multiple-choice to open-ended formats.
A VQA model typically consists of the following steps. First, features are extracted from visual and textual information
respectively. Then, intra-modal and inter-modal representations are learned by aligning and fusing the features. Finally,
the obtained image-question knowledge representation is predicted to complete the question answering task. We
summarize these natural language and image understanding and inference in Figure 1.
The VQA task was first introduced by Agrawal et al. [ 23] alongside the classic VQA v1 dataset, which contains
614,163 manually curated question-answer pairs based on MS COCO images. The earliest VQA tasks focus on the
direct understanding of visual and textual information, mainly using deep learning methods, in which VGG-Net [ 124],
Faster-RCNN [ 346] are used for visual feature extraction, and LSTM [ 23], GRU are used for textual feature extraction.
The introduction of attention mechanisms marked a pivotal advancement in the field. Stacked attention [ 474] and co-
attention [ 275] frameworks significantly improved the fusion of visual and textual features by learning complementary
information between modalities. Additionally, Graph Neural Networks (GNNs) have been increasingly utilized to
capture the complex relationships between visual and textual elements through multimodal heterogeneous graphs [ 87].
Manuscript submitted to ACMNatural Language Understanding and Inference with MLLM in Visual Question Answering: A Survey 3
In recent years, the Transformer architecture [ 419] has further revolutionized VQA. Visual-language pre-training
models such as BLIP [ 241] leverage self-attention and cross-attention mechanisms to enhance multimodal fusion,
enabling significant progress in zero-shot VQA. Emerging multimodal large language models (MLLMs), including
Flamingo [ 14], BLIP-2 [ 242], InternVL [ 62] have demonstrated exceptional performance, particularly in open-ended
and zero-shot question answering scenarios.
While early VQA models focused on direct feature extraction and alignment, recent advances have shifted toward
knowledge-based reasoning, which goes beyond perception to deeper cognitive understanding. Some approaches
explore how to transform fragmented internal knowledge into structured knowledge. Knowledge can be represented as
the form of knowledge triplets and applied to one-hop or multi-hop reasoning [ 87]. Other approaches form a joint image-
question-knowledge representation by introducing an external knowledge base [ 143] or performing passage retrieval
[109]. Then this joint representation is utilized for reasoning. Compared with the previous work, the understanding of
the knowledge representation coming from images and texts at this stage is more in-depth. In addition, Multimodal
large language models now integrate sophisticated reasoning methods such as instruction tuning and Chain-of-Thought
prompting to improve answer accuracy. In this respect, VQA is a comprehensive task that bridges computer vision and
natural language processing (NLP). On the one hand, computer vision aims to teach machines how to see, working on
ways to acquire, process, and understand images. NLP, on the other hand, is a field concerned with enabling interactions
between computers and humans in natural language, which not only aims to teach machines how to read but also
pays attention to the thought process of question answering. It is worth noting that natural language generation
methods play an important role in VQA, since it has a non-negligible role in the answer generation process, especially
contributing to assisting the model to achieve better results in open-ended question answering.
VQAâ€™s impact extends across diverse applications, from aiding visually impaired users in navigating digital and
real-world environments to improving image retrieval, autonomous driving, medical diagnostics, and conversational AI
systems [ 35,385]. The task can also evolve into visual dialogue, where systems must answer questions based on an
entire conversation history, further highlighting its potential in real-world problem-solving.
1.2 Comparison with Other Survey Works
Over the years, Visual Question Answering (VQA) has garnered significant attention in the research community, leading
to the publication of numerous high-quality surveys. These surveys provide valuable insights for both beginners and
experienced researchers by outlining challenges, recent trends, and open problems. Table 1 highlights some of the
prominent generalized surveys in the field.
In 2017, Wu et al. [ 445] provide a foundational review, offering an overall definition of VQA task types and a
comprehensive overview of existing models. However, their survey did not cover more advanced VQA approaches. That
same year, Kafle et al. [ 182] review VQA tasks with a focus on datasets and evaluation metrics, discussing the challenges
and limitations of existing methods. Zhang et al. [ 495] later review information fusion methods in VQA, categorizing
them into two-channel and multi-channel fusion strategies. While this work advanced the understanding of multimodal
fusion, it provides limited insights into other critical components of the task. By 2020, Srivastava et al. [ 392] highlight
the latest applications of deep learning methods in VQA, offering a detailed analysis of model performance. Patel et al.
[324] further contribute by focusing on video-based question answering, emphasizing temporal information processing.
More recently, Faria et al. [ 79] explore the language bias problem in VQA, providing detailed analyses of scene-text
datasets and strategies to address bias. Barra et al. [ 35] and Singh et al. [ 385] offered shorter surveys that summarized
recent advances in neural network-based models, pre-trained language models, and their applications. Additionally,
Manuscript submitted to ACM4 Kuang and Xie, et al.
Table 1. Some of the prominent generalized long surveys along with the published year, topics, challenges and key contributions.
Surveys Year Topic Challenges Contributions
Wu et al. [445] 2017Visual Question Answering,
Knowledge BaseQuestion form is unknown,
Visual and textual comprehension,
Lack of knowledgeFirst comprehensive overview of the field,
Definition and classification of the task,
In-depth analysis of the question/answer pairs
Kafle et al. [182] 2017Image Understanding,
VQA datasetsSolving a wide range of CV tasks,
Dataset bias,
VQA algorithmCompare VQA with other computer vision tasks,
Exploring whether current VQA benchmarks
are suitable for evaluating
Zhang et al. [495] 2019ImageQA and VideoQA,
Information FusionFusion of semantic information of
text and vision,
temporal relationship in VideoQAAbstract fusion framework that can fit the
majority of existing VQA models,
Two-channel fusion and multi-channel fusion,
Clear organization on the proposed fusion techniques
Srivastava et al. [392] 2021Deep learning in VQA,
Robust datasetsReal-life Datasets,
Prior knowledge biasCover major datasets published for validating the
Visual Question Answering task,
Discuss state-of-the-art architectures and compare results
Patel et al. [324] 2021Video Question Answering,
Temporal reasoningCollection of Video-based QA dataset,
Video content must have varied
actionsReview a number of methods and datasets for VideoQA
Faria et al. [79] 2023Language bias for VQA,
Scene TextVQA,
OOD data reasoningGeneralization in VQA,
Zero-shot VQA,
Consolidated VQA benchmarkDiscuss the steps involving VQA task,
Introduce the most recent and significant works comprising
strategies for VQA pipeline
Md.F. Ishmam et al. [167] 2024 vision language pre-trainingencompass traditional VQA and
VLP-based methodsIntroduces a detailed taxonomy to categorize VQA,
Highlights the recent trends, challenges, and scopes for
improvement for more domain such as multimodalQA
Ma et al. [286] 2024Robust VQA,
dataset biasPoor performance in out-of-
distribution dataset of VQAOverview of in-distribution and out-of-distribution datasets,
Typology that presents existing debiasing methods
Ishmam et al. [ 167] presented a taxonomy of vision-language pretraining strategies and their relevance to VQA, while
Ma et al. [286] addressed dataset bias and proposed debiasing methods to enhance VQA robustness.
Compared to previous work, our survey provides an up-to-date overview of VQA development, with a particular
focus on the latest models and their timeliness. We address knowledge reasoning techniques applied in recent years
and the multimodal large language models in few-shot VQA, which have been underexplored in prior surveys.
1.3 Contribution of this Survey
In this paper, we give details of the processing models, relevant datasets and evaluation methods for the VQA task.
We define the framework paradigm and taxonomy of VQA task in Fig. 2, including natural language understanding of
image and text and natural language inference. The main contributions of our paper are as follows:
(1)To give an up-to-date synthesis of VQA paradigm, including natural language understanding of images and
text (perceptual ability), as well as the natural language inference module based on image-question information
(cognitive ability) on the core VQA tasks, as shown in Fig. 2;
(2)To elaborate on the latest progress of visual-language pre-training models, graph neural network models, and
multimodal large language models in VQA for natural language understanding of images and text;
(3)To highlight the progress of natural language inference in VQA by detailing the knowledge reasoning and
multimodal large language model reasoning methods.
The rest of this survey is organized as follows: In Sec. 2, we discuss natural language understanding for images
and text, focusing on improved extraction, embedding, and multimodal fusion methods. Sec. 3 reviews large language
models in zero-shot VQA, introducing three paradigms for their application. In Sec. 4, we explore natural language
inference, including knowledge sources, acquisition, and reasoning processes, along with multimodal large language
Manuscript submitted to ACMNatural Language Understanding and Inference with MLLM in Visual Question Answering: A Survey 5
Answering textual questions 
given flexible form of visual informationVisual Question Answering
Natural Language Understanding of Image and Text Natural Language Inference
VGG -Net, ResNet , 
Faster -RCNN, 
Transformer, CLIP,â€¦Visual FeatureFeature Extraction Fusion Mechanism MMLLM Understanding Knowledge Acquisition
MMLLM ReasoningBOW , LSTM,  CNN,  
Word2Vec,  GRU,  
BERT,â€¦Textual Feature
Improve visual 
embedding, adding 
additional information, 
Improving input 
structure,â€¦Embedding 
ImprovementVector concatenation, 
element -wise addition 
and product,â€¦Vector Operation
Non-Attention Based 
Deep Learning , attention 
mechanism , GNN, â€¦Deep Learning
Single -stream, Dual -
stream, Variants, â€¦Visual -language 
Pretraining ModelsLLMs act as central 
controller, visual 
foundation models, 
memory module,â€¦LLM Aided Visual 
Understanding
Image captioning, 
OCR, In -context 
Learning,â€¦Image to text for 
visual understanding
Training Paradigm of 
Frozen LLM, Image to 
text alignment, 
Techniques,â€¦General Multimodal 
LLMsStructure Knowledge, 
Unstructured 
Knowledge,â€¦Knowledge Sources
Entity -based Extraction, 
Feature -based 
Extraction,â€¦Knowledge Extraction
Prompt Tuning, 
Instruction Tuning, 
Chain of ,â€¦Knowledge 
Reasoning MethodsKnowledge Base triplet, 
SPARQL Queries,â€¦Conventional 
Reasoning
SPARQL Queries, Fact 
scoring, Graph -based, 
Passage Retrieval, â€¦One-hop Reasoning
Memory network; 
factual, visual, semantic 
graph; Implicit 
reasoning, Knowledge 
graph completion,â€¦Multi -hop ReasoningKnowledge Reasoning
Fig. 2. Taxonomy Graph of VQA Task.
model reasoning methods in VQA. Finally, we discuss VQA challenges and propose future research directions in Sec. 6,
concluding in Sec. 7.
2 COMPREHENSION OF IMAGE AND TEXT IN VQA
2.1 Feature Extraction
The majority of VQA models require modal feature extraction prior to answering questions, which can be used for
subsequent multimodal feature fusion to eliminate the gap between modals.
2.1.1 Visual Feature Extraction. For visual feature extraction, on the one hand, convolutional neural networks (CNNs)
(e.g., VGG-Net [ 382], GoogleNet [ 400] and ResNet [ 139]) are often used to extract global image features in early VQA
tasks. VGG-Net [ 382] increases the convolutional layers to 19 and replaces 5Ã—5with 3Ã—3convolutions, reducing
parameters and enhancing nonlinear mapping for improved expressive ability. ResNet [ 139] introduces residual blocks
to mitigate gradient issues in deeper networks, weakening strong connections. On the other hand, Visual Transformer
(ViT) [ 93] applies the Transformer [ 419] framework to extract image features and compute attention map of the image
by attending each pixel to every other pixel. Based on this, CLIP [ 341] shows the strong performance to extract features
on pixel-level, and further becomes a widely-used image encoder to assistant the vision-language pretraining models
and multimodal large language models with pixel-level image understanding ability.
Using global image features to perform VQA task weakens the relationship between tasks and objects in the image,
so numerous models prominent the task-relevant regions by extracting region-based image features. Actually, it is a
spatial-attention mechanism to extract finer-grained features. They first select the regions of interest to the task and
input them into CNNs to extract region features. Specifically, there are two ways to extract region-based features. One is
based on the uniform grid [ 174]. By dividing image into uniformly sampled grids, the region features corresponding to
Manuscript submitted to ACM6 Kuang and Xie, et al.
Table 2. The VQA models with different published year, architecture, and dataset.
Models YearArchitectureDatasetsVisual Feature Textual Feature Fusion Strategy
IBOWIMG 2015 GoogLeNet BoW Vector Concatenation DAQUAR, VQA1.0, MS COCO
ABC-CNN 2015 VGG -Net LSTM Element-Wise Addition, CNN DAQUAR, VQA1.0, COCO-QA
SAN 2016 VGG -Net LSTM Element-Wise Addition, Attention VQA1.0, COCO-QA
Full-CNN 2016 VGG -Net CNN CNN DAQUAR, COCO-QA
AMN 2020 VGG -Net Word2Vec Attention MovieQA
Marioqa 2017 C3D GRU Attention CLEVER, MovieQA
MLB 2016 ResNet GRU Bilinear Pooling Fusion VQA1.0
MCB 2016 ResNet LSTM Bilinear Pooling Fusion VQA1.0, VQA2.0, Visual7W
Pixel-BERT 2020 ResNet BERT Transformer VQA1.0, VQA2.0
SOHO 2021 ResNet BERT Transformer VQA1.0, VQA2.0
LXMERT 2019 Faster-RCNN BERT Cross-Modal Transformer A-OKVQA, GQA, VizWiz
ViLBERT 2019 Faster-RCNN BERT Cross-Modal Transformer A-OKVQA, VQA2.0
Oscar 2020 Faster-RCNN BERT BERT VQA2.0
ConceptBert 2020 Faster-RCNN BERT Transformer A-OKVQA, VQA1.0
MuKEA 2022 Faster-RCNN BERT LXMERT VQA2.0, A-OKVQA, KRVQA
ViLT 2021 Transformer BERT ViT VQA2.0
ALBEF 2021 Transformer BERT Cross-Modal Transformer VQA2.0
(a) Visual Feature Extractor Distribution (b) Textual Feature Extractor Distribution
Fig. 3. Percentage distribution of the usage of visual and textual feature extractors.
each grids can be achieved after inputting them into CNNs. And the relevance weight of each grid feature is determined
by the task. Another way is based on region proposal, which applys object recognition techniques to generate bounding
boxs for the image, then inputs them with their corresponding size and position information into CNNs to extract
region features. Compared with global-based features, this can better identify objects attribute, quantity and location
relationship [17]. The commonly used object recognition techniques is Faster RCNN [346].
For VQA tasks with video input, temporal channels are typical employed for visual feature extraction. Two common
approaches are utilized for this purpose. One is C3D [ 412], which scales convolutional networks to three dimensions to
capture temporal information effectively. Alternatively, optical flow [ 102] can be used to extract video features. This
method analyzes pixel movement and frame-to-frame correlation to capture temporal dynamics. Please refer to Table 2
and Fig. 3 for a summary of various methods for extracting visual features.
2.1.2 Textual Feature Extraction. Another crucial aspect is the extraction of texture features. Classic models for text
encoding include Bag-of-Words (BoW) and Word2Vec [ 298]. With the emergence of Recurrent Neural Networks (RNNs),
Manuscript submitted to ACMNatural Language Understanding and Inference with MLLM in Visual Question Answering: A Survey 7
models like LSTM, Bi-LSTM, and GRU have been extensively utilized for handling sequential data. In addition, CNNs,
originally prominent in computer vision, have also been applied for text feature extraction. Word2Vec, RNN and
CNN are all common models for extracting textual features until the advent of Bert [ 85]. Bert, namely Bi-directional
Encoder Representation from Transformers, is a pre-training model proposed by Google AI in 2018. Bertâ€™s architecture
is based on bidirectional Transformer components, and remarkably, fine-tuning with an additional output layer achieves
outstanding performance across various downstream tasks. Bert stands as a milestone work in the history of NLP.
We give a summary of textual feature extraction models in Table 2, and conclude the visual and textual feature
extrctor distribution in Fig 3 to better show what the most used feature extraction methods are.
2.1.3 Improvement of Embedding. Numerous methods have been introduced for extracting embeddings of different
modalities. To adapt to more complex questions and generate more accurate answers, significant improvements have
been made in feature embedding, including Improving Visual Embedding [ 161,162], Adding Additional Information
[117, 253], and Improving Input Structure [218, 465]. We give the detailed improvement methods in Appendix. A.
2.2 Fusion Mechanism
2.2.1 Simple Vector Operations. Traditional fusion methods typically utilize simple vector operations like element-wise
addition, element-wise product, and vector concatenation to directly operate on visual features ğ‘£ğ¼and text features ğ‘£ğ‘„
for generating joint representations. For element-wise addition and product, ğ‘£ğ¼andğ‘£ğ‘„need to be related in the same
dimension. In cases where they are not, a linear projection [ 306] can be employed to embed these vectors into the same
space using transformation matrices ğ‘Šğ‘£andğ‘Šğ‘. Vector connection splices ğ‘£ğ¼andğ‘£ğ‘„together as fusion vector ğ‘£ğ¹.
In general, the use of element-wise addition does not increase extra computation, and is therefore a common feature
fusion method. In contrast, vector concatenation increases computational complexity significantly.
2.2.2 Deep Learning Method. The fusion of visual and textual features using deep learning can be categorized into
three main approaches: non-attentional deep learning, attention-based deep learning, and graph neural networks.
Non-Attention Based Deep Learning .CNNs and RNNs are commonly utilized for non-attention-based multimodal
fusion. Ren et al. [ 344] propose aligning image features ğ‘£ğ¼with word embeddings, feeding them into an LSTM model
alongside question words ğ‘£ğ‘„ğ‘–. Ma et al. [ 285] leverage memory-augmented neural networks for visual question
answering (VQA), using memory modules to maintain longer-term information.
CNN-based approaches have also been explored. Ma et al. [ 287] proposed an end-to-end framework incorporating
image, sentence, and multimodal CNNs to enhance image-question interplay. Noh et al. [ 314] use a modified VGG-16
for image feature extraction and GRU for text, enabling adaptive parameter prediction. To preserve spatial information,
Gao et al. [ 115] introduce Question-Guided Hybrid Convolution, using question-guided kernels to learn discriminative
multimodal feature representations.
Bilinear pooling is widely employed for fine-grained visual recognition. Fukui et al. [ 107] introduce Multimodal
Compact Bilinear Pooling (MCB) to compress bilinear models, efficiently combining multimodal features. Schwartz et al.
[363] use MCB with attention mechanisms to capture high-order correlations, while Yu et al. [ 484] propose Multimodal
Factorization Bilinear (MFB) pooling for effective fusion, employing sum pooling to reduce dimensionality.
Attention Based Deep Learning .Based on the information in the question, there are parts of the image that are
more relevant to the question, which is also the part of the model that needs more attention. Therefore, the attention
mechanism is introduced into the multimodal fusion mechanism of VQA, and Figure 4 compares the non-attention and
Manuscript submitted to ACM8 Kuang and Xie, et al.
Visual
Feature 
ExtractorTextual
Feature 
Extractor
Input Image Input QuestionFusion Feature
(a) Non-Attention Based Deep Learning (b) Attention Based Deep LearningEncoder  
BlockConcat
LSTMEncoder  
Block
ConcatLSTM LSTM
Visual
Feature 
ExtractorTextual
Feature 
Extractor
Input Image Input Question...ConcatFusion Feature
Fig. 4. Comparison of fusion mechanism utilizing (a) non-attention based deep learning and (b) attention based deep learning.
attention-based deep learning methods. Li et al. [ 250] propose QRU, iteratively updating question representations based
on relevant image regions. Shih et al. [ 379] introduce edge boxes to obtain image regions, and selectively combine
image region features with text features, marking an early instance of attention-based deep learning methods in VQA. It
maps the region image features ğ‘‰=(ğ‘£1,ğ‘£2,...,ğ‘£ğ‘š)and text features ğ‘to a common ğ‘›-dimensional space, then calculates
the inner product between regions and question answers to determine relative weights.
Researches establish loose, global associations between questions and images. Zhu et al. [ 520] propose LSTM-Att to
establish semantic associations between image regions and text descriptions, capturing specific associations between
image-related questions and regions. Attention models have evolved to capture finer-grained associations. However, it
is not enough to only focus on local regions in visual features, and it is equally important to determine which words
need to be focused on in the problem. Lu et al. [ 275] introduce co-attention mechanisms to jointly perform visual and
question-guided attention. The joint attention is calculated as follows:
ğ¶=ğ‘¡ğ‘ğ‘›â„(ğ‘„ğ‘‡ğ‘Šğ‘ğ‘‰), (1)
ğ»ğ‘£=ğ‘¡ğ‘ğ‘›â„(ğ‘Šğ‘£ğ‘‰+(ğ‘Šğ‘ğ‘„)ğ¶), (2)
ğ»ğ‘=ğ‘¡ğ‘ğ‘›â„(ğ‘Šğ‘ğ‘„+(ğ‘Šğ‘£ğ‘‰)ğ¶ğ‘‡), (3)
whereğ‘‰is a visual feature, ğ‘„is a text feature, ğ¶is co-attention, and ğ‘Šis the weight parameter.
Previous approaches mainly exploit low-level features while ignoring the rich semantics contained in high-level
features. Yu et al. [ 484] present a multi-level attention network to reduce the semantic gap and visual attention for
fine-grained spatial reasoning, aligning image regions and questions through feature multiplication for fine-grained
spatial reasoning. To further enhance multimodal fusion, Nguyen et al. [ 311] propose a symmetrical co-attention
mechanism, where each word in the question attends to image regions and vice versa. Wu et al. [ 438] addressed complex
reasoning tasks by introducing a chain of reasoning model, enabling dynamic relational reasoning between objects.
GNN-based Fusion Approaches .Traditional VQA methods often ignore the structural information in images or
questions. Recent approaches leverage Graph Neural Networks (GNNs) for better feature fusion. Norcliffe et al. [ 315]
construct scene graphs conditioned on the question, using a ğ¾-kernel Graph Convolutional Network (GCN) to capture
Manuscript submitted to ACMNatural Language Understanding and Inference with MLLM in Visual Question Answering: A Survey 9
object interactions. Li et al. [ 245] propose Relation-Aware Graph Attention Networks (ReGAT), encoding semantic,
spatial, and implicit relations through attention mechanisms sensitive to node features and positional similarity:
ğ›¼ğ‘–ğ‘—=ğ›¼ğ‘
ğ‘–ğ‘—exp((Uğ‘£â€²
ğ‘–)ğ‘‡Vğ‘£â€²
ğ‘—)
Ã
ğ‘—âˆˆN(ğ‘–)ğ›¼ğ‘
ğ‘–ğ‘—exp((Uğ‘£â€²
ğ‘–)ğ‘‡Vğ‘£â€²
ğ‘—). (4)
For explicit relations, the attention mechanism accounts for edge labels and directions, allowing the model to capture
richer semantic dependencies.
In addition to image graphs, Teney et al. [ 406] construct a question graph based on the token syntactic relations. Sub-
sequently, a GRU-based GNN is deployed to aggregate first-order information, and a cross-modal attention mechanism
aligns textual tokens {ğ‘¥â€²ğ‘„
ğ‘–}ğ‘ğ‘„
ğ‘–=1from the question graph with visual objects {ğ‘¥â€²ğ‘†
ğ‘–}ğ‘ğ‘†
ğ‘–=1of the scene graph as follows:
ğ›¼ğ‘–ğ‘—=ğœ 
ğ‘Š5 
ğ‘¥â€²ğ‘„
ğ‘–
||ğ‘¥â€²ğ‘„
ğ‘–||â—¦ğ‘¥â€²ğ‘†
ğ‘–
||ğ‘¥â€²ğ‘†
ğ‘–||!
+ğ‘5!
. (5)
Instead of using fully-connected graphs to represent images and questions, Huang et al. [ 160] prune edges in the
visual and question graphs based on the object overlapping region and syntactic dependencies respectively. In the
aggregation stage, a dual-channel graph convolutional network simultaneously captures relations between objects in
images and relations among textual tokens in questions.
Transforming questions into instructions to guide scene graph learning has garnered recent attention. Shi et al. [ 376]
propose using NLP tools to parse a given problem into a series of programs, and select different neural modules to
infer the scene graph according to the corresponding programs. Since the instructions represented by the program
set are finite and discontinuous, Hu et al. [ 156] parse questions into several textual instruction embeddings {ğ‘ğ‘¡}ğ‘‡
ğ‘¡=1
which are used to guide the process of message passing. In the procedure of scene graph learning, a GCN conditioned
on instruction embeddings dynamically predicts edge weights ğ‘¤(ğ‘¡)
ğ‘—,ğ‘–to focus on different connections and aggregates
information from neighboring nodes Ëœğ‘¥ğ‘—,ğ‘¡to the target Ëœğ‘¥ğ‘–,ğ‘¡in each iteration. Liang et al. [ 259] regard VQA as an answer
generation task and propose a model LRTA consisting of four stages (Look, Read, Think, Answer). LRTA parses the
problem into instructions using a transformer-based framework and traverses the scene graph using a recursive neural
symbolic execution module that executes one instruction at each inference step.
2.2.3 Vision-Language Pre-training Models. Vision-Language Pre-training models are trained on large-scale unlabeled
data via self-supervision and fine-tuned for specific tasks, allowing for knowledge transfer and improved performance
with minimal labeled data. In multimodal research, methods fall into three categories: Dual-Stream, Single-Stream, and
other variants. Fig. 5 provide a comparison of Dual-Stream and Single-Stream.
Dual-Stream .The dual-stream paradigm in multimodal research involves processing visual and textual inputs
separately before integrating them through a cross-modal fusion module. Exemplifying this approach are ViLBERT
[276] and LXMERT [ 402]. ViLBERT extends the BERT architecture to a multimodal cross-stream model, employing
separate Transformers for visual and textual inputs and integrating them through a co-attention module. Similarly,
LXMERT introduces an architecture to learn the language-vision connection, utilizing object relationship encoders and
pre-training tasks to capture intra-modal and inter-modal relations.
In recent advancements, ERNIE-ViL [ 479] proposes integrating structured knowledge from scene graphs to enhance
semantic alignment. This model leverages the structured knowledge obtained from scene graphs to facilitate fine-grained
semantic understanding. Building upon previous research, Dou et al. [ 94] present the METER model, which refines
Manuscript submitted to ACM10 Kuang and Xie, et al.
(a) Dual-Stream (b) Single-StreamVisual 
FeatureTextual 
FeatureMulti-Head 
AttentionAdd & NormFeed ForwardAdd & NormFusion Feature
Multi-Head 
AttentionAdd & NormFeed ForwardAdd & NormFusion Feature
Multi-Head 
AttentionAdd & NormFeed ForwardAdd & NormFusion Feature
Visual 
FeatureTextual 
FeatureVisual Textual
Fig. 5. Comparison of fusion mechanism utilizing (a) dual-stream pre-trained models and (b) single-stream pre-trained models.
the dual-stream approach by stacking transformer layers with self-attention, co-attention, and feedforward networks.
METER conducts comprehensive experiments across various aspects of general pre-training models, providing insights
into the efficacy of different architectural components.
Single-Stream .Different from Dual-Stream approaches, Single-Stream models integrate textual and visual inputs
for semantic learning. Li et al. [ 235] propose Unicoder-VL, which matches specific text phrases with image features and
inputs them jointly into a multi-layer Transformer for cross-modal representation learning. They introduce a formulation
to combine text and image features, leveraging both region and location information. To mitigate overfitting with limited
target tasks, Su et al. [ 395] train VL-BERT on large-scale image description and plain text datasets simultaneously,
enabling the learning of more general feature representations. Their model employs stacked multi-layer Transformer
modules to adaptively aggregate information from different modalities.
Chen et al. [ 60] propose UNITER, a general image-text representation learning model that conducts fine-grained
semantic alignment between words and image regions. They introduce a novel pre-training task with conditional
masking, enhancing the alignment process. Meanwhile, Kim et al. [ 193] present ViLT, a lightweight model focused on
modal interactions without relying on region or deep convolutional features. Instead, they use a pre-trained Vision
Transformer (ViT) to extract visual features. In Single-Stream models, itâ€™s impractical to encapsulate intra-modal and
vision-language learning in the same Transformer. Xue et al. [ 455] introduce self-attention to the visual domain to
facilitate learning visual modalities. They utilize Swin Transformer [ 272] to obtain visual feature embeddings and
perform visual masking based on internal attention scores from the inter-modal Transformer.
Previous pre-trained models rely heavily on the image feature extraction process (such as Faster R-CNN [ 346]
and ResNet [ 139]), requiring high hardware equipment and time-consuming training. Kim et al. [ 193] propose the
lightweight model ViLT, the main computation of which is concentrated on the modal interactions. ViLT does not use
region features or deep convolutional features but uses a pre-trained ViT model [93] to extract visual features.
Ë†ğ‘§ğ‘‘=ğ‘€ğ‘†ğ´(ğ¿ğ‘(ğ‘§ğ‘‘âˆ’1))+ğ‘§ğ‘‘âˆ’1, (6)
ğ‘§ğ‘‘=ğ‘€ğ¿ğ‘ƒ(ğ¿ğ‘(Ë†ğ‘§ğ‘‘))+Ë†ğ‘§ğ‘‘, (7)
Manuscript submitted to ACMNatural Language Understanding and Inference with MLLM in Visual Question Answering: A Survey 11
whereğ‘§indicates the vision-language embedding, ğ¿ğ‘means LayerNorm, and ğ‘€ğ‘†ğ´ indicates multiheaded self-attention.
Variants and Improvements in Vision-Language Pre-training Approaches .There are some variants and
improvements in training techniques, with the detailed introduction in Appendix. B.
Early methods such as ViLBERT [ 277] employ task-specific networks for vision-language multi-task learning. Lu et
al. introduced a shared backbone with task-specific layers for each task, optimizing model parameters across different
vision-language tasks.To enhance model robustness, adversarial learning was applied to vision-language pre-training
by Gan et al. [ 108], who proposed the VILLA framework. This method introduces adversarial noise into the image and
word embedding spaces, improving model generalization. Contrastive learning is introduced in the multimodal domain
by Li et al. [ 251]. UNIMO leverages a three-stream architecture to process language, visual, and cross-modal fusion
independently, optimizing representations for both single- and multi-modal tasks. The objective function involves
maximizing the similarity between positive pairs of image and text embeddings:
Eğ‘‰,ğ‘Š[âˆ’logexp(ğ‘‘(ğ‘‰+,ğ‘Š+)/ğœ)Ã
Xexp(ğ‘‘(ğ‘‰â€²,ğ‘Šâ€²)/ğœ)]. (8)
Radford et al. [ 341] introduced CLIP, a significant step forward in learning image-text relationships through contrastive
learning. CLIP normalizes word and region embeddings and computes their similarity via dot product, learning robust
representations for zero-shot vision-language tasks, where ğ¼ğ‘’andğ‘‡ğ‘’represent the joint multimodal features:
ğ¼ğ‘’=ğ¿2ğ‘ğ‘œğ‘Ÿğ‘š(ğ¼ğ‘“,ğ‘Šğ‘–), ğ‘‡ğ‘’=ğ¿2ğ‘ğ‘œğ‘Ÿğ‘š(ğ‘‡ğ‘“,ğ‘Šğ‘¡). (9)
Li et al. [ 240] further improved vision-language models with ALBEF, which leverages momentum distillation to
align image-text pairs and combat noisy data, introducing image-text contrastive loss. Recent models like BLIP [ 241]
and PNP-VQA [ 411] have further extended this line of work by integrating multimodal generation and understanding
within a unified framework, demonstrating strong performance in zero-shot VQA tasks.
These vision-language pre-training methods have not only bridged the gap between vision and language under-
standing in VQA but have also become essential for aligning visual and textual information. This alignment serves as a
foundational element for transitioning from purely text-based Large Language Models to multimodal Large Language
Models.
3 COMPREHENSION OF IMAGE AND TEXT WITH LLMS IN ZERO-SHOT VQA
As text large language models (LLMs) have shown amazing performance in multiple textual tasks and attracted great
attention from the whole community, more and more research attempts have been made to explore the introduction of
LLMS into other domains [ 14]. However, LLMs cannot directly process image information, so there is a rising need for
generalized multimodal large language models (MLLMs) to accomplish various multimodal tasks [204].
3.1 LLM Aided Visual Understanding
Since Large Language Models (LLMs) are text-based, early Multimodal Large Language Models (MLLMs) explore
leveraging LLMs as central controllers for multimodal tasks [ 131]. LLMs act as a central controller that (1) analyze
the prompt and history of dialogues, (2) split a complex task into simpler sub-tasks and (3) assign these tasks to
appropriate models. Fig.6 shows the LLM aided visual understanding models. For instance, Microsoft Visual ChatGPT
[441] integrates LLMs with various Visual Foundation Models (VFMs) (e.g., BLIP [ 241], Stable Diffusion, ControlNet),
using a Prompt Manager to manage input/output formats. The LLM analyzes prompts, divides tasks, and invokes VFMs
Manuscript submitted to ACM12 Kuang and Xie, et al.
User Multimodal QueryTextual
LLMHistory of Dialogue
History of ReasoningIterative Reasoning
Visual Foundation Models
Prompt 
Manager
Control CenterOutputs
Input Image Input QuestionUser Multimodal QueryTextual
LLMVisual Foundation Models
Prompt 
Manager
Control CenterOutputs
Input Image Input QuestionIntermediate Answer
(a) LLM Control Center(b) LLM Control Center with Iterative Reasoning
Fig. 6. Two architectures of the LLM aided visual understanding models.
to generate outputs. Additionally, Visual ChatGPT utilizes dialogue history management and iterative reasoning to
invoke further VFMs for more accurate results.
MM-REACT [ 471] focuses on broader visual interpretation by incorporating Azure APIs for tasks like celebrity
recognition and Bing search. This enhances the LLMâ€™s role as a controller for visual understanding and interaction. As
tasks grow more complex, LLMs evolve into decision-makers, as in IdealGPT [ 478], where autonomous agents handle
complex tasks. These agents consist of modules for profiling, memory, planning, and action [ 373], allowing the LLM to
understand dynamic environments and organize responses effectively.
3.2 Image to Text Generation for Visual Understanding
For the issue that textual large language models cannot process images, another inspiration is to transform images
directly into corresponding textual descriptions [ 469], as shown in Fig.7. This typically involves models like Image
Captioning and Optical Character Recognition (OCR) [ 393]. For example, PICA [ 469] uses image captioning to generate
textual descriptions, which are concatenated with questions and fed into the LLM for question answering. To improve
In-context Learning, PICA selects 16 training examples closest to the current test image-question pair using CLIP [ 341].
However, converting images into text can lead to inaccuracies or loss of essential visual details. To address this,
IMG2LLM [ 129] generates more relevant captions and question-answer examples directly from the image. This model
focuses on selecting image regions pertinent to the question and refining the captions for accuracy. It also synthesizes
question-answer pairs to provide more representative in-context prompts. Prophet [ 367] enhances the selection of
in-context examples and the generation of answer heuristics for VQA tasks. Prophet uses a Vanilla VQA [ 174] model to
generate answer candidates, which serve as examples. These answer-aware heuristics, along with the testing samples,
are input into the LLM as prompts, improving VQA performance.
3.3 General Multimodal LLM
LLM-aided visual understanding and image-to-text generation methods demonstrate the ability to leverage pure-
text LLMs for multimodal tasks. However, these approaches do not inherently grant LLMs image comprehension
capabilities. Instead, LLMs either rely on visual foundation models for image processing or convert images into textual
Manuscript submitted to ACMNatural Language Understanding and Inference with MLLM in Visual Question Answering: A Survey 13
Image Image
In-context 
example 1Testing 
questionPrompt 
HeadLLM
Image
In-context 
example 2â€¦â€¦Image
In-context 
example nPrompt Head;  
Context:  [example1:Description, Question, Answer; 
Example2:â€¦â€¦];  
Testing: [Description, Question]
Caption Filter Image Computation
Example SelectorImage -to-Text Generation ModelsContext Schema
Input ImageImageVisual 
Encoder
Cross 
AttentionFeed 
Forward
Cross 
AttentionFeed 
Forward
Text
Input TextText
QueriesContrastive Learning
Frozen LLMTrained Image -to-LLM 
Alignment Module
(a) Architecture of the LLM by Image to Text Understanding. (b) Architecture of the General Multimodal LLM.
Fig. 7. Architecture of the LLM by Image to Text Understanding and General MLLM.
representations. To address this, there is growing interest in developing general-purpose MLLMs with direct image
comprehension abilities by integrating additional modalities into a unified framework, as shown in Fig. 7.
3.3.1 The Training Paradigm of General Multimodal LLM. The training architecture for general multimodal LLMs,
typically using frozen LLMs, follows a two-stage process, as shown in Fig 7. First, the LLM is frozen while external
components, such as visual encoders and alignment modules, are trained to map image features into the textual space
of the LLM [ 426]. In the second stage, the visual components are frozen, and the LLM is fine-tuned with multimodal
data, often using techniques like LoRA or Q-LoRA [ 84]. This method significantly reduces the cost of extending LLMs
to multimodal tasks, with a focus on designing effective image-to-text alignment modules, instead of training a new
multimodal large model. Thus, it plays a pivotal role in the advancement of MLLMs, with the core challenge of designing
more effective Image-to-Text alignment modules.
3.3.2 Image-to-Text Alignment in Multimodal LLM. Flamingo [ 14] introduces a vision encoder and a Perceiver Resampler
to generate a fixed-length feature sequence, integrated with cross-attention layers for improved visual-textual alignment.
PaLM-E [ 96] integrates visual features into the pre-trained PaLM model, leading to robust performance across real-world
tasks, a method adopted by models like LLaVA [ 269] and Shikra [ 56]. However, freezing the LLM during training can
limit alignment effectiveness. BLIP-2 [ 242] addresses this by proposing the Q-Former, using a two-stage pre-training
approach where the visual encoder learns critical visual features through contrastive learning and image-text matching.
This method enhances zero-shot capabilities, though it shows limited in-context learning improvements.
To reduce computational demands, smaller and faster models like PaLI-3 [ 58], based on SigLIP [ 494], offer competitive
performance across multimodal benchmarks while requiring fewer resources.
3.3.3 Advanced Closed and Open-source General MLLMs. As MLLM continues to develop, researchers continue to
expand the data and parameter scale for MLLM pre-training, and continue pre-training, hoping to teach MLLM more
and more new general knowledge and make it more powerful. In this context, the development of MLLM is no longer
based on the improvement of VQA, a downstream task, but more focused on its general capabilities and performance in
multiple tasks in the entire multimodal field. Many commercial models have emerged, such as OpenAIâ€™s GPT series,
Manuscript submitted to ACM14 Kuang and Xie, et al.
Table 3. MLLM models with their base LLM models, MLLM types, Techniques and Performances
Models Year BaseType TechniqueSetupPerformance on VQA
LLM-aid img2txt general mm SFT ICL CoT OK-VQA VQAv2
Viusal ChatGPT [441] 2023 ChatGPT text-davinci-003 ! ! ! - Case study
MM-REACT [471] 2023 gpt-3.5-turbo ! ! ! - Case study
PICa [469] 2022GPT-3 (175B) ! ! few-shot 46.9 -
GPT-3 (175B) ! ! few-shot 48.0 -
Img2LLM [129] 2023OPT-3 (66B) ! ! zero-shot 43.2 60.3
OPT-3 (175B) ! ! zero-shot 45.6 61.9
Prophet [367] 2023 GPT-3 API ! ! few-shot 61.1 -
Flamingo [14] 2022Chinchilla(70B) ! ! zero-shot 50.6 56.3
Chinchilla(71B) ! ! few-shot(4) 57.4 63.1
Chinchilla(72B) ! ! few-shot(32) 57.8 67.6
BLIP-2 [242] 2023OPT(6.7B) ! ! zero-shot 36.4 52.6
FlanT5(XXL) ! ! zero-shot 45.9 65.0
LLaVA-1.5 [269] 2023 Vicuna(7B) ! ! few-shot - 78.5
Qwen-VL-Chat [31] 2023 Qwen(7B) ! ! few-shot 56.6 78.2
mOLUG-owl2 [475] 2023 LLaMA-2(7B) ! ! few-shot 57.7 79.4
InternVL2 [62] 2024 InternLM2-Chat ! ! few-shot - -
including GPT-4v with image and text interleaving processing capabilities [ 61], and GPT-4o frameworks with more
powerful multimodal general capabilities and support for more multimodal inputs [ 168], such as Googleâ€™s Genimi and
Gemini Pro series [405].
In parallel, the open-source community has developed several high-performing models that rival, and in some cases
surpass, these commercial counterparts. The mPLUG series [475], for example, has been praised for its versatile
cross-modal capabilities. mPLUG-OWL excels in open-world vision-language tasks, benefiting from its ability to
handle a wide range of image-text pairs and unstructured data sources. The InternVL series , such as InternVL-2 or
InternLM-XComposer2.5-VL[ 62], distinguishes itself by focusing on improving vision-language alignment through
sophisticated pre-training techniques, allowing for highly effective cross-modal understanding. which has proven
especially effective in sophisticated tasks like math reasoning. Similarly, the LLaVA series [270] has garnered attention
for its innovative approach to integrating large-scale vision-language models. Through a carefully curated combination
of instruction tuning and multimodal dialogue datasets, the LLaVa series has achieved strong performance in both
visual comprehension and interactive reasoning.
3.4 Techniques of the MLLMs
We summarize the recent MLLM models with their base LLM models, MLLM types, Techniques and Performances in
Table 3. There are various techniques that have helped the researchers in generalizing LLM to MLLM, which equip the
LLM with the image understanding capability.
Fine-tuning and Instruction-tuning. Fine-tuning enhances MLLMâ€™s ability to interpret input images and questions
[169,360,433]. Typically, a VQA prompt includes: (1) task background description, (2) input context (image, image
description, OCR), (3) the target question and answer candidates, and (4) reference answers [ 54,129,519]. The prompt
design varies across models. For instance, Visual ChatGPT [ 441] incorporates multiple visual model formats, while
Manuscript submitted to ACMNatural Language Understanding and Inference with MLLM in Visual Question Answering: A Survey 15
domain-specific VQA may include specialized background knowledge or pre-labeled visual prompts for object detection
and segmentation tasks [460].
Fine-tuning with specific datasets helps MLLM perform specialized tasks. For example, general MLLMs struggle with
mathematical reasoning [ 282], so tailored datasets for diagram understanding [ 502], geometry reasoning [ 257], and
handwritten equation recognition [ 326] have been created to improve performance. To avoid losing general capabilities,
fine-tuning often involves both task-specific and general datasets.
In-context Learning. To improve few-shot or zero-shot learning in VQA, researchers leverage in-context learning (ICL),
which uses example contexts to enhance LLM performance [ 91,130,373]. Early ICL prompts were textual, selecting
image-question pairs similar to the target [ 94,283,367], while more recent approaches generate examples directly
from the target image itself [ 129]. Multimodal ICL integrates additional visual context, beyond text, to aid learning
[284,504]. Current research explores two key aspects: (1) improving example selection for better task understanding,
with methods like RAG utilizing retrieval and generation to create more representative examples [ 367,414,469]; and
(2) designing richer multimodal context schemas, such as MMICL, which generates subgraphs of target images with
symbolic correspondences to textual elements [283, 504].
Visual Perception Capability. Early studies directly extend LLM to equip with visual functions and choose to call basic
visual models, such as Object Detection, Image Captioning, and OCR, to provide usable visual information for plain text
LLM [ 367,469]. Further, researchers explored directly giving LLM visual understanding capabilities. By designing a
visual encoder adapted to LLM, the ability to match images and texts is pre-trained in a large amount of image and text
data [ 77,233,242]. The general MLLM achieves pixel-level image understanding based on image encoders such as visual
Transformer. Compared with the previous MLLM based on object-level image understanding, it has better image-text
alignment and fine-grained understanding capabilities [ 270]. Based on this training, MLLM enhances the semantic
segmentation capability of images through better visual annotation [ 441], and combines multiple and larger image
data to train MLLM to understand more diverse images [ 475]. However, since a large amount of training data comes
from natural images in real scenes, MLLM exposes its defects in understanding specific images, such as understanding
document images such as PDF (especially formulas in images) [ 88], understanding related text and symbols in real
scenes [256], and understanding complex charts [493]. More real-scene OCR technologies are being explored.
4 KNOWLEDGE REASONING IN VQA
4.1 Knowldege Sources
External knowledge is indispensable for knowledge-based VQA task whose answers cannot be readily inferred from
images but requires common sense knowledge. We summarize the most widely used knowledge sources for recent
knowledge-based VQA methods, which can be divided into two categories based on the data format, structured
knowledge such as DBpedia [ 27] and ConceptNet [ 271], and unstructured knowledge [ 87,249]. We give detailed
introduction of the knowledge sources in Appendix. C., and conclude the different knowledge source in models in Table
4.
4.2 Knowledge Extraction
4.2.1 Entity-based Extraction. Entity-based methods extract visual or textual concepts from image-question pairs,
using them as anchors for knowledge extraction. For instance, Wu et al. [ 444] extract key attributes from images and
Manuscript submitted to ACM16 Kuang and Xie, et al.
Table 4. Knowledge reasoning models
Models Knowledge SourceKnowledge Reasoning
Methods description
Explicit Knowledge-based[423] DBpedia Conventional SPARQL queries
FVQA [424] WebChild, ConceptNet, DBpedia Conventional KB query triplet
Ask me anything [444] DBpedia One-hop SPARQL queries
Out of the box [308] ConceptNet, DBpedia One&Multi-hop fact scoring and source scoring, Graph-based
Passage Retrieval [338] Wikipedia passage One-hop Dense Passage Retrieval
Transform-Retrieve-Generate [109] Wikipedia passage One-hop Dense Passage Retrieval, generative model
Incorporating external knowledge [236] ConceptNet Multi-hop Memory-based: dynamic memory network
visual knowledge memory [396] Visual Genome Multi-hop Memory-based: key-value structural memory
Inner Knowledge-Based Img2Doc[247] Inner Knowledge Multi-hop Memory-based
Mucko [522] Inner Knowledge Multi-hop Graph-based: factual,visual,semantic graph
context-aware knowledge aggr. [237] Wikipedia Multi-hop Graph-based: semantic graph
See is Knowing [342] Others Multi-hop Implicit Reasoning: ERMLP mode
MuKEA [87] Inner Knowledge VQA2.0 Multi-hop Implicit Reasoning: knowledge graph completion
generate SPARQL queries for DBpedia retrieval. Similarly, Wang et al. [ 423] detect visual concepts (objects, attributes,
scenes) and link them to synonymous entities in DBpedia to construct RDF graphs. To enhance extraction, Su et al.
[396] apply subgraph hashing to match knowledge triplets with question phrases and expand relevant connections.
Recently, Li et al. [ 237] introduced an approach that treats visual and textual concepts as KG anchors, expanding to
include first-order neighbors and scoring nodes to select relevant knowledge.
4.2.2 Feature-based Extraction. Feature-based methods focus on converting knowledge into continuous representations.
Narasimhan et al. [ 307] use LSTMs to extract key relations from questions and calculate affinity scores between the
image-question features and knowledge base facts, selecting the most relevant knowledge. Ziaeefard et al. [ 525] further
develop this by transforming facts into semi-phrases, which are represented using BERT. Ding et al. [ 87] merge
knowledge extraction with image-question feature extraction through a pre-trained visual-linguistic model. Li et al.
[248] view knowledge extraction as graph representation learning, linking images to captions and KGs, and using
Deepwalk to generate knowledge-aware embeddings. More details about the knowledge extraction methods can be
found in Appendix. D.
4.3 Knowledge Reasoning
Knowledge reasoning in knowledge-based VQA involves deriving answers from extracted knowledge [372].
4.3.1 Conventional Reasoning Methods. Conventional methods often employ rule-based or template-based approaches.
Wang et al. [ 423] extract visual concepts from images and parse questions using templates to generate SPARQL queries
for answer reasoning. Wang et al. [ 424] take this further by using LSTMs to parse questions into KB query triplets,
filtering concepts and relations from both the image and the knowledge base, and applying distinct rules based on the
source of the knowledge.
4.3.2 One-Hop Reasoning Methods. Methods based on conventional reasoning are usually unstable under complex
conditions and unscalable to different domains. One-hop reasoning methods utilize deep learning to solve these problems
from first-order knowledge, where latent rules are learned for fact selection in training.
As for structured knowledge, Wu et al. [ 444] proposed an encoder-decoder answer generation architecture. They
first generate SPARQL queries based on an image for KB searching. Then, collected knowledge comment paragraphs
Manuscript submitted to ACMNatural Language Understanding and Inference with MLLM in Visual Question Answering: A Survey 17
are combined together and sent to Doc2Vec [ 221] to get the knowledge feature. Finally, knowledge and image features
are combined with question tokens, which are sequentially fed into an encoder-decoder LSTM framework to get the
answer. Narasimhan et al. [ 308] treat answer reasoning as the combination of fact scoring and source scoring. They first
extract the image feature, visual caption feature, and question feature, and utilize a Multi-Layer Perception (MLP) to fuse
these features into an image-question representation ğ‘”ğ‘€ğ¿ğ‘ƒğ‘¤1(ğ‘¥,ğ‘„). Then, they leverage ğ‘”ğ‘€ğ¿ğ‘ƒğ‘¤1(ğ‘¥,ğ‘„)to score vectorized
external knowledge ğ‘”ğ¹(ğ‘“ğ‘–), and choose the most relevant fact as the candidate answer Ë†ğ‘“:
ğ‘†ğ‘¤1(ğ‘”ğ¹(ğ‘“ğ‘–),ğ‘”ğ‘€ğ¿ğ‘ƒ
ğ‘¤1(ğ‘¥,ğ‘„))=ğ‘ğ‘œğ‘ (ğ‘”ğ¹(ğ‘“ğ‘–),ğ‘”ğ‘€ğ¿ğ‘ƒ
ğ‘¤1(ğ‘¥,ğ‘„))
=ğ‘”ğ¹(ğ‘“ğ‘–)Â·ğ‘”ğ‘€ğ¿ğ‘ƒğ‘¤1(ğ‘¥,ğ‘„)ğ‘”ğ¹(ğ‘“ğ‘–)Â·ğ‘”ğ‘€ğ¿ğ‘ƒğ‘¤1(ğ‘¥,ğ‘„)).(10)
Finally, the problem ğ‘„is sent into an LSTM to predict the answer source Ë†ğ‘ =â„ğ‘ ğ‘¤2(ğ‘„), where Ë†ğ‘ âˆˆ{ğ¼ğ‘šğ‘ğ‘”ğ‘’,ğ¾ğµ}. If
Ë†ğ‘ =ğ¼ğ‘šğ‘ğ‘”ğ‘’ , the head entity of Ë†ğ‘“is taken as the answer, and vice versa.
Qu et al. [ 338] take the idea of Dense Passage Retrieval (DPR) [ 184] and propose a coarse-grained approach for
outside-knowledge-based VQA, that is, finding several passages containing answers as output. They use a BERT to
extract the passage representations of candidate collection before the training process and store them in the memory
slot to reduce redundant computations. To further merge the visual problem information, the given image and question
are simultaneously fed into a pre-trained visual language model LXMERT [ 402] to obtain a query representation. The
dot products of query representation and passage representations are calculated to obtain the top- ğ‘˜passages.
Since coarse-grained passages only provide texts that may contain an answer but not the answer itself, Gao et al.
[109] take candidate passages as their external knowledge, and deploy a generative model for answer reasoning. To
enhance multimodal compatibility, they first utilize the combination of caption text ğ¶ğ‘–, attribute text ğ¿ğ‘–, and optical
character recognition (OCR) text ğ‘‚ğ‘–to represent the given image ğ‘£ğ‘–=(ğ¶ğ‘–,ğ¿ğ‘–,ğ‘‚ğ‘–). Then, each candidate passages ğ‘ğ‘–,ğ‘˜of
top-ğ‘˜collections are concatenated with question context ğ‘„ğ‘–and image context ğ‘£ğ‘–to a Transformer-based encoder to
getğ‘˜hidden layer representations:
zğ‘„ğ‘–=(zğ‘„ğ‘–
1,zğ‘„ğ‘–
2,Â·Â·Â·,zğ‘„ğ‘–
ğ‘˜), (11)
zğ‘„ğ‘–
ğ‘˜=ğ¸ğ‘†ğ‘’ğ‘™ğ‘“ğ´ğ‘¡ğ‘¡ğ‘›(ğ‘„ğ‘–,ğ‘£ğ‘–,ğ‘ğ‘–,ğ‘˜). (12)
Finally, these representations are sent to a decoder to generate the answer, and an auto-regressive cross-entropy loss
is used to train the entire model.
ğ‘ƒ(ğ‘1),...,ğ‘ƒ(ğ‘1)=ğœ(ğ·ğ‘†ğ‘’ğ‘™ğ‘“ğ´ğ‘¡ğ‘¡ğ‘›(zğ‘„ğ‘–)). (13)
4.3.3 Multi-Hop Reasoning Methods. One-hop reasoning methods typically extract shallow knowledge from the KB
and are incapable of exploiting implicit knowledge and handling inter-fact relations. To solve these problems, multi-hop
reasoning has been widely used in recent methods, which refers to performing multi-step inference on the KB to explore
the logical and semantic relations between facts. There are three main branches of multi-hop reasoning: memory-based
reasoning, graph-based reasoning, and implicit reasoning.
Memory-based Reasoning .Memory-based methods treat reasoning as the process of knowledge memory. These
methods iteratively memorize candidate facts to extract relevant knowledge and ignore extraneous knowledge, which
brings the capacity of multi-hop reasoning.
Li et al. [ 236] first filter the extracted knowledge, where facts are scored based on the knowledge graph topology
and the top- ğ‘facts are selected as candidates. Then, the representations of candidate facts are extracted and stored in
Manuscript submitted to ACM18 Kuang and Xie, et al.
memory slots for reading and writing. In the next process, they deploy a dynamic memory network with ğ‘‡iterations for
knowledge accumulation, which consists of an attention component and a memory updating component. The attention
component assigns weights for each knowledge representation ğ‘€ğ‘–:
ğ›¼(ğ‘¡)=ğ‘ ğ‘œğ‘“ğ‘¡ğ‘šğ‘ğ‘¥(wğ‘¡ğ‘ğ‘›â„(W2z(ğ‘¡)
ğ‘–+b2)),
z(ğ‘¡)
ğ‘–=[ğ‘€ğ‘–;ğ‘š(ğ‘¡âˆ’1);q],
q=ğ‘¡ğ‘ğ‘›â„(W1[f(ğ¼);f(ğ‘„);f(ğ´)]+b1),(14)
in which f(ğ¼),f(ğ‘„),f(ğ´)are features of images, questions, and multi-choice answers respectively. The memory updating
component accumulates knowledge based on attention weights to update the memory vector ğ‘š(ğ‘¡):
ğ‘š(ğ‘¡)=ğ‘…ğ¸ğ¿ğ‘ˆ(W3[ğ‘š(ğ‘¡âˆ’1);c(ğ‘¡);q]+b3),
c(ğ‘¡)=ğ‘âˆ‘ï¸
ğ‘–=1ğ›¼(ğ‘¡)ğ‘€ğ‘–,ğ‘¡=1,...,ğ‘‡.(15)
Finally, f(ğ¼),f(ğ‘„),f(ğ´)andğ‘š(ğ‘‡)are fused together to obtain the confidence score for each candidate answer.
Instead of directly considering each fact as an entirety, Su et al. [ 396] use key-value structural memory slots. They
first decompose each knowledge triplet (ğ‘ ,ğ‘Ÿ,ğ‘¡)into three key-value pairs (i.e., (ğ‘ ,ğ‘Ÿ)-ğ‘¡,(ğ‘ ,ğ‘¡)-ğ‘Ÿ,(ğ‘Ÿ,ğ‘¡)-ğ‘ ), which are passed
to the joint embedding module to get the key representation kğ‘–and value representation vğ‘–:
kğ‘–=Î¨(ğ‘’1,ğ‘¢ğ‘–)+Î¨(ğ‘’2,ğ‘¢ğ‘–),vğ‘–=Î¨(ğ‘’3,ğ‘¢ğ‘–), (16)
whereğ‘’1,ğ‘’2,ğ‘’3are different entries of ğ‘–-th triplet, and ğ‘¢ğ‘–is the image feature. Then, a memory network iteratively
refines the memory vector by performing key addressing and value addressing to obtain the answer. Li et al. [ 247] store
all entries of each knowledge triplet in a value slot (i.e., [ğ¹ğ‘ ,ğ¹ğ‘Ÿ,ğ¹ğ‘¡]) and take their average representation as the key
embedding. Then, a memory reading module captures the correlation between query embedding Ë†ğ‘and key-value pairs
(Ë†ğ‘˜ğ‘–-Ë†ğ‘£ğ‘–) and obtains the question-aware knowledge representation ğ‘šğ‘¡which is further used to guide the graph learning:
ğ‘šğ‘¡=ğ‘âˆ‘ï¸
ğ‘–=1ğ‘ğ‘–Ë†ğ‘£ğ‘–,
ğ‘ğ‘–=ğ‘ ğ‘œğ‘“ğ‘¡ğ‘šğ‘ğ‘¥(Ë†ğ‘Â·Ë†ğ‘˜ğ‘‡
ğ‘–),
Ë†ğ‘£ğ‘–=âˆ‘ï¸
Ë†ğ‘£ğ‘–ğ‘—âˆˆ[ğ¹ğ‘ ,ğ¹ğ‘Ÿ,ğ¹ğ‘¡](1âˆ’ğ‘ ğ‘œğ‘“ğ‘¡ğ‘šğ‘ğ‘¥(Ë†ğ‘Â·Ë†ğ‘£ğ‘‡
ğ‘–ğ‘—))Ë†ğ‘£ğ‘–ğ‘—/2.(17)
Graph-based Reasoning .Graph-based methods tend to represent extracted facts as graphs, and then use the
message passing paradigm to aggregate knowledge from multi-hop neighborhoods to the target nodes. This process
enables the model to explicitly exploit the attribute information and relational information embedded in the KB, which
is the most commonly used multi-hop reasoning method for the knowledge-based VQA task.
Narasimhan et al. [ 308] propose a Graph Convolutional Network (GCN) based model for graph-based reasoning,
which mainly consists of two components: the factual graph construction and answer scoring. For the factual graph
construction, facts fetched from the KB are composed into a homogeneous graph, where each node represents an entity,
and each edge represents a relation. For incorporating images and questions information, the representation of each
Manuscript submitted to ACMNatural Language Understanding and Inference with MLLM in Visual Question Answering: A Survey 19
node is formed by the concatenation of image feature ğ‘”ğ‘‰ğ‘¤(ğ¼), question feature ğ‘”ğ‘„
ğ‘¤(ğ‘„)and entity feature ğ‘”ğ¶ğ‘¤(ğ‘’):
ğ»0
ğ‘–=(ğ‘”ğ‘‰
ğ‘¤(ğ¼);ğ‘”ğ‘„
ğ‘¤(ğ‘„);ğ‘”ğ¶
ğ‘¤(ğ‘’)), ğ‘’ğ‘–âˆˆğ¸, (18)
whereğ¸is the entity set. In the process of answer scoring, a GCN integrates node features based on graph topology and
outputs the ğ¿-th layer features Ë†ğ‘”(ğ‘’ğ‘–)=ğ»ğ¿
ğ‘–which are fed into an MLP to predict answer Ë†ğ´:
Ë†ğ´=arg max
ğ‘’ğ‘–âˆˆğ¸ğ‘€ğ¿ğ‘ƒ(Ë†ğ‘”(ğ‘’ğ‘–)). (19)
In addition to using graph structure to represent the structural relationships between facts, Zhu et al. [ 522] further
introduce visual graphs and semantic graphs to comprehensively depict image information. The visual graph is a scene
graph, which is composed of objects in the given image and their positional relations. As for the semantic graph, an
image is first sent to the DenseCap [ 179] to generate several captions, which are parsed into a semantic graph. The
knowledge reasoning of this model comprises two parts: intra-modal knowledge selection and inter-modal knowledge
reasoning. In the first stage, the visual, semantic, and factual graphs are aggregated separately using different GCNs to
obtain the updated node features: {Ë†ğ‘£ğ‘‰
ğ‘–}ğ‘ğ‘‰
ğ‘–=1,{Ë†ğ‘£ğ‘†
ğ‘–}ğ‘ğ‘†
ğ‘–=1,{Ë†ğ‘£ğ¹
ğ‘–}ğ‘ğ¹
ğ‘–=1respectively. In the second stage, the information on the
visual and semantic graphs is mapped to the factual graph (i.e., visual-to-factual and semantic-to-factual). Taking the
semantic-to-factual process as an example:
ğ‘šğ‘†â†’ğ¹
ğ‘–=âˆ‘ï¸
ğ‘—âˆˆğ‘ğ‘‰Î¥ğ‘†â†’ğ¹
ğ‘—ğ‘–Ë†ğ‘£ğ‘†
ğ‘—,
Î¥ğ‘†â†’ğ¹
ğ‘—ğ‘–=ğ‘ ğ‘œğ‘“ğ‘¡ğ‘šğ‘ğ‘¥(ğ‘¤ğ‘ğ‘¡ğ‘ğ‘›â„(ğ‘Š8Ë†ğ‘£ğ‘†
ğ‘—+ğ‘Š9[Ë†ğ‘£ğ¹
ğ‘–,ğ‘])),(20)
in which Î¥ğ‘†â†’ğ¹
ğ‘—ğ‘–is the attention vector between Ë†ğ‘£ğ¹
ğ‘–and each node of semantic graph, and ğ‘šğ‘†â†’ğ¹
ğ‘–is the weighted sum of
nodes on semantic graph for Ë†ğ‘£ğ¹
ğ‘–. Finally,ğ‘šğ‘†â†’ğ¹
ğ‘–,ğ‘šğ‘‰â†’ğ¹
ğ‘–,Ë†ğ‘£ğ¹
ğ‘–are fed into an element-wise gate network to get the final
representation Ëœğ‘£ğ¹
ğ‘–:
Ëœğ‘£ğ¹
ğ‘–=ğ‘Š11(ğ‘”ğ‘ğ‘¡ğ‘’ğ‘–â—¦[ğ‘šğ‘†â†’ğ¹
ğ‘–;ğ‘šğ‘‰â†’ğ¹
ğ‘–;Ë†ğ‘£ğ¹
ğ‘–]),
ğ‘”ğ‘ğ‘¡ğ‘’ğ‘–=ğœ(ğ‘Š10[ğ‘šğ‘†â†’ğ¹
ğ‘–;ğ‘šğ‘‰â†’ğ¹
ğ‘–;Ë†ğ‘£ğ¹
ğ‘–]).(21)
Li et al. [ 237] treat knowledge reasoning as a process of anchor entity feature learning, where anchor entities could
continuously acquire knowledge from the KB and facilitate answer prediction. They first use an object detector and a
natural language parsing tool to extract entities from images, questions and candidate answers as anchor entities. In
the second process, the first-order neighbors of anchor entities are obtained from the KB to form a global knowledge
graph, which is fed into the attention-based GNN for feature aggregation. Then, extracted knowledge is distilled into
three auxiliary features Â®ğ‘’(ğ‘ğ‘¡ğ‘¥),Â®ğ‘¢,Â®ğ‘’(ğ‘ğ‘›ğ‘ ):
Â®ğ‘’(ğ‘ğ‘¡ğ‘¥)=âˆ‘ï¸
ğ‘’ğ‘–âˆˆCğ›¼ğ‘–Â®ğ‘’ğ‘–,Â®ğ‘¢=ğ‘…ğ¸ğ¿ğ‘ˆ(ğ›½Â·ğ‘Š3),Â®ğ‘’(ğ‘ğ‘›ğ‘ )=âˆ‘ï¸
ğ‘’ğ‘—âˆˆAğ›½ğ‘—Â®ğ‘’ğ‘—, (22)
whereğ›¼ğ‘–âˆğ‘’ğ‘¥ğ‘(â„ğ‘Â·Â®ğ‘’ğ‘–), ğ›½ğ‘—âˆğ‘’ğ‘¥ğ‘(Â®ğ‘’(ğ‘ğ‘¡ğ‘¥)Â·Â®ğ‘’ğ‘—), (23)
in whichâ„ğ‘is the query embedding, Cis the anchor entity set extracted from the question and image, and Ais the
anchor entity set extracted from candidate answers. Finally, auxiliary features are fused with image feature Ëœğ‘£ğ‘˜andâ„ğ‘:
Â®ğ‘“=ğµğ‘ğ‘ ğ‘’ğ¹ğ‘¢ğ‘ ğ‘–ğ‘œğ‘›({Ëœğ‘£ğ‘˜};{Â®ğ‘’(ğ‘ğ‘¡ğ‘¥),Â®ğ‘’(ğ‘ğ‘›ğ‘ ),Â®ğ‘¢}âˆª{â„ğ‘
ğ‘š}). (24)
Manuscript submitted to ACM20 Kuang and Xie, et al.
Implicit Reasoning .Different from memory-based or graph-based methods, implicit reasoning treats the multi-hop
reasoning task as an entity feature space learning issue, which aims to map the head, relation, and tail entities into a
common feature space such that they can establish some statistical associations.
Ramnath et al. [ 342] proposed the â€œSee is Knowingâ€ framework, where the visual information is represented as
several knowledge vectors to facilitate answer prediction. They first deploy an ERMLP model [ 92] on the large-scale
KG, which captures the intrinsic connections between entities by learning to identify whether given facts exist. ERMLP
can implicitly perform multi-hop reasoning in learning and generate a dense embedding for each entity. Then, scene,
object, and action concepts are detected in given images and passed to ERMLP to get their knowledge-aware embedding
ğ‘’ğ‘—
ğ‘–,ğ‘—âˆˆ[1,ğ‘š]. Finally, these visual knowledge embeddings and the query embedding ğ´(ğ‘ğ‘–)are passed into an attention
module:
ğ´(ğ¼ğ‘–)=ğ‘šâˆ‘ï¸
ğ‘—=1ğ›¼ğ‘—
ğ¼ğ‘’ğ‘—
ğ‘–,ğ›¼ğ‘—
ğ¼=ğ‘’ğ‘¥ğ‘(ğ‘¤ğ‘‡ğ›¼ğ¼[ğ´(ğ‘ğ‘–);ğ‘’ğ‘—
ğ‘–])
Ãğ‘š
ğ‘˜=1ğ‘’ğ‘¥ğ‘(ğ‘¤ğ‘‡ğ›¼ğ¼[ğ´(ğ‘ğ‘–);ğ‘’ğ‘˜
ğ‘–]), (25)
in whichğ´(ğ¼ğ‘–),ğ´(ğ‘ğ‘–)are further used to query the answer from the KG.
As facts on the KG are typically rigid and incapable of complex scene understanding, Ding et al. [ 87] propose MuKEA,
which extracts and accumulates complex knowledge from VQA scenarios directly. They consider the model learning
process as a multimodal knowledge graph completion problem, i.e., extracting multimodal knowledge triplets from
image-question pairs and training the model taking the idea of TransE [ 39]. During the head entity extraction, a
pre-trained visual-language model LXMERT [ 402] is used to obtain the image embeddings and question embeddings
jointly. These embeddings are fed into a hard attention mechanism, which captures the correlations between each
object and question token to get the head entity. During tail entity extraction, the answer representation is used as the
tail embedding. This implicit reasoning allows the model to continuously acquire multimodal knowledge from the VQA
dataset and fully exploit potential clues between facts.
4.4 MULTIMODAL REASONING WITH LLMs
Recent years have witnessed the remarkable progress of MLLMs, especially in their surprising zero/few-shot reasoning
abilities. In varieties of multimodal reasoning tasks (e.g., VQA), MLLMs often demonstrate impressive effectiveness.
Therefore, MLLMs and VQA task run towards each other at the same time, forging a new direction for VQA research.
Specifically, MLLMs are capable to make great comprehension and integration of information from image and textual
questions after pretraining on large-scale multimodal data, enabling them to reason carefully and generate appropriate
answers. Furthermore, several strategies have recently emerged to enhance the reasoning capabilities of MLLMs and
improve the VQA performance, such as multimodal instruction tuning [ 77], multimodal in-context learning [ 14],
multimodal chain-of-thought [283], and LLM-aided visual reasoning [373].
4.4.1 Multimodal Instruction Tuning. Pretrained MLLMs often struggle with generalizing to novel tasks and aligning
with usersâ€™ intentions, resulting in incorrect and dissatisfactory responses. To address these limitations, a strategy
known as multimodal instruction tuning (M-IT) [ 360] has been introduced. Instruction refers to the task description.
Multimodal instruction tuning is a technique that involves finetuning pretrained MLLMs on a collection of multimodal
instruction-following data. Tuning in this way, MLLMs are guided to understand and adapt to the task of interest, thus
boosting their zero-shot reasoning capabilities and task-specific performance. The success of some notable frameworks
on VQA (e.g., BLIP-2 [242]) validates the effectiveness of this idea.
Manuscript submitted to ACMNatural Language Understanding and Inference with MLLM in Visual Question Answering: A Survey 21
4.4.2 Multimodal In-Context Learning. As the demand for customized MLLMs for specific VQA task continues to grow,
finetuning them by instruction tuning proves to be resource-intensive and may diminish the modelâ€™s generalization
capabilities. Furthermore, state-of-the-art MLLMs like GPT-4V are primarily accessible only through API calls, with
their parametric weights remaining proprietary and unavailable to the public. This scenario underscores the growing
need for a new methodology, multimodal in-context learning (M-ICL), which allow learning from analogy [ 90] without
requiring parametric updates. Specifically in M-ICL, MLLMs learn from a few examples noted as demonstration. The
examples not only provide supplementary contextual knowledge for MLLMs but also exert flexible control over output,
thereby solving complex and unseen tasks in a few-shot manner [ 470]. Researches in M-ICL [ 14,468] are shown
empirically to enhance the reasoning ability of MLLMs on VQA tasks.
4.4.3 Multimodal Chain-of-Thought. In recent studies, chain-of-thought has gained widespread usage in eliciting the
multi-step reasoning abilities of LLMs. Specifically, CoT aims to enable LLMs to imitate the step-by-step thinking
process of humans. It encourages the LLMs to generate not only the final answer but also the intermediate reasoning
chains that lead to the answer by adding a prompt like â€œLetâ€™s think step by stepâ€. Subsequently, to extend CoT reasoning
to multimodality, several works [ 146,350] have been proposed to extend the unimodal CoT to Multimodal CoT (M-CoT).
Given the inputs in different modalities, Multimodal CoT decomposes multi-step problems into intermediate reasoning
steps (rationale) and then infers the answer. The success of multiple researches [ 281,503] validates the effectiveness of
M-CoT in enhancing MLLMs reasoning ability in VQA tasks.
4.4.4 LLM-Aided Visual Reasoning. Building on the achievements of tool-augmented LLMs [ 323], researchers have
explored the potential of invoking external tools and modular approaches for visual reasoning tasks like VQA. Specifically,
vision foundation models [ 440] like image captioning and optical character recognition are usually invoked for better
visual information understanding. Invoking external tools [ 283] such as knowledge retrieval and web search engines
help LLMs access real-time information and leverage domain-specific knowledge from external resources. Multiple
researches [ 373,464] indicate that effective utilization of external tools enables LLMs to accommodate various reasoning
capabilities for accomplishing complex VQA tasks.
4.4.5 Math and Logical Reasoning. MLLMs are increasingly being applied to mathematical and logical reasoning tasks.
The core challenge is that solving math and logic problems requires not only linguistic understanding but also a deeper
ability to perform structured reasoning, abstract thinking, and accurate computation, all of which pose limitations for
traditional language models [282, 448].
To address this, various methods have been explored. One prevalent approach is symbolic manipulation, where
MLLMs are trained to recognize symbolic representations of mathematical expressions (e.g., formulas, diagrams) and
perform algebraic transformations [ 254,446]. These models integrate visual features from images or written equations
with text-based inputs, allowing for more nuanced reasoning in both spatial and symbolic contexts. There are some
attempts that try to improve the visual ability for math diagrams or logical charts [ 502], to explore the potential math
and logical reasoning performance. Another key method is chain of thought reasoning [ 177,220]. In this approach,
models learn to extract symbolic rules from data and apply them within a structured reasoning framework. This
enables them to handle tasks that require step-by-step deduction, such as proofs or multi-step logical arguments. Recent
advancements also involve the multi-agent systems to integrate external symbolic solvers, where MLLMs collaborate
with dedicated math engines (e.g., Wolfram Alpha) to perform complex calculations or proofs, improving accuracy for
higher-level reasoning tasks.
Manuscript submitted to ACM22 Kuang and Xie, et al.
(a) The Nightingale Rose Chart  of the Usage of VQA Dataset
GQA (27%)
DocVQA (7%)
Visual7W (6%)(b) The tendency of the Usage of VQA Dataset 
TextVQA (13%)
VizWiz (9%)VQAv2 (22%)
OKVQA (16%)
Fig. 8. The statistics of the widely-used dataset from 2020 to 2024.
5 DATASETS AND METRICS
5.1 Datasets
In this part, we systematically summarize the most widely used VQA datasets that are divided into two categories: (1)
datasets whose questions are typically based on common sense knowledge (Sec 5.1.1). (2) datasets based on external
knowledge (Sec 5.1.2). The statistics of all datasets are listed in Table 5, and we conclude the seven most widely-used
dataset in Fig. 8. Some representative datasets are selected to be presented here, and the rest are shown in Appendix. E.
5.1.1 Datasets without External Knowledge.
DAQUAR [289].DAQUAR is the first proposed VQA challenge. It is built on the NYU-Depth v2 [ 381] dataset, which
contains 1,449 images and 12,468 Q&A pairs. Its annotation generation methods include synthetic and human. The
synthetic annotation uses eight predefined templates and the original annotation of NYU-Depth v2. Human annotations
come from 5 in-house participants. Although the proposal of DAQUAR is important to VQA, it also has some problems.
For instance, the magnitude of images is too small; the image quality is poor and disorganized; the dataset is unbalanced;
there are too many single-choice questions.
VQAv1 [23].VQAv1 is one of the most widely used datasets, which contains 204,721 real images from the COCO
dataset (123,287 images for training and 81,434 images for testing). It covers 614,163 free-form questions and 7,984,119
answers, allowing yes/no, multiple-choice, and open-ended forms of questions. These questions are collected by humans,
and each question is manually annotated by 10 different people. The annotations also include the answers given by
humans without looking at the images.
VQAv2 [123].VQAv2 is the enhanced version of the VQAv1 dataset, which contains 204,721 images sourced from the
COCO dataset. It has 443,757, 214,354, and 447,793 question annotations on the training set, validation set, and test set,
respectively. VQAv2 has a total of 1,105,904 free-form Q&A pairs annotated by humans, twice as many as VQAv1, and
provides a complementary image for each question so that the same question can be combined with two similar images
Manuscript submitted to ACMNatural Language Understanding and Inference with MLLM in Visual Question Answering: A Survey 23
Table 5. Datasets for VQA and their main characteristics. Avg.Q/Image indicates how many questions a image meanly corresponds
to, and Avg.Q length and Avg.A length denotes the avarage length of question and answer respectively.
DatasetKnowledge
based?Published
yearImage source Images Q&A pairsAvg.Q/
ImgaeAvg.Q
lengthAvg.A
lengthQuestions
Generation
DAQUAR [289] 2015 NYU-Depth V2 1,449 12,468 8.6 11.5 1.2 Human
COCO-QA [345] 2015 COCO 117,684 117,684 1.0 8.6 1.0 Automatic
Visual Madlibs [481] 2015 COCO 10,738 360,001 33.5 4.9 2.8 Human
FM-IQA [110] 2015 COCO 158,392 316,193 2.0 7.4 3.8 Human
VQAv1 [23] 2015 COCO 204,721 614,163 3.0 6.2 1.1 Human
Visual Genome [215] 2016 COCO & YFCC100M 108,077 1,445,322 13.4 5.7 1.8 Human
Visual7W [215] 2016 Visual Genome 47,300 327,939 6.9 6.9 2.0 Human
VQAv2 [123] 2017 COCO 204,721 1,105,904 5.4 6.1 1.2 Human
CLEVR [180] 2017 Synthetic 100,000 999,968 10.0 18.4 1.0 Synthetic
CLEVR-CoGenT-A [180] 2017 Synthetic 100,000 999,951 10.0 - - Synthetic
CLEVR-CoGenT-B [180] 2017 Synthetic 30,000 299,972 10.0 - - Synthetic
VQA-CPv1 [9] 2018 COCO 205,000 370,000 1.8 - - Human
VQA-CPv2 [9] 2018 COCO 219,000 658,000 3.0 - - Human
VizWiz 2018 blind people by phone 72,205 72,205 1.0 - - Human
VQA-Rephrasings [365] 2019 VQAv2 40,504 162,016 4.0 - - Human
GQA [163] 2019 COCO & Flickr 113,018 22,669,678 200.6 - - Synthetic
DocVQA [294] 2021 UCSF Library 12,767 50,000 3.9 9.5 2.4 Human
InfographicVQA [295] 2021 Internet 5,485 30,035 5.5 11.5 1.6 Human
IconQA [280] 2022 digital textbooks 96,817 107,439 1.1 8.4 - Human
PDFVQA [88] 2023 PubMed PDF Doc 25,147 130,700 5.2 - - Automatic
E-VQA [467] 2023 News article 2,690 9,088 3.4 - - Automatic
KB-VQA [423] ! 2015 COCO & ImageNet 700 2,402 3.4 6.8 2.0 Human
FVQA [424] ! 2017 COCO 2,190 5,826 2.7 9.5 1.2 Human
R-VQA [278] ! 2018 Visual Genome 335,000 4,335,966 13.4 - - Human
KVQA [366] ! 2019 Wikidata 24,602 183,007 7.4 10.1 1.6 Human
OK-VQA [292] ! 2019 COCO 14,031 14,055 1.0 8.1 1.3 Human
ViQuAE [228] ! 2022 Wikidata 3,300 3,700 1.1 12.4 - Automatic
KRVQA [50] ! 2022 Visual Genome 32,910 157,201 4.8 11.7 - Automatic
Lora [111] ! 2023 food-and-kitchen 100,000 200,000 2 - - Automatic
to generate different answers. Compared with VQAv1, VQAv2 reduces the bias and imbalance of the dataset through
the above improvements.
CLEVR [180].CLEVR is a synthetic dataset and contains 100,000 rendered images and about 1M synthetic Q&A
pairs where 853,000 questions are totally different. To make the task more challenging, questions are divided into five
categories: querying attributes, comparing attributes, existence, counting, and integer comparison and each image
is represented as a visual scene composed of simple geometric bodies, where a VQA model needs to handle novel
combinations of unseen attributes during training and goes through a long reasoning process to answer the question.
IconQA [280].IconQA consists of 96,817 Icon images and 107,439 Q&A pairs. These Icon images come from IXL
Math Learning, an open-source mathematics textbook on the Internet, and Q&A pairs are obtained by manual collection
and filtering. The questions of this dataset are mainly divided into three subtasks: 57,672 multi-image-choice, 31,578
multi-text-choice, and 18,189 filling-in-the-blank. The questions of IconQA are derived from real-world mathematical
questions, which require commonsense reasoning and arithmetic reasoning.
5.1.2 Datasets with External Knowledge Base.
Manuscript submitted to ACM24 Kuang and Xie, et al.
KB-VQA [423].KB-VQA is the first VQA dataset requiring an external KB, which includes 700 images from the COCO
dataset and 2,402 Q&A pairs. KB-VQA has 23 templates for questions, and each question is proposed by five workers
according to one of the appropriate templates. The proposers assign different labels to questions of different knowledge
levels. Answering questions at the â€œKB-knowledgeâ€ level requires the use of a KB like DBpedia. The â€œKB-knowledgeâ€
level questions in KB-VQA are far more than that of other contemporaneous VQA datasets.
FVQA [424].FVQA has 2,190 images and 5,826 questions which are split into five train/test sets (1,100/1,090 images
and 2,927/2,899 questions for training/testing per set). The questions can be divided into 32 categories in total. Its
annotations include not only Q&A pairs, but also extra knowledge. FVQA builds a KB by collecting knowledge triples
from WebChild, ConceptNet, and DBpedia, which contains 193,449 sentences as supporting facts related to 580 visual
concepts (234 objects, 205 scenes, and 141 attributes). This dataset contains a supporting fact in each Q&A pair.
OK-VQA [292].The Outside Knowledge-VQA (OK-VQA) dataset consists of 14,055 questions (including 12,951 unique
questions) and 14,031 real images from the COCO dataset. The labeling process of OK-VQA is divided into two steps:
first, workers are asked to provide questions that require external knowledge to answer for a given image, and then
five different workers are asked to label answers for each image-text pair. After the annotation is completed, further
filtering is required. If the answer to a question have more than five Q&A instances, the question will be deleted, thereby
ensuring an even distribution of answers and eliminating potential bias.
5.2 VQA Datasets with MLLM Benchmark
With the rapid advancement of multimodal large language models (MLLMs), increasingly sophisticated general-purpose
benchmarks have emerged to evaluate their performance across various dimensions. These benchmarks are designed to
assess a broad range of capabilities, often with a foundation in visual question answering annotation. The creation of such
evaluation data typically involves filtering image data from extensive sources, followed by generating corresponding
question-and-answer (QA) pairs, either through automated processes or manual annotation. Although these benchmarks
are not always explicitly developed to evaluate methodologies specific to the VQA task, they can nonetheless be regarded
as generalized VQA datasets due to their shared emphasis on image-based question answering.
One of the earliest unified MLLM benchmarks, MME [ 104], compiles a substantial collection of images and generates
corresponding QA pairs to evaluate MLLM performance, emphasizing consistency and objectivity. Similar efforts have
been observed in subsequent benchmarks, such as SEEDBENCH [ 231] and SEEDBENCH-2 [ 229], both of which aim
to standardize MLLM evaluation. However, more recent benchmarks have shifted focus towards assessing specific
capabilities, expanding the scope of evaluation to include more specialized dimensions. For example, recent benchmarks
evaluate models on their ability to comprehend visual information [ 47,105,230], demonstrate reasoning skills [ 348,498],
and perform in-context learning [ 267,380]. In addition, some benchmarks address challenges such as hallucination
detection and mitigation [ 75,267], where models are tested for their ability to provide accurate information without
generating false or misleading content.
Moreover, recent developments in MLLM benchmarks have extended their evaluation scope beyond general compre-
hension tasks, venturing into highly specialized domains. These include mathematics, physics, music, medicine, and
more, as seen in datasets such as MathVista [ 282], MMMU [ 488], and CMMMU [ 496]. These domain-specific benchmarks
highlight the growing recognition that MLLMs must be versatile, capable of handling complex, domain-oriented tasks
that require a deep understanding of specialized knowledge. This multidimensional evaluation framework ensures that
MLLMs are not only assessed for their general performance but also their abilities to excel in high-stakes areas.
Manuscript submitted to ACMNatural Language Understanding and Inference with MLLM in Visual Question Answering: A Survey 25
5.3 Evaluation Metrics
Common VQA evaluation metrics can be divided into two categories: objective evaluation metrics and subjective
evaluation metrics, which are often used for two mainstream VQA tasks: open-ended and multiple-choice.
5.3.1 Subjective Evaluation Metrics. The most common human evaluation is to ask human judges to directly evaluate
the quality of the generated answers, either from an overall perspective or from a specific dimension. These specific
dimensions need to be able to reflect the interrelated properties of the answer sentences. For example, Bai et al. [ 32]
evaluate from three dimensions: grammar, faithfulness, and coherence. If evaluating in terms of the entire answer,
human judges are usually asked to select one of three levels of fine-grained evaluation as a score, including 0 (completely
false), 1 (partially true), and 2 (exactly true).
On the FM-IQA dataset, Gao et al. [ 110] propose a Visual Turing Test (VTT) based on human evaluation. In this test,
a lot of sets of images and questions, along with their corresponding human-annotated answers or answers generated
by a VQA model, are presented to human judges. Human judges need to discriminate whether the answers are given by
humans or computers based on the given materials. If an answer of the VQA system is judged to be that of a human,
the answer passes the VTT. Finally, the percentage of answers that pass the VTT is counted, and several additional
VTTs are set to calculate the standard deviation.
5.3.2 Objective Evaluation Metrics.
QA Evaluation Metrics .Simple Accuracy [ 526] based on string matching is first proposed. For the multiple-choice
VQA task, the comparison between the predicted answer and the ground truth is straightforward. However, as generated
answers are mostly phrases consisting of multiple words in the open-ended VQA task, Simple Accuracy is often difficult
to evaluate. On the one hand, indiscriminate judgment is clearly flawed, because the severity of wrong answers varies.
For example, compared with the completely irrelevant answers obtained from prediction, the severity of answers that
contain temporal errors is significantly lower, where the punishment should be different for these situations. On the
other hand, there may be multiple matching answers to the same question, while their appropriateness may vary.
For instance, the correct answer to â€œWhatâ€™s swimming in the water?â€ is â€œbluefishâ€, while â€œscadâ€ means the same as
â€œbluefishâ€, and â€œfishâ€ is also appropriate.
S. Antol et al. [23] propose ğ´ğ‘ğ‘ğ‘¢ğ‘Ÿğ‘ğ‘ğ‘¦ğ‘‰ğ‘„ğ´ , which is used for open-ended evaluation on the VQAv1 dataset:
ğ´ğ‘ğ‘ğ‘¢ğ‘Ÿğ‘ğ‘ğ‘¦ğ‘‰ğ‘„ğ´=min(ğ‘›
3,1), (26)
whereğ‘›is the number of predicted answers that are the same as those given by annotators. This means that the
answer predicted by the algorithm with the same answer as three or more annotations is 100% accurate. However, this
evaluation standard is also unreasonable. For example, the annotators of the COCO-VQA dataset only have consensus
on a few questions, which limits the highest accuracy of the model.
In addition, ğ´ğ‘ğ‘ğ‘¢ğ‘Ÿğ‘ğ‘ğ‘¦ğ‘‰ğ‘„ğ´ is prone to errors on â€œyes/noâ€ type questions. In this type of question, the answer â€œyesâ€ or
â€œnoâ€ may repeat more than three times in one question, which would result in a high score of both â€œyesâ€ and â€œnoâ€ on the
ğ´ğ‘ğ‘ğ‘¢ğ‘Ÿğ‘ğ‘ğ‘¦ğ‘‰ğ‘„ğ´ . Other Metrics are shown in Appendix. F.
Generation Evaluation Metrics .Some of the metrics originally used to evaluate answer generation can also be used
in the open-ended VQA task. The mainstream evaluation metrics among them typically include Bilingual evaluation
understudy (BLEU) [ 322], Recall-Oriented Understudy for Gisting Evaluation (ROUGE) [ 262] and Metric for Evaluation
Manuscript submitted to ACM26 Kuang and Xie, et al.
VQAv2 OK-VQA GQA VizWiz
Question ï¼š
Is this person expecting 
company?
What is just under the tree?Question ï¼š
Whatâ€™s the name of this 
snack?
Question ï¼š
What is the  warmest  outdoor  
temperature  at which  this 
kind of weather  can 
happen?
Question ï¼š
What color is the food on 
the red object left of the 
small girl that is holding a 
hamburger, yellow or 
brown?Imageï¼š Imageï¼š Imageï¼š Imageï¼š
Question ï¼š Question ï¼š Question ï¼š Question ï¼š
Fig. 9. Examples of four widely-used datasets.
of Translation with Explicit Ordering (METEOR) [ 34]. The effectiveness of using these generation metrics for VQA
system evaluation has been confirmed in [ 3,132]. We give a breif introduction of BLEU metric, and the ROUGE and
METEOR are introduced in Appendix. F.
BLEU measures the quality of the answer by comparing the coincidence degree of n-gram phrases of different lengths
in the predicted answer and the real answer. The higher the coincidence degree is, the higher the quality of the answer
is. BLEU calculates the coincidence accuracy of the corresponding answer according to the following formula:
ğ¶ğ‘ƒğ‘›(ğ¶,ğ‘†)=Ã
ğ‘–Ã
ğ‘˜ğ‘šğ‘–ğ‘›(â„ğ‘˜(ğ‘ğ‘–),ğ‘šğ‘ğ‘¥ğ‘—âˆˆğ‘šâ„ğ‘˜(ğ‘ ğ‘–ğ‘—))Ã
ğ‘–Ã
ğ‘˜â„ğ‘˜(ğ‘ğ‘–), (27)
whereğ‘ğ‘–is the candidate answer and its corresponding group of reference answers is ğ‘†ğ‘–={ğ‘ ğ‘–1,ğ‘ ğ‘–2,...,ğ‘ ğ‘–ğ‘š}âˆˆğ‘†,ğœ”ğ‘˜
represents the possible n-grams of the ğ‘˜-th group,â„ğ‘˜(ğ‘ğ‘–)represents the number of occurrences of ğœ”ğ‘˜in the candidate
answerğ‘ğ‘–, andâ„ğ‘˜(ğ‘ ğ‘–ğ‘—)represents the number of occurrences of ğœ”ğ‘˜in the reference answer ğ‘ ğ‘–ğ‘—. BLEU is the weighted
geometric average of n-grams coincidence accuracy, which represents the ratio of the correct matching times of n-grams
to the occurrence times of all n-grams. The calculation formula is as follows:
ğµğ¿ğ¸ğ‘ˆğ‘(ğ¶,ğ‘†)=ğ‘(ğ¶,ğ‘†)ğ‘’ğ‘¥ğ‘(âˆ‘ï¸ğ‘
ğ‘›=1ğœ”ğ‘›ğ‘™ğ‘œğ‘”ğ¶ğ‘ƒğ‘›(ğ¶,ğ‘†)), (28)
where N=1,2,3,4, ğœ”ğ‘˜is generally1
ğ‘›for everyğ‘›.
5.4 Examples and Comparisons of the Performance of Four Widely-used Datasets
To enhance comprehension of the discrete contributions made by diverse datasets and the focal points within the
evaluation framework, we elected to examine four datasets that are widely utilized. An overview of each datasetâ€™s
attributes and illustrative instances of image-related challenges is provided in Fig. 9.
For every dataset, we have synthesized an appraisal of the performances delivered by assorted models, encompassing
the most advanced methodologies, shown in Table. 6. This comparative analysis encompasses outcomes from con-
ventional deep learning architectures, attention mechanism-driven approaches, vision-language pre-training models,
and multimodal large language models. A critical consideration in these evaluations is the variation in experimental
configurations across different models, particularly in scenarios with limited or absent samples, which can lead to
Manuscript submitted to ACMNatural Language Understanding and Inference with MLLM in Visual Question Answering: A Survey 27
Table 6. Comparisons of performance of different models on four widely-used datasets.
Model name Year Dataset VQA Accuruacy Model name Year Dataset VQA Accuruacy
mPLUG-Huge 2022 VQAv2 test-std 83.62 PaLI-X 2023 OK-VQA 66.1
Florence 2021 VQAv2 test-std 80.36 Prophet 2023 OK-VQA 62.5
LLaVa 2023 VQAv2 test-std 78.5(few-shot) Flamingo 2022 OK-VQA 50.6
Flamingo 2022 VQAv2 test-std 67.6(few-shot) PICa 2021 OK-VQA 48.0
BLIP-2 2023 VQAv2 test-std 65(zero-shot) BLIP-2 2023 OK-VQA 45.9
IMG2LLM 2023 VQAv2 test-std 61.9(zero-shot) MuKEA 2022 OK-VQA 42.59
UNITER 2019 VQAv2 test-std 73.4 KRISP 2021 OK-VQA 38.9
LXMERT 2019 VQAv2 test-std 72.5 ConceptBERT 2020 OK-VQA 33.66
VisualBERT 2019 VQAv2 test-std 71.0 LXMERT 2019 OK-VQA 32.04
Up-Down 2017 VQAv2 test-std 70.3 ViLBERT 2019 OK-VQA 31.35
DMN 2018 VQAv2 test-std 68.4 Mucko 2020 OK-VQA 29.2
MUTAN 2017 VQAv2 test-std 67.4 MUTAN 2017 OK-VQA 26.41
PaLI-X 2023 GQA test-dev 67.3 mOLUG-owl2 2023 VizWiz 2020 54.5(zero-shot)
LXMERT 2019 GQA test-dev 60.0 LLaVA 2023 VizWiz 2020 50.0(zero-shot)
BLIP-2 2023 GQA test-dev 44.7(zero-shot) KOSMOS-1 2023 VizWiz 2020 35.3(few-shot)
TRRNet 2023 GQA Test2019 74.03 InstructBLIP 2023 VizWiz 2020 32.08(zero-shot)
VinVL 2023 GQA Test2019 64.85 Flamingo 2022 VizWiz 2020 49.8(few-shot)
BAN 2017 GQA Test2020 57.1 PaLI 2022 VizWiz 2020 73.3
Up-Down 2017 GQA Test2021 49.74 CLIP 2022 VizWiz 2020 61.64
a substantial diminution in performance compared to standard settings. It is our aspiration that these exhaustive
performance comparisons facilitate a deeper understanding among researchers of the disparities between models and
inspire novel investigative endeavors.
6 PROBLEMS AND CHALLENGES
We present problems and challenges that can be further improved with respect to the existing status of VQA.
Visual Reasoning Techniques .As the ability to capture information indirectly presented in images is still underde-
veloped, the current VQA is more of a â€œWhat you see is what you getâ€ task. Knowledge inference techniques can be
applied to this problem to discover more latent knowledge. Also, more common-sense knowledge can be applied to
help machines for image understanding. The current VQA is more of a â€œWhat you see is what you getâ€ task. Currently,
the ability to capture information indirectly presented in images is at a lower level of development.
Robustness (data bias) .VQA models are often trained on datasets that contain inherent biases in both visual and
language modalities. As a result, models tend to rely on statistical correlations rather than genuinely understanding the
relationship between the question and the image content. For example, in some datasets, certain objects or scenes are
more frequently associated with specific answers, leading the model to produce incorrect results when these patterns
are not present. This undermines the modelâ€™s ability to generalize effectively to unseen or real-world data.
Explainability .VQA models often behave as black boxes, providing answers without clear reasoning, which raises
concerns about the trustworthiness of the system. Current methods, such as attention mechanisms, only partially reveal
the decision-making process, often failing to offer human-interpretable explanations. Enhancing the explainability of
VQA systems is crucial for their adoption in sensitive fields, like medical imaging or autonomous vehicles.
Manuscript submitted to ACM28 Kuang and Xie, et al.
Natural Language Generation .Most of the methods choose from a given library of answers and answer questions
through prediction-based methods. However, the answer generation capability has significant limitations. Although
there are researches in open-ended VQA, the ability to generate answers still faces a big challenge.
7 CONCLUSION
Since the VQA task was proposed, it has received great attention and gained rapid development and wide application. This
survey presents a comprehensive review of the state-of-the-art on VQA task from two aspects. For the understanding of
image-question pairs, feature extraction from individual modalities and information fusion methods between modalities
are introduced, with particular attention to the application of visual-language pre-training models, and multimodal large
language models. For the knowledge reasoning of graphical knowledge, we review knowledge sources and describe
methods of acquisition and specific reasoning processes, highlighting differences in the type and difficulty of the models
they include. Datasets of VQA and different evaluation metrics follow. We believe that the suggested possible future
directions will benefit the specific task of VQA as well as the general objective of visual scene understanding.
REFERENCES
[1] . SIGCOMM Comput. Commun. Rev. , 13-14(5-1), 1984. ISSN 0146-4833.
[2]CHI â€™08: CHI â€™08 extended abstracts on Human factors in computing systems , New York, NY, USA, 2008. ACM. ISBN 978-1-60558-012-X. General
Chair-Czerwinski, Mary and General Chair-Lund, Arnie and Program Chair-Tan, Desney.
[3]Asma Ben Abacha, Sadid A Hasan, Vivek V Datla, Joey Liu, Dina Demner-Fushman, and Henning MÃ¼ller. Vqa-med: Overview of the medical visual
question answering task at imageclef 2019. CLEF (Working Notes) , 2(6), 2019.
[4]Mahdi Abavisani, Liwei Wu, Shengli Hu, Joel Tetreault, and Alejandro Jaimes. Multimodal categorization of crisis events in social media. In
Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 14679â€“14689, 2020.
[5]Rafal Ablamowicz and Bertfried Fauser. Clifford: a maple 11 package for clifford algebra computations, version 11, 2007. URL http://math.tntech.
edu/rafal/cliff11/index.html.
[6]Patricia S. Abril and Robert Plant. The patent holderâ€™s dilemma: Buy, sell, or troll? Communications of the ACM , 50(1):36â€“44, January 2007. doi:
10.1145/1188913.1188915. URL http://doi.acm.org/10.1145/1219092.1219093.
[7]A. Adya, P. Bahl, J. Padhye, A.Wolman, and L. Zhou. A multi-radio unification protocol for IEEE 802.11 wireless networks. In Proceedings of the
IEEE 1st International Conference on Broadnets Networks (BroadNetsâ€™04) , pages 210â€“217, Los Alamitos, CA, 2004. IEEE.
[8]Triantafyllos Afouras, Joon Son Chung, and Andrew Zisserman. Lrs3-ted: a large-scale dataset for visual speech recognition. arXiv preprint
arXiv:1809.00496 , 2018.
[9]Aishwarya Agrawal, Dhruv Batra, Devi Parikh, and Aniruddha Kembhavi. Donâ€™t just assume; look and answer: Overcoming priors for visual
question answering. In Proceedings of the IEEE conference on computer vision and pattern recognition , pages 4971â€“4980, 2018.
[10] I. F. Akyildiz, W. Su, Y. Sankarasubramaniam, and E. Cayirci. Wireless sensor networks: A survey. Comm. ACM , 38(4):393â€“422, 2002.
[11] I. F. Akyildiz, T. Melodia, and K. R. Chowdhury. A survey on wireless multimedia sensor networks. Computer Netw. , 51(4):921â€“960, 2007.
[12] Firoj Alam, Ferda Ofli, and Muhammad Imran. Crisismmd: Multimodal twitter datasets from natural disasters. In Twelfth international AAAI
conference on web and social media , 2018.
[13] Harith Alani, Sanghee Kim, David E Millard, Mark J Weal, Wendy Hall, Paul H Lewis, and Nigel R Shadbolt. Automatic ontology-based knowledge
extraction from web documents. IEEE Intelligent Systems , 18(1):14â€“21, 2003.
[14] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm
Reynolds, et al. Flamingo: a visual language model for few-shot learning. Advances in neural information processing systems , 35:23716â€“23736, 2022.
[15] Chris Alberti, Jeffrey Ling, Michael Collins, and David Reitter. Fusion of detected objects in text for visual question answering. In Proceedings of the
2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing
(EMNLP-IJCNLP) , pages 2131â€“2140, 2019.
[16] Using the amsthm Package . American Mathematical Society, April 2015. http://www.ctan.org/pkg/amsthm.
[17] Peter Anderson, Xiaodong He, Chris Buehler, Damien Teney, Mark Johnson, Stephen Gould, and Lei Zhang. Bottom-up and top-down attention for
image captioning and visual question answering. In Proceedings of the IEEE conference on computer vision and pattern recognition , pages 6077â€“6086,
2018.
[18] Sten Andler. Predicate path expressions. In Proceedings of the 6th. ACM SIGACT-SIGPLAN symposium on Principles of Programming Languages ,
POPL â€™79, pages 226â€“236, New York, NY, 1979. ACM Press. doi: 10.1145/567752.567774. URL http://doi.acm.org/10.1145/567752.567774.
Manuscript submitted to ACMNatural Language Understanding and Inference with MLLM in Visual Question Answering: A Survey 29
[19] J. Andreas, M. Rohrbach, T. Darrell, and K. Dan. Deep compositional question answering with neural module networks. Computer Science , 27:
55â€“56, 2015.
[20] Jacob Andreas, Marcus Rohrbach, Trevor Darrell, and Dan Klein. Learning to compose neural networks for question answering. In Proceedings of
the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies , pages 1545â€“1554,
San Diego, California, June 2016. Association for Computational Linguistics. doi: 10.18653/v1/N16-1181. URL https://aclanthology.org/N16-1181.
[21] Jacob Andreas, Marcus Rohrbach, Trevor Darrell, and Dan Klein. Neural module networks. In Proceedings of the IEEE conference on computer vision
and pattern recognition , pages 39â€“48, 2016.
[22] David A. Anisi. Optimal motion control of a ground vehicle. Masterâ€™s thesis, Royal Institute of Technology (KTH), Stockholm, Sweden, 2003.
[23] Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C Lawrence Zitnick, and Devi Parikh. Vqa: Visual question
answering. In Proceedings of the IEEE international conference on computer vision , pages 2425â€“2433, 2015.
[24] Sam Anzaroot and Andrew McCallum. UMass citation field extraction dataset, 2013. URL http://www.iesl.cs.umass.edu/data/data-umasscitationfield.
[25] Sam Anzaroot, Alexandre Passos, David Belanger, and Andrew McCallum. Learning soft linear constraints with application to citation field
extraction, 2014.
[26] J. E. Archer, Jr., R. Conway, and F. B. Schneider. User recovery and reversal in interactive systems. ACM Trans. Program. Lang. Syst. , 6(1):1â€“19,
January 1984.
[27] SÃ¶ren Auer, Christian Bizer, Georgi Kobilarov, Jens Lehmann, Richard Cyganiak, and Zachary Ives. Dbpedia: A nucleus for a web of open data. In
The semantic web , pages 722â€“735. Springer, 2007.
[28] Alexei Baevski, Wei-Ning Hsu, Alexis Conneau, and Michael Auli. Unsupervised speech recognition. Advances in Neural Information Processing
Systems , 34:27826â€“27839, 2021.
[29] AmirAli Bagher Zadeh, Paul Pu Liang, Soujanya Poria, Erik Cambria, and Louis-Philippe Morency. Multimodal language analysis in the wild: CMU-
MOSEI dataset and interpretable dynamic fusion graph. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics
(Volume 1: Long Papers) , pages 2236â€“2246, Melbourne, Australia, July 2018. Association for Computational Linguistics. doi: 10.18653/v1/P18-1208.
URL https://aclanthology.org/P18-1208.
[30] P. Bahl, R. Chancre, and J. Dungeon. SSCH: Slotted seeded channel hopping for capacity improvement in IEEE 802.11 ad-hoc wireless networks. In
Proceeding of the 10th International Conference on Mobile Computing and Networking (MobiComâ€™04) , pages 112â€“117, New York, NY, 2004. ACM.
[31] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: A versatile
vision-language model for understanding, localization, text reading, and beyond. arXiv preprint arXiv:2308.12966 , 2023.
[32] Yang Bai, Ziran Li, Ning Ding, Ying Shen, and Hai-Tao Zheng. Infobox-to-text generation with tree-like planning based attention network. In
Proceedings of the Twenty-Ninth International Conference on International Joint Conferences on Artificial Intelligence , pages 3773â€“3779, 2021.
[33] Tadas BaltruÅ¡aitis, Chaitanya Ahuja, and Louis-Philippe Morency. Multimodal machine learning: A survey and taxonomy. IEEE transactions on
pattern analysis and machine intelligence , 41(2):423â€“443, 2018.
[34] Satanjeev Banerjee and Alon Lavie. Meteor: An automatic metric for mt evaluation with improved correlation with human judgments. In
Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization , pages 65â€“72, 2005.
[35] Silvio Barra, Carmen Bisogni, Maria De Marsico, and Stefano Ricciardi. Visual question answering: Which investigated applications? Pattern
Recogn. Lett. , 151(C):325â€“331, nov 2021. ISSN 0167-8655.
[36] Ilya Bekkerman and Joseph Tabrikian. Target detection and localization using mimo radars and sonars. IEEE Transactions on Signal Processing , 54
(10):3873â€“3883, 2006.
[37] Hedi Ben-Younes, RÃ©mi Cadene, Matthieu Cord, and Nicolas Thome. Mutan: Multimodal tucker fusion for visual question answering. In Proceedings
of the IEEE international conference on computer vision , pages 2612â€“2620, 2017.
[38] Jacob Benesty, Jingdong Chen, Yiteng Huang, and Israel Cohen. Noise reduction in speech processing , volume 2. Springer Science & Business Media,
2009.
[39] Antoine Bordes, Nicolas Usunier, Alberto Garcia-Duran, Jason Weston, and Oksana Yakhnenko. Translating embeddings for modeling multi-
relational data. Advances in neural information processing systems , 26, 2013.
[40] Lutz Bornmann, K. Brad Wray, and Robin Haunschild. Citation concept analysis (CCA)â€”a new form of citation analysis revealing the usefulness of
concepts for other researchers illustrated by two exemplary case studies including classic books by Thomas S. Kuhn and Karl R. Popper, May 2019.
[41] Mic Bowman, Saumya K. Debray, and Larry L. Peterson. Reasoning about naming systems. ACM Trans. Program. Lang. Syst. , 15(5):795â€“825,
November 1993. doi: 10.1145/161468.161471.
[42] Johannes Braams. Babel, a multilingual style-option system for use with latexâ€™s standard document styles. TUGboat , 12(2):291â€“301, June 1991.
[43] Peter F Brown, John Cocke, Stephen A Della Pietra, Vincent J Della Pietra, Frederick Jelinek, John Lafferty, Robert L Mercer, and Paul S Roossin. A
statistical approach to machine translation. Computational linguistics , 16(2):79â€“85, 1990.
[44] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry,
Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey
Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam
McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In Hugo Larochelle, Marcâ€™Aurelio Ranzato,
Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin, editors, Advances in Neural Information Processing Systems 33: Annual Conference on
Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual , 2020.
Manuscript submitted to ACM30 Kuang and Xie, et al.
[45] Jonathan F. Buss, Arnold L. Rosenberg, and Judson D. Knott. Vertex types in book-embeddings. Technical report, Amherst, MA, USA, 1987.
[46] Jonathan F. Buss, Arnold L. Rosenberg, and Judson D. Knott. Vertex types in book-embeddings. Technical report, Amherst, MA, USA, 1987.
[47] Rizhao Cai, Zirui Song, Dayan Guan, Zhenhao Chen, Xing Luo, Chenyu Yi, and Alex Kot. Benchlmm: Benchmarking cross-style visual capability
of large multimodal models. arXiv preprint arXiv:2312.02896 , 2023.
[48] Lee W Campbell and Aaron F Bobick. Recognition of human body motion using phase space constraints. In Proceedings of IEEE international
conference on computer vision , pages 624â€“630. IEEE, 1995.
[49] Jianjian Cao, Xiameng Qin, Sanyuan Zhao, and Jianbing Shen. Bilateral cross-modality graph matching attention for feature fusion in visual
question answering. IEEE Transactions on Neural Networks and Learning Systems , 2022.
[50] Qingxing Cao, Bailin Li, Xiaodan Liang, Keze Wang, and Liang Lin. Knowledge-routed visual question reasoning: Challenges for deep representation
embedding. IEEE Transactions on Neural Networks and Learning Systems , 2021.
[51] W Thomas Cathey and Edward R Dowski. New paradigm for imaging systems. Applied optics , 41(29):6080â€“6092, 2002.
[52] Asli Celikyilmaz, Elizabeth Clark, and Jianfeng Gao. Evaluation of text generation: A survey. arXiv preprint arXiv:2006.14799 , 2020.
[53] Guan-Lin Chao, Abhinav Rastogi, Semih Yavuz, Dilek Hakkani-TÃ¼r, Jindong Chen, and Ian Lane. Learning question-guided video representation
for multi-turn video question answering. arXiv preprint arXiv:1907.13280 , 2019.
[54] Feilong Chen, Minglun Han, Haozhi Zhao, Qingyang Zhang, Jing Shi, Shuang Xu, and Bo Xu. X-llm: Bootstrapping advanced large language
models by treating multi-modalities as foreign languages. arXiv preprint arXiv:2305.04160 , 2023.
[55] Kan Chen, Jiang Wang, Liang-Chieh Chen, Haoyuan Gao, Wei Xu, and Ram Nevatia. Abc-cnn: An attention based convolutional neural network
for visual question answering. arXiv preprint arXiv:1511.05960 , 2015.
[56] Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang, Feng Zhu, and Rui Zhao. Shikra: Unleashing multimodal llmâ€™s referential dialogue magic.
arXiv preprint arXiv:2306.15195 , 2023.
[57] Xi Chen, Xiao Wang, Soravit Changpinyo, AJ Piergiovanni, Piotr Padlewski, Daniel Salz, Sebastian Goodman, Adam Grycner, Basil Mustafa, Lucas
Beyer, et al. Pali: A jointly-scaled multilingual language-image model. arXiv preprint arXiv:2209.06794 , 2022.
[58] Xi Chen, Xiao Wang, Lucas Beyer, Alexander Kolesnikov, Jialin Wu, Paul Voigtlaender, Basil Mustafa, Sebastian Goodman, Ibrahim Alabdulmohsin,
Piotr Padlewski, Daniel Salz, Xi Xiong, Daniel Vlasic, Filip Pavetic, Keran Rong, Tianli Yu, Daniel Keysers, Xiaohua Zhai, and Radu Soricut. Pali-3
vision language models: Smaller, faster, stronger, 2023.
[59] Xiaojun Chen, Shengbin Jia, and Yang Xiang. A review: Knowledge reasoning over knowledge graph. Expert Systems with Applications , 141:112948,
2020.
[60] Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and Jingjing Liu. Uniter: Universal image-text
representation learning. In European conference on computer vision , pages 104â€“120. Springer, 2020.
[61] Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhangwei Gao, Erfei Cui, Wenwen Tong, Kongzhi Hu, Jiapeng Luo, Zheng Ma, et al. How far
are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites. arXiv preprint arXiv:2404.16821 , 2024.
[62] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, et al. Internvl:
Scaling up vision foundation models and aligning for generic visual-linguistic tasks. In Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition , pages 24185â€“24198, 2024.
[63] Zhuo Chen, Jiaoyan Chen, Yuxia Geng, Jeff Z Pan, Zonggang Yuan, and Huajun Chen. Zero-shot visual question answering using knowledge
graph. In International Semantic Web Conference , pages 146â€“162. Springer, 2021.
[64] Heng-Da Cheng, X_ H_ Jiang, Ying Sun, and Jingli Wang. Color image segmentation: advances and prospects. Pattern recognition , 34(12):2259â€“2281,
2001.
[65] Kyunghyun Cho, Bart Van MerriÃ«nboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. Learning
phrase representations using rnn encoder-decoder for statistical machine translation. arXiv preprint arXiv:1406.1078 , 2014.
[66] Jan K Chorowski, Dzmitry Bahdanau, Dmitriy Serdyuk, Kyunghyun Cho, and Yoshua Bengio. Attention-based models for speech recognition.
Advances in neural information processing systems , 28, 2015.
[67] Malcolm Clark. Post congress tristesse. In TeX90 Conference Proceedings , pages 84â€“89. TeX Users Group, March 1991.
[68] Kenneth L. Clarkson. Algorithms for Closest-Point Problems (Computational Geometry) . PhD thesis, Stanford University, Palo Alto, CA, 1985. UMI
Order Number: AAT 8506171.
[69] Kenneth Lee Clarkson. Algorithms for Closest-Point Problems (Computational Geometry) . PhD thesis, Stanford University, Stanford, CA, USA, 1985.
AAT 8506171.
[70] Cohen. Special issue: Digital libraries, November 1996.
[71] Sarah Cohen, Werner Nutt, and Yehoshua Sagic. Deciding equivalances among conjunctive aggregate queries. J. ACM , 54(2), April 2007. doi:
10.1145/1219092.1219093. URL http://doi.acm.org/10.1145/1219092.1219093.
[72] Mauro Conti, Roberto Di Pietro, Luigi V. Mancini, and Alessandro Mei. (old) distributed data source verification in wireless sensor networks. Inf.
Fusion , 10(4):342â€“353, 2009. ISSN 1566-2535. doi: http://dx.doi.org/10.1016/j.inffus.2009.01.002.
[73] Mauro Conti, Roberto Di Pietro, Luigi V. Mancini, and Alessandro Mei. (new) distributed data source verification in wireless sensor networks. Inf.
Fusion , 10(4):342â€“353, October 2009. ISSN 1566-2535. doi: 10.1016/j.inffus.2009.01.002. URL http://portal.acm.org/citation.cfm?id=1555009.1555162.
[74] CROSSBOW. XBOW sensor motes specifications, 2008. http://www.xbow.com.
Manuscript submitted to ACMNatural Language Understanding and Inference with MLLM in Visual Question Answering: A Survey 31
[75] Chenhang Cui, Yiyang Zhou, Xinyu Yang, Shirley Wu, Linjun Zhang, James Zou, and Huaxiu Yao. Holistic analysis of hallucination in gpt-4v
(ision): Bias and interference challenges. arXiv preprint arXiv:2311.03287 , 2023.
[76] D. Culler, D. Estrin, and M. Srivastava. Overview of sensor networks. IEEE Comput. , 37(8 (Special Issue on Sensor Networks)):41â€“49, 2004.
[77] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven C. H. Hoi.
Instructblip: Towards general-purpose vision-language models with instruction tuning. In Alice Oh, Tristan Naumann, Amir Globerson, Kate
Saenko, Moritz Hardt, and Sergey Levine, editors, Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information
Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023 , 2023.
[78] Randall Davis, Howard Shrobe, and Peter Szolovits. What is a knowledge representation? AI magazine , 14(1):17â€“17, 1993.
[79] Ana Claudia Akemi Matsuki de Faria, Felype de Castro Bastos, Jose Victor Nogueira Alves da Silva, Vitor Lopes Fabris, Valeska de Sousa Uchoa,
DÃ©cio GonÃ§alves de Aguiar Neto, and Claudio Filipi Goncalves dos Santos. Visual question answering: A survey on techniques and common trends
in recent literature. CoRR , abs/2305.11033, 2023.
[80] Vincent J Della Pietra. The mathematics of statistical machine translation: Parameter estimation. Using Large Corpora , page 223, 1994.
[81] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on
computer vision and pattern recognition , pages 248â€“255. Ieee, 2009.
[82] Yang Deng, Wai Lam, Yuexiang Xie, Daoyuan Chen, Yaliang Li, Min Yang, and Ying Shen. Joint learning of answer selection and answer summary
generation in community question answering. In Proceedings of the AAAI Conference on Artificial Intelligence , volume 34, pages 7651â€“7658, 2020.
[83] Yang Deng, Wenxuan Zhang, Yaliang Li, Min Yang, Wai Lam, and Ying Shen. Bridging hierarchical and sequential context modeling for question-
driven extractive answer summarization. In Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information
Retrieval , pages 1693â€“1696, 2020.
[84] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. Qlora: Efficient finetuning of quantized llms. In A. Oh, T. Neumann,
A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, Advances in Neural Information Processing Systems , volume 36, pages 10088â€“10115.
Curran Associates, Inc., 2023.
[85] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding.
arXiv preprint arXiv:1810.04805 , 2018.
[86] E. Dijkstra. Go to statement considered harmful. In Classics in software engineering (incoll) , pages 27â€“33. Yourdon Press, Upper Saddle River, NJ,
USA, 1979. ISBN 0-917072-14-6. URL http://portal.acm.org/citation.cfm?id=1241515.1241518.
[87] Yang Ding, Jing Yu, Bang Liu, Yue Hu, Mingxin Cui, and Qi Wu. Mukea: Multimodal knowledge extraction and accumulation for knowledge-based
visual question answering. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 5089â€“5098, 2022.
[88] Yihao Ding, Siwen Luo, Hyunsuk Chung, and Soyeon Caren Han. Vqa: A new dataset for real-world vqa on pdf documents. In Joint European
Conference on Machine Learning and Knowledge Discovery in Databases , pages 585â€“601. Springer, 2023.
[89] Chenhe Dong, Yinghui Li, Haifan Gong, Miaoxin Chen, Junxin Li, Ying Shen, and Min Yang. A survey of natural language generation. ACM
Comput. Surv. , 55(8), dec 2022. ISSN 0360-0300. doi: 10.1145/3554727. URL https://doi.org/10.1145/3554727.
[90] Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, Lei Li, and Zhifang Sui. A survey for in-context
learning. CoRR .
[91] Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, Lei Li, and Zhifang Sui. A survey for in-context
learning. CoRR , abs/2301.00234, 2023.
[92] Xin Dong, Evgeniy Gabrilovich, Geremy Heitz, Wilko Horn, Ni Lao, Kevin Murphy, Thomas Strohmann, Shaohua Sun, and Wei Zhang. Knowledge
vault: A web-scale approach to probabilistic knowledge fusion. In Proceedings of the 20th ACM SIGKDD international conference on Knowledge
discovery and data mining , pages 601â€“610, 2014.
[93] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias
Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint
arXiv:2010.11929 , 2020.
[94] Zi-Yi Dou, Yichong Xu, Zhe Gan, Jianfeng Wang, Shuohang Wang, Lijuan Wang, Chenguang Zhu, Pengchuan Zhang, Lu Yuan, Nanyun Peng, et al.
An empirical study of training end-to-end vision-and-language transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition , pages 18166â€“18176, 2022.
[95] Bruce P. Douglass, David Harel, and Mark B. Trakhtenbrot. Statecarts in use: structured analysis and object-orientation. In Grzegorz Rozenberg
and Frits W. Vaandrager, editors, Lectures on Embedded Systems , volume 1494 of Lecture Notes in Computer Science , pages 368â€“394. Springer-Verlag,
London, 1998. doi: 10.1007/3-540-65193-4_29. URL http://dx.doi.org/10.1007/3-540-65193-4_29.
[96] Danny Driess, Fei Xia, Mehdi S. M. Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong,
Tianhe Yu, Wenlong Huang, Yevgen Chebotar, Pierre Sermanet, Daniel Duckworth, Sergey Levine, Vincent Vanhoucke, Karol Hausman, Marc
Toussaint, Klaus Greff, Andy Zeng, Igor Mordatch, and Pete Florence. Palm-e: An embodied multimodal language model. In Andreas Krause,
Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, International Conference on Machine Learning,
ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA , volume 202 of Proceedings of Machine Learning Research , pages 8469â€“8488. PMLR, 2023.
[97] D. D. Dunlop and V. R. Basili. Generalizing specifications for uniformly implemented loops. ACM Trans. Program. Lang. Syst. , 7(1):137â€“158, January
1985.
Manuscript submitted to ACM32 Kuang and Xie, et al.
[98] Ian Editor, editor. The title of book one , volume 9 of The name of the series one . University of Chicago Press, Chicago, 1st. edition, 2007. doi:
10.1007/3-540-09237-4. URL http://dx.doi.org/10.1007/3-540-09456-9.
[99] Ian Editor, editor. The title of book two , chapter 100. The name of the series two. University of Chicago Press, Chicago, 2nd. edition, 2008. doi:
10.1007/3-540-09237-4. URL http://dx.doi.org/10.1007/3-540-09456-9.
[100] Moataz El Ayadi, Mohamed S Kamel, and Fakhri Karray. Survey on speech emotion recognition: Features, classification schemes, and databases.
Pattern recognition , 44(3):572â€“587, 2011.
[101] Ali Farhadi, Mohsen Hejrati, Mohammad Amin Sadeghi, Peter Young, Cyrus Rashtchian, Julia Hockenmaier, and David Forsyth. Every picture tells
a story: Generating sentences from images. In European conference on computer vision , pages 15â€“29. Springer, 2010.
[102] Gunnar FarnebÃ¤ck. Two-frame motion estimation based on polynomial expansion. In Scandinavian conference on Image analysis , pages 363â€“370.
Springer, 2003.
[103] Simon Fear. Publication quality tables in L ATEX, April 2005. http://www.ctan.org/pkg/booktabs.
[104] Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Jinrui Yang, Xiawu Zheng, Ke Li, Xing Sun, Yunsheng Wu, and
Rongrong Ji. Mme: A comprehensive evaluation benchmark for multimodal large language models. arXiv preprint arXiv:2306.13394 , 2023.
[105] Chaoyou Fu, Renrui Zhang, Haojia Lin, Zihan Wang, Timin Gao, Yongdong Luo, Yubo Huang, Zhengye Zhang, Longtian Qiu, Gaoxiang Ye, et al. A
challenger to gpt-4v? early explorations of gemini in visual expertise. arXiv preprint arXiv:2312.12436 , 2023.
[106] Akira Fukui, Dong Huk Park, Daylen Yang, Anna Rohrbach, Trevor Darrell, and Marcus Rohrbach. Multimodal compact bilinear pooling for visual
question answering and visual grounding. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing , pages 457â€“468,
Austin, Texas, November 2016. Association for Computational Linguistics. doi: 10.18653/v1/D16-1044. URL https://aclanthology.org/D16-1044.
[107] Akira Fukui, Dong Huk Park, Daylen Yang, Anna Rohrbach, Trevor Darrell, and Marcus Rohrbach. Multimodal compact bilinear pooling for visual
question answering and visual grounding. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing , pages 457â€“468,
Austin, Texas, November 2016. Association for Computational Linguistics. doi: 10.18653/v1/D16-1044. URL https://aclanthology.org/D16-1044.
[108] Zhe Gan, Yen-Chun Chen, Linjie Li, Chen Zhu, Yu Cheng, and Jingjing Liu. Large-scale adversarial training for vision-and-language representation
learning. Advances in Neural Information Processing Systems , 33:6616â€“6628, 2020.
[109] Feng Gao, Qing Ping, Govind Thattai, Aishwarya Reganti, Ying Nian Wu, and Prem Natarajan. Transform-retrieve-generate: Natural language-
centric outside-knowledge visual question answering. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages
5067â€“5077, 2022.
[110] Haoyuan Gao, Junhua Mao, Jie Zhou, Zhiheng Huang, Lei Wang, and Wei Xu. Are you talking to a machine? dataset and methods for multilingual
image question. Advances in neural information processing systems , 28, 2015.
[111] Jingying Gao, Qi Wu, Alan Blair, and Maurice Pagnucco. Lora: A logical reasoning augmented dataset for visual question answering. Advances in
Neural Information Processing Systems , 36, 2024.
[112] Jiyang Gao, Runzhou Ge, Kan Chen, and Ram Nevatia. Motion-appearance co-memory networks for video question answering. In Proceedings of
the IEEE Conference on Computer Vision and Pattern Recognition , pages 6576â€“6585, 2018.
[113] Lianli Gao, Pengpeng Zeng, Jingkuan Song, Xianglong Liu, and Heng Tao Shen. Examine before you answer: Multi-task learning with adaptive-
attentions for multiple-choice vqa. In Proceedings of the 26th ACM international conference on Multimedia , pages 1742â€“1750, 2018.
[114] Lianli Gao, Pengpeng Zeng, Jingkuan Song, Yuan-Fang Li, Wu Liu, Tao Mei, and Heng Tao Shen. Structured two-stream attention network for
video question answering. In Proceedings of the AAAI Conference on Artificial Intelligence , volume 33, pages 6391â€“6398, 2019.
[115] Peng Gao, Hongsheng Li, Shuang Li, Pan Lu, Yikang Li, Steven CH Hoi, and Xiaogang Wang. Question-guided hybrid convolution for visual
question answering. In Proceedings of the European Conference on Computer Vision (ECCV) , pages 469â€“485, 2018.
[116] Peng Gao, Zhengkai Jiang, Haoxuan You, Pan Lu, Steven CH Hoi, Xiaogang Wang, and Hongsheng Li. Dynamic fusion with intra-and inter-modality
attention flow for visual question answering. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages 6639â€“6648,
2019.
[117] FranÃ§ois GardÃ¨res, Maryam Ziaeefard, Baptiste Abeloos, and Freddy Lecue. Conceptbert: Concept-aware representation for visual question
answering. In Findings of the Association for Computational Linguistics: EMNLP 2020 , pages 489â€“498, 2020.
[118] Dan Geiger and Christopher Meek. Structured variational inference procedures and their realizations (as incol). In Proceedings of Tenth International
Workshop on Artificial Intelligence and Statistics, The Barbados. The Society for Artificial Intelligence and Statistics, January 2005.
[119] Michael Gerndt. Automatic Parallelization for Distributed-Memory Multiprocessing Systems . PhD thesis, University of Bonn, Bonn, Germany,
December 1989.
[120] Sharon Goldwater, Thomas L Griffiths, and Mark Johnson. A bayesian framework for word segmentation: Exploring the effects of context. Cognition ,
112(1):21â€“54, 2009.
[121] Tao Gong, Chengqi Lyu, Shilong Zhang, Yudong Wang, Miao Zheng, Qian Zhao, Kuikun Liu, Wenwei Zhang, Ping Luo, and Kai Chen. Multimodal-
gpt: A vision and language model for dialogue with humans. CoRR , abs/2305.04790, 2023.
[122] Michel Goossens, S. P. Rahtz, Ross Moore, and Robert S. Sutor. The Latex Web Companion: Integrating TEX, HTML, and XML . Addison-Wesley
Longman Publishing Co., Inc., Boston, MA, USA, 1st edition, 1999. ISBN 0201433117.
[123] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the v in vqa matter: Elevating the role of image understanding
in visual question answering. In Proceedings of the IEEE conference on computer vision and pattern recognition , pages 6904â€“6913, 2017.
[124] Alex Graves. Long short-term memory. Supervised sequence labelling with recurrent neural networks , pages 37â€“45, 2012.
Manuscript submitted to ACMNatural Language Understanding and Inference with MLLM in Visual Question Answering: A Survey 33
[125] Barbara Jean Grosz. The representation and use of focus in dialogue understanding. University of California, Berkeley, 1977.
[126] Matthew Van Gundy, Davide Balzarotti, and Giovanni Vigna. Catch me, if you can: Evading network signatures with web-based polymorphic
worms. In Proceedings of the first USENIX workshop on Offensive Technologies , WOOT â€™07, Berkley, CA, 2007. USENIX Association.
[127] Matthew Van Gundy, Davide Balzarotti, and Giovanni Vigna. Catch me, if you can: Evading network signatures with web-based polymorphic
worms. In Proceedings of the first USENIX workshop on Offensive Technologies , WOOT â€™08, pages 99â€“100, Berkley, CA, 2008. USENIX Association.
[128] Matthew Van Gundy, Davide Balzarotti, and Giovanni Vigna. Catch me, if you can: Evading network signatures with web-based polymorphic
worms. In Proceedings of the first USENIX workshop on Offensive Technologies , WOOT â€™09, pages 90â€“100, Berkley, CA, 2009. USENIX Association.
[129] Jiaxian Guo, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Boyang Li, Dacheng Tao, and Steven Hoi. From images to textual prompts:
Zero-shot visual question answering with frozen large language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 10867â€“10877, 2023.
[130] Tanmay Gupta and Aniruddha Kembhavi. Visual programming: Compositional visual reasoning without training. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition (CVPR) , pages 14953â€“14962, June 2023.
[131] Tanmay Gupta and Aniruddha Kembhavi. Visual programming: Compositional visual reasoning without training. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition , pages 14953â€“14962, 2023.
[132] Danna Gurari, Qing Li, Abigale J Stangl, Anhong Guo, Chi Lin, Kristen Grauman, Jiebo Luo, and Jeffrey P Bigham. Vizwiz grand challenge:
Answering visual questions from blind people. In Proceedings of the IEEE conference on computer vision and pattern recognition , pages 3608â€“3617,
2018.
[133] Torben Hagerup, Kurt Mehlhorn, and J. Ian Munro. Maintaining discrete probability distributions optimally. In Proceedings of the 20th International
Colloquium on Automata, Languages and Programming , volume 700 of Lecture Notes in Computer Science , pages 253â€“264, Berlin, 1993. Springer-Verlag.
[134] Robert M Haralick, Karthikeyan Shanmugam, and Itsâ€™ Hak Dinstein. Textural features for image classification. IEEE Transactions on systems, man,
and cybernetics , (6):610â€“621, 1973.
[135] David Harel. Logics of programs: Axiomatics and descriptive power. MIT Research Lab Technical Report TR-200, Massachusetts Institute of
Technology, Cambridge, MA, 1978.
[136] David Harel. First-Order Dynamic Logic , volume 68 of Lecture Notes in Computer Science . Springer-Verlag, New York, NY, 1979. doi: 10.1007/3-540-
09237-4. URL http://dx.doi.org/10.1007/3-540-09237-4.
[137] Harvard CodeBlue. CodeBlue: Sensor networks for medical care, 2008. http://www.eecs.harvard.edu/mdw/ proj/codeblue/.
[138] Monica Haurilet, Alina Roitberg, and Rainer Stiefelhagen. Itâ€™s not about the journey; itâ€™s about the destination: Following soft paths under
question-guidance for visual reasoning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 1930â€“1939,
2019.
[139] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on
computer vision and pattern recognition , pages 770â€“778, 2016.
[140] Xiangnan He, Kuan Deng, Xiang Wang, Yan Li, Yongdong Zhang, and Meng Wang. Lightgcn: Simplifying and powering graph convolution
network for recommendation. In Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval ,
pages 639â€“648, 2020.
[141] Xuehai He, Yichen Zhang, Luntian Mou, Eric Xing, and Pengtao Xie. Pathvqa: 30000+ questions for medical visual question answering. arXiv
preprint arXiv:2003.10286 , 2020.
[142] J. Heering and P. Klint. Towards monolingual programming environments. ACM Trans. Program. Lang. Syst. , 7(2):183â€“213, April 1985.
[143] Yu-Jung Heo, Eun-Sol Kim, Woo Suk Choi, and Byoung-Tak Zhang. Hypergraph transformer: Weakly-supervised multi-hop reasoning for
knowledge-based visual question answering. arXiv preprint arXiv:2204.10448 , 2022.
[144] Simao Herdade, Armin Kappeler, Kofi Boakye, and Joao Soares. Image captioning: Transforming objects into words. Advances in Neural Information
Processing Systems , 32, 2019.
[145] Maurice Herlihy. A methodology for implementing highly concurrent data objects. ACM Trans. Program. Lang. Syst. , 15(5):745â€“770, November
1993. doi: 10.1145/161468.161469.
[146] Vaishnavi Himakunthala, Andy Ouyang, Daniel Rose, Ryan He, Alex Mei, Yujie Lu, Chinmay Sonar, Michael Saxon, and William Yang Wang. Letâ€™s
think frame by frame with VIP: A video infilling and prediction dataset for evaluating video chain-of-thought. In Houda Bouamor, Juan Pino, and
Kalika Bali, editors, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10,
2023, pages 204â€“219. Association for Computational Linguistics, 2023.
[147] Lynette Hirschman and Robert Gaizauskas. Natural language question answering: the view from here. natural language engineering , 7(4):275â€“300,
2001.
[148] C. A. R. Hoare. Chapter ii: Notes on data structuring. In O. J. Dahl, E. W. Dijkstra, and C. A. R. Hoare, editors, Structured programming (incoll) ,
pages 83â€“174. Academic Press Ltd., London, UK, UK, 1972. ISBN 0-12-200550-3. URL http://portal.acm.org/citation.cfm?id=1243380.1243382.
[149] Aidan Hogan, Eva Blomqvist, Michael Cochez, Claudia dâ€™Amato, Gerard De Melo, Claudio Gutierrez, Sabrina Kirrane, JosÃ© Emilio Labra Gayo,
Roberto Navigli, Sebastian Neumaier, et al. Knowledge graphs. ACM Computing Surveys (CSUR) , 54(4):1â€“37, 2021.
[150] Billy S. Hollis. Visual Basic 6: Design, Specification, and Objects with Other . Prentice Hall PTR, Upper Saddle River, NJ, USA, 1st edition, 1999. ISBN
0130850845.
Manuscript submitted to ACM34 Kuang and Xie, et al.
[151] Lars HÃ¶rmander. The analysis of linear partial differential operators. IV , volume 275 of Grundlehren der Mathematischen Wissenschaften [Fundamental
Principles of Mathematical Sciences] . Springer-Verlag, Berlin, Germany, 1985. ISBN 3-540-13829-3. Fourier integral operators.
[152] Lars HÃ¶rmander. The analysis of linear partial differential operators. III , volume 275 of Grundlehren der Mathematischen Wissenschaften [Fundamental
Principles of Mathematical Sciences] . Springer-Verlag, Berlin, Germany, 1985. ISBN 3-540-13828-5. Pseudodifferential operators.
[153] MD Zakir Hossain, Ferdous Sohel, Mohd Fairuz Shiratuddin, and Hamid Laga. A comprehensive survey of deep learning for image captioning.
ACM Computing Surveys (CsUR) , 51(6):1â€“36, 2019.
[154] Fengye Hu, Lu Wang, Shanshan Wang, Xiaolan Liu, and Gengxin He. A human body posture recognition algorithm based on bp neural network
for wireless body area networks. China Communications , 13(8):198â€“208, 2016.
[155] Jingwen Hu, Yuchen Liu, Jinming Zhao, and Qin Jin. MMGCN: Multimodal fusion via deep graph convolution network for emotion recognition in
conversation. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference
on Natural Language Processing (Volume 1: Long Papers) , pages 5666â€“5675, Online, August 2021. Association for Computational Linguistics. doi:
10.18653/v1/2021.acl-long.440. URL https://aclanthology.org/2021.acl-long.440.
[156] Ronghang Hu, Anna Rohrbach, Trevor Darrell, and Kate Saenko. Language-conditioned graph networks for relational reasoning. In Proceedings of
the IEEE/CVF international conference on computer vision , pages 10294â€“10303, 2019.
[157] Xiaowei Hu, Zhe Gan, Jianfeng Wang, Zhengyuan Yang, Zicheng Liu, Yumao Lu, and Lijuan Wang. Scaling up vision-language pre-training for
image captioning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 17980â€“17989, 2022.
[158] Deng Huang, Peihao Chen, Runhao Zeng, Qing Du, Mingkui Tan, and Chuang Gan. Location-aware graph convolutional networks for video
question answering. In Proceedings of the AAAI Conference on Artificial Intelligence , volume 34, pages 11021â€“11028, 2020.
[159] Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected convolutional networks. In Proceedings of the
IEEE conference on computer vision and pattern recognition , pages 4700â€“4708, 2017.
[160] Qingbao Huang, Jielong Wei, Yi Cai, Changmeng Zheng, Junying Chen, Ho-fung Leung, and Qing Li. Aligned dual channel graph convolutional
network for visual question answering. In Proceedings of the 58th annual meeting of the association for computational linguistics , pages 7166â€“7176,
2020.
[161] Zhicheng Huang, Zhaoyang Zeng, Bei Liu, Dongmei Fu, and Jianlong Fu. Pixel-bert: Aligning image pixels with text by deep multi-modal
transformers. arXiv preprint arXiv:2004.00849 , 2020.
[162] Zhicheng Huang, Zhaoyang Zeng, Yupan Huang, Bei Liu, Dongmei Fu, and Jianlong Fu. Seeing out of the box: End-to-end pre-training for
vision-language representation learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 12976â€“12985,
2021.
[163] Drew A Hudson and Christopher D Manning. Gqa: A new dataset for real-world visual reasoning and compositional question answering. In
Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages 6700â€“6709, 2019.
[164] IEEE. Ieee tcsc executive committee. In Proceedings of the IEEE International Conference on Web Services , ICWS â€™04, pages 21â€“22, Washington, DC,
USA, 2004. IEEE Computer Society. ISBN 0-7695-2167-3. doi: http://dx.doi.org/10.1109/ICWS.2004.64. URL http://dx.doi.org/10.1109/ICWS.2004.64.
[165] Ilija Ilievski and Jiashi Feng. Generative attention model with adversarial self-learning for visual question answering. In Proceedings of the on
Thematic Workshops of ACM Multimedia 2017 , pages 415â€“423, 2017.
[166] Ilija Ilievski, Shuicheng Yan, and Jiashi Feng. A focused dynamic attention model for visual question answering. arXiv preprint arXiv:1604.01485 ,
2016.
[167] Md. Farhan Ishmam, Md. Sakib Hossain Shovon, M.F. Mridha, and Nilanjan Dey. From image to language: A critical analysis of visual question
answering (vqa) approaches, challenges, and opportunities. Information Fusion , 106:102270, 2024. ISSN 1566-2535.
[168] Raisa Islam and Owana Marzia Moushi. Gpt-4o: The cutting-edge advancement in multimodal llm. Authorea Preprints , 2024.
[169] Srinivasan Iyer, Xi Victoria Lin, Ramakanth Pasunuru, Todor Mihaylov, Daniel Simig, Ping Yu, Kurt Shuster, Tianlu Wang, Qing Liu, Punit Singh
Koura, et al. Opt-iml: Scaling language model instruction meta learning through the lens of generalization. arXiv preprint arXiv:2212.12017 , 2022.
[170] Allan Jabri, Armand Joulin, and Laurens van der Maaten. Revisiting visual question answering baselines. In European conference on computer
vision , pages 727â€“739. Springer, 2016.
[171] Yunseok Jang, Yale Song, Youngjae Yu, Youngjin Kim, and Gunhee Kim. Tgif-qa: Toward spatio-temporal reasoning in visual question answering.
InProceedings of the IEEE conference on computer vision and pattern recognition , pages 2758â€“2766, 2017.
[172] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig. Scaling up visual
and vision-language representation learning with noisy text supervision. In International Conference on Machine Learning , pages 4904â€“4916. PMLR,
2021.
[173] Aiwen Jiang, Fang Wang, Fatih Porikli, and Yi Li. Compositional memory for visual question answering. arXiv preprint arXiv:1511.05676 , 2015.
[174] Huaizu Jiang, Ishan Misra, Marcus Rohrbach, Erik Learned-Miller, and Xinlei Chen. In defense of grid features for visual question answering. In
Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 10267â€“10276, 2020.
[175] Jianwen Jiang, Ziqiang Chen, Haojie Lin, Xibin Zhao, and Yue Gao. Divide and conquer: Question-guided spatio-temporal contextual attention for
video question answering. In Proceedings of the AAAI Conference on Artificial Intelligence , volume 34, pages 11101â€“11108, 2020.
[176] Yu Jiang, Vivek Natarajan, Xinlei Chen, Marcus Rohrbach, Dhruv Batra, and Devi Parikh. Pythia v0. 1: the winning entry to the vqa challenge 2018.
arXiv preprint arXiv:1807.09956 , 2018.
Manuscript submitted to ACMNatural Language Understanding and Inference with MLLM in Visual Question Answering: A Survey 35
[177] Zhuoxuan Jiang, Haoyuan Peng, Shanshan Feng, Fan Li, and Dongsheng Li. Llms can find mathematical reasoning mistakes by pedagogical
chain-of-thought. CoRR , abs/2405.06705, 2024.
[178] Weike Jin, Zhou Zhao, Mao Gu, Jun Yu, Jun Xiao, and Yueting Zhuang. Multi-interaction network with object relation for video question answering.
InProceedings of the 27th ACM international conference on multimedia , pages 1193â€“1201, 2019.
[179] Justin Johnson, Andrej Karpathy, and Li Fei-Fei. Densecap: Fully convolutional localization networks for dense captioning. In Proceedings of the
IEEE conference on computer vision and pattern recognition , pages 4565â€“4574, 2016.
[180] Justin Johnson, Bharath Hariharan, Laurens Van Der Maaten, Li Fei-Fei, C Lawrence Zitnick, and Ross Girshick. Clevr: A diagnostic dataset for
compositional language and elementary visual reasoning. In Proceedings of the IEEE conference on computer vision and pattern recognition , pages
2901â€“2910, 2017.
[181] Kushal Kafle and Christopher Kanan. Answer-type prediction for visual question answering. In Proceedings of the IEEE conference on computer
vision and pattern recognition , pages 4976â€“4984, 2016.
[182] Kushal Kafle and Christopher Kanan. Visual question answering: Datasets, algorithms, and future challenges. Computer Vision and Image
Understanding , 163:3â€“20, 2017.
[183] Aishwarya Kamath, Mannat Singh, Yann LeCun, Gabriel Synnaeve, Ishan Misra, and Nicolas Carion. Mdetr-modulated detection for end-to-end
multi-modal understanding. In Proceedings of the IEEE/CVF International Conference on Computer Vision , pages 1780â€“1790, 2021.
[184] Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. Dense passage retrieval
for open-domain question answering. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) , pages
6769â€“6781, 2020.
[185] Vahid Kazemi and Ali Elqursh. Show, ask, attend, and answer: A strong baseline for visual question answering. arXiv preprint arXiv:1704.03162 ,
2017.
[186] Mahmoud Khademi. Multimodal neural graph memory networks for visual question answering. In Proceedings of the 58th Annual Meeting of the
Association for Computational Linguistics , pages 7177â€“7188, 2020.
[187] Daesik Kim, Seonhoon Kim, and Nojun Kwak. Textbook question answering with multi-modal context graph understanding and self-supervised
open-set comprehension. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics , pages 3568â€“3584, 2019.
[188] Jin-Hwa Kim, Sang-Woo Lee, Donghyun Kwak, Min-Oh Heo, Jeonghee Kim, Jung-Woo Ha, and Byoung-Tak Zhang. Multimodal residual learning
for visual qa. Advances in neural information processing systems , 29, 2016.
[189] Jin-Hwa Kim, Kyoung-Woon On, Woosang Lim, Jeonghee Kim, Jung-Woo Ha, and Byoung-Tak Zhang. Hadamard product for low-rank bilinear
pooling. In International Conference on Learning Representations , 2017. URL https://openreview.net/forum?id=r1rhWnZkg.
[190] Jin-Hwa Kim, Jaehyun Jun, and Byoung-Tak Zhang. Bilinear attention networks. Advances in neural information processing systems , 31, 2018.
[191] Kyung-Min Kim, Min-Oh Heo, Seong-Ho Choi, and Byoung-Tak Zhang. Deepstory: Video story qa by deep embedded memory networks. arXiv
preprint arXiv:1707.00836 , 2017.
[192] Kyung-Min Kim, Seong-Ho Choi, Jin-Hwa Kim, and Byoung-Tak Zhang. Multimodal dual attention memory for video story question answering.
InProceedings of the European Conference on Computer Vision (ECCV) , pages 673â€“688, 2018.
[193] Wonjae Kim, Bokyung Son, and Ildoo Kim. Vilt: Vision-and-language transformer without convolution or region supervision. In International
Conference on Machine Learning , pages 5583â€“5594. PMLR, 2021.
[194] Thomas N. Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. In International Conference on Learning
Representations , 2017. URL https://openreview.net/forum?id=SJU4ayYgl.
[195] Ryan Kiros, Yukun Zhu, Russ R Salakhutdinov, Richard Zemel, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. Skip-thought vectors. Advances
in neural information processing systems , 28, 2015.
[196] Markus Kirschmer and John Voight. Algorithmic enumeration of ideal classes for quaternion orders. SIAM J. Comput. , 39(5):1714â€“1747, January
2010. ISSN 0097-5397. doi: https://doi.org/10.1137/080734467. URL http://dx.doi.org/10.1137/080734467.
[197] Donald E. Knuth. Seminumerical Algorithms . Addison-Wesley, 1981.
[198] Donald E. Knuth. Seminumerical Algorithms , volume 2 of The Art of Computer Programming . Addison-Wesley, Reading, MA, 2nd edition, 10 January
1981.
[199] Donald E. Knuth. The T EXbook . Addison-Wesley, Reading, MA., 1984.
[200] Donald E. Knuth. The Art of Computer Programming, Vol. 1: Fundamental Algorithms (3rd. ed.) . Addison Wesley Longman Publishing Co., Inc., 1997.
[201] Donald E. Knuth. The Art of Computer Programming , volume 1 of Fundamental Algorithms . Addison Wesley Longman Publishing Co., Inc., 3rd
edition, 1998. (book).
[202] Venkat Kodali and Daniel Berleant. Recent, rapid advancement in visual question answering: a review. In 2022 IEEE International Conference on
Electro Information Technology (eIT) , pages 139â€“146. IEEE, 2022.
[203] Philipp Koehn. Statistical machine translation . Cambridge University Press, 2009.
[204] Jing Yu Koh, Daniel Fried, and Russ R Salakhutdinov. Generating images with multimodal language models. Advances in Neural Information
Processing Systems , 36, 2024.
[205] Wei-Chang Kong. The implementation of electronic commerce in smes in singapore (as incoll). In E-commerce and cultural values , pages 51â€“74. IGI
Publishing, Hershey, PA, USA, 2001. ISBN 1-59140-056-2. URL http://portal.acm.org/citation.cfm?id=887006.887010.
Manuscript submitted to ACM36 Kuang and Xie, et al.
[206] Wei-Chang Kong. E-commerce and cultural values , name of chapter: The implementation of electronic commerce in SMEs in Singapore (Inbook-w-
chap-w-type), pages 51â€“74. IGI Publishing, Hershey, PA, USA, 2001. ISBN 1-59140-056-2. URL http://portal.acm.org/citation.cfm?id=887006.887010.
[207] Wei-Chang Kong. Chapter 9. In Theerasak Thanasankit, editor, E-commerce and cultural values (Incoll-w-text (chap 9) â€™titleâ€™) , pages 51â€“74. IGI
Publishing, Hershey, PA, USA, 2002. ISBN 1-59140-056-2. URL http://portal.acm.org/citation.cfm?id=887006.887010.
[208] Wei-Chang Kong. The implementation of electronic commerce in smes in singapore (incoll). In Theerasak Thanasankit, editor, E-commerce and
cultural values , pages 51â€“74. IGI Publishing, Hershey, PA, USA, 2003. ISBN 1-59140-056-2. URL http://portal.acm.org/citation.cfm?id=887006.887010.
[209] Wei-Chang Kong. E-commerce and cultural values - (InBook-num-in-chap) , chapter 9, pages 51â€“74. IGI Publishing, Hershey, PA, USA, 2004. ISBN
1-59140-056-2. URL http://portal.acm.org/citation.cfm?id=887006.887010.
[210] Wei-Chang Kong. E-commerce and cultural values (Inbook-text-in-chap) , chapter: The implementation of electronic commerce in SMEs in Singapore,
pages 51â€“74. IGI Publishing, Hershey, PA, USA, 2005. ISBN 1-59140-056-2. URL http://portal.acm.org/citation.cfm?id=887006.887010.
[211] Wei-Chang Kong. E-commerce and cultural values (Inbook-num chap) , chapter (in type field) 22, pages 51â€“74. IGI Publishing, Hershey, PA, USA,
2006. ISBN 1-59140-056-2. URL http://portal.acm.org/citation.cfm?id=887006.887010.
[212] E. Korach, D. Rotem, and N. Santoro. Distributed algorithms for finding centers and medians in networks. ACM Trans. Program. Lang. Syst. , 6(3):
380â€“401, July 1984.
[213] Jacob Kornerup. Mapping powerlists onto hypercubes. Masterâ€™s thesis, The University of Texas at Austin, 1994. (In preparation).
[214] David Kosiur. Understanding Policy-Based Networking . Wiley, New York, NY, 2nd. edition, 2001.
[215] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A Shamma,
et al. Visual genome: Connecting language and vision using crowdsourced dense image annotations. International journal of computer vision , 123
(1):32â€“73, 2017.
[216] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. Communications of the
ACM , 60(6):84â€“90, 2017.
[217] Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob
Devlin, Kenton Lee, et al. Natural questions: a benchmark for question answering research. Transactions of the Association for Computational
Linguistics , 7:453â€“466, 2019.
[218] John Lafferty, Andrew McCallum, and Fernando CN Pereira. Conditional random fields: Probabilistic models for segmenting and labeling sequence
data. 2001.
[219] Leslie Lamport. LATEX: A Document Preparation System . Addison-Wesley, Reading, MA., 1986.
[220] Nguyen-Khang Le, Dieu-Hien Nguyen, Dinh-Truong Do, Chau Nguyen, and Le Minh Nguyen. Vietnamese elementary math reasoning using large
language model with refined translation and dense-retrieved chain-of-thought. In Toyotaro Suzumura and Mayumi Bono, editors, New Frontiers in
Artificial Intelligence - JSAI International Symposium on Artificial Intelligence, JSAI-isAI 2024, Hamamatsu, Japan, May 28-29, 2024, Proceedings ,
volume 14741 of Lecture Notes in Computer Science , pages 260â€“268. Springer, 2024.
[221] Quoc Le and Tomas Mikolov. Distributed representations of sentences and documents. In International conference on machine learning , pages
1188â€“1196. PMLR, 2014.
[222] Thao Minh Le, Vuong Le, Svetha Venkatesh, and Truyen Tran. Hierarchical conditional relation networks for video question answering. In
Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages 9972â€“9981, 2020.
[223] Yann LeCun, Bernhard Boser, John S Denker, Donnie Henderson, Richard E Howard, Wayne Hubbard, and Lawrence D Jackel. Backpropagation
applied to handwritten zip code recognition. Neural computation , 1(4):541â€“551, 1989.
[224] Jan Lee. Transcript of question and answer session. In Richard L. Wexelblat, editor, History of programming languages I (incoll) , pages 68â€“71. ACM,
New York, NY, USA, 1981. ISBN 0-12-745040-8. doi: http://doi.acm.org/10.1145/800025.1198348. URL http://doi.acm.org/10.1145/800025.1198348.
[225] Newton Lee. Interview with bill kinder: January 13, 2005. Comput. Entertain. , 3(1):4, Jan.-March 2005. doi: 10.1145/1057270.1057278. URL
http://doi.acm.org/10.1145/1057270.1057278.
[226] Chenyi Lei, Lei Wu, Dong Liu, Zhao Li, Guoxin Wang, Haihong Tang, and Houqiang Li. Multi-question learning for visual question answering. In
Proceedings of the AAAI Conference on Artificial Intelligence , volume 34, pages 11328â€“11335, 2020.
[227] Jie Lei, Licheng Yu, Mohit Bansal, and Tamara L Berg. Tvqa: Localized, compositional video question answering. arXiv preprint arXiv:1809.01696 ,
2018.
[228] Paul Lerner, Olivier Ferret, Camille Guinaudeau, HervÃ© Le Borgne, Romaric BesanÃ§on, JosÃ© G Moreno, and JesÃºs LovÃ³n Melgarejo. Viquae, a
dataset for knowledge-based visual question answering about named entities. In Proceedings of the 45th International ACM SIGIR Conference on
Research and Development in Information Retrieval , pages 3108â€“3120, 2022.
[229] Bohao Li, Yuying Ge, Yixiao Ge, Guangzhi Wang, Rui Wang, Ruimao Zhang, and Ying Shan. Seed-bench-2: Benchmarking multimodal large
language models. arXiv preprint arXiv:2311.17092 , 2023.
[230] Bohao Li, Yuying Ge, Yi Chen, Yixiao Ge, Ruimao Zhang, and Ying Shan. Seed-bench-2-plus: Benchmarking multimodal large language models
with text-rich visual comprehension. arXiv preprint arXiv:2404.16790 , 2024.
[231] Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yixiao Ge, and Ying Shan. Seed-bench: Benchmarking multimodal llms with generative
comprehension. In CVPR , 2024.
[232] Cheng-Lun Li, Ayse G. Buyuktur, David K. Hutchful, Natasha B. Sant, and Satyendra K. Nainwal. Portalis: using competitive online interactions to
support aid initiatives for the homeless. In CHI â€™08 extended abstracts on Human factors in computing systems , pages 3873â€“3878, New York, NY,
Manuscript submitted to ACMNatural Language Understanding and Inference with MLLM in Visual Question Answering: A Survey 37
USA, 2008. ACM. ISBN 978-1-60558-012-X. doi: 10.1145/1358628.1358946. URL http://portal.acm.org/citation.cfm?id=1358628.1358946.
[233] Dongxu Li, Junnan Li, and Steven Hoi. Blip-diffusion: Pre-trained subject representation for controllable text-to-image generation and editing.
Advances in Neural Information Processing Systems , 36, 2024.
[234] Feng Li, Hao Zhang, Yi-Fan Zhang, Shilong Liu, Jian Guo, Lionel M Ni, PengChuan Zhang, and Lei Zhang. Vision-language intelligence: Tasks,
representation learning, and large models. arXiv preprint arXiv:2203.01922 , 2022.
[235] Gen Li, Nan Duan, Yuejian Fang, Ming Gong, and Daxin Jiang. Unicoder-vl: A universal encoder for vision and language by cross-modal pre-training.
InProceedings of the AAAI Conference on Artificial Intelligence , volume 34, pages 11336â€“11344, 2020.
[236] Guohao Li, Hang Su, and Wenwu Zhu. Incorporating external knowledge to answer open-domain visual questions with dynamic memory networks.
arXiv preprint arXiv:1712.00733 , 2017.
[237] Guohao Li, Xin Wang, and Wenwu Zhu. Boosting visual question answering with context-aware knowledge aggregation. In Proceedings of the 28th
ACM International Conference on Multimedia , pages 1227â€“1235, 2020.
[238] J. Li, R. R. Selvaraju, A. D. Gotmare, S. Joty, C. Xiong, and S. Hoi. Align before fuse: Vision and language representation learning with momentum
distillation, 2021.
[239] Jing Li, Aixin Sun, Jianglei Han, and Chenliang Li. A survey on deep learning for named entity recognition. IEEE Transactions on Knowledge and
Data Engineering , 34(1):50â€“70, 2020.
[240] Junnan Li, Ramprasaath Selvaraju, Akhilesh Gotmare, Shafiq Joty, Caiming Xiong, and Steven Chu Hong Hoi. Align before fuse: Vision and
language representation learning with momentum distillation. Advances in neural information processing systems , 34:9694â€“9705, 2021.
[241] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pre-training for unified vision-language understanding
and generation. arXiv preprint arXiv:2201.12086 , 2022.
[242] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large
language models. In International conference on machine learning , pages 19730â€“19742. PMLR, 2023.
[243] KunChang Li, Yinan He, Yi Wang, Yizhuo Li, Wenhai Wang, Ping Luo, Yali Wang, Limin Wang, and Yu Qiao. Videochat: Chat-centric video
understanding. arXiv preprint arXiv:2305.06355 , 2023.
[244] LH Li, M Yatskar, D Yin, CJ Hsieh, and KW Chang. A simple and performant baseline for vision and language. arXiv preprint arXiv:1908.03557 , 2019.
[245] Linjie Li, Zhe Gan, Yu Cheng, and Jingjing Liu. Relation-aware graph attention network for visual question answering. In Proceedings of the
IEEE/CVF international conference on computer vision , pages 10313â€“10322, 2019.
[246] Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, and Kai-Wei Chang. Visualbert: A simple and performant baseline for vision and language.
arXiv preprint arXiv:1908.03557 , 2019.
[247] Mingxiao Li and Marie-Francine Moens. Dynamic key-value memory enhanced multi-step graph reasoning for knowledge-based visual question
answering. arXiv preprint arXiv:2203.02985 , 2022.
[248] Qun Li, Fu Xiao, Bir Bhanu, Biyun Sheng, and Richang Hong. Inner knowledge-based img2doc scheme for visual question answering. ACM
Transactions on Multimedia Computing, Communications, and Applications (TOMM) , 18(3):1â€“21, 2022.
[249] Qun Li, Fu Xiao, Bir Bhanu, Biyun Sheng, and Richang Hong. Inner Knowledge-based Img2Doc Scheme for Visual Question Answering. ACM
Transactions on Multimedia Computing, Communications, and Applications , 18(3):1â€“21, 2022. ISSN 1551-6857, 1551-6865.
[250] Ruiyu Li and Jiaya Jia. Visual question answering with question representation update (qru). Advances in Neural Information Processing Systems , 29,
2016.
[251] Wei Li, Can Gao, Guocheng Niu, Xinyan Xiao, Hao Liu, Jiachen Liu, Hua Wu, and Haifeng Wang. Unimo: Towards unified-modal understanding
and generation via cross-modal contrastive learning. arXiv preprint arXiv:2012.15409 , 2020.
[252] Xiang Li, Shuo Chen, Xiaolin Hu, and Jian Yang. Understanding the disharmony between dropout and batch normalization by variance shift. In
Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages 2682â€“2690, 2019.
[253] Xiujun Li, Xi Yin, Chunyuan Li, Pengchuan Zhang, Xiaowei Hu, Lei Zhang, Lijuan Wang, Houdong Hu, Li Dong, Furu Wei, et al. Oscar:
Object-semantics aligned pre-training for vision-language tasks. In European Conference on Computer Vision , pages 121â€“137. Springer, 2020.
[254] Yanjie Li, Weijun Li, Lina Yu, Min Wu, Jingyi Liu, Wenqiang Li, Shu Wei, and Yusong Deng. Mllm-sr: Conversational symbolic regression base
multi-modal large language models. arXiv preprint arXiv:2406.05410 , 2024.
[255] Yikang Li, Nan Duan, Bolei Zhou, Xiao Chu, Wanli Ouyang, Xiaogang Wang, and Ming Zhou. Visual question generation as dual task of visual
question answering. In Proceedings of the IEEE conference on computer vision and pattern recognition , pages 6116â€“6124, 2018.
[256] Yinghui Li, Zishan Xu, Shaoshen Chen, Haojing Huang, Yangning Li, Yong Jiang, Zhongli Li, Qingyu Zhou, Hai-Tao Zheng, and Ying Shen. Towards
real-world writing assistance: A chinese character checking benchmark with faked and misspelled characters. arXiv preprint arXiv:2311.11268 , 2023.
[257] Zhihao Li, Yao Du, Yang Liu, Yan Zhang, Yufang Liu, Mengdi Zhang, and Xunliang Cai. Eagle: Elevating geometric reasoning through llm-empowered
visual instruction tuning. arXiv preprint arXiv:2408.11397 , 2024.
[258] Junwei Liang, Lu Jiang, Liangliang Cao, Li-Jia Li, and Alexander G Hauptmann. Focal visual-text attention for visual question answering. In
Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pages 6135â€“6143, 2018.
[259] Weixin Liang, Feiyang Niu, Aishwarya Reganti, Govind Thattai, and Gokhan Tur. Lrta: a transparent neural-symbolic reasoning framework with
modular supervision for visual question answering. arXiv preprint arXiv:2011.10731 , 2020.
[260] Weixin Liang, Yanhao Jiang, and Zixuan Liu. Graphvqa: Language-guided graph neural networks for scene graph question answering. NAACL-HLT
2021, page 79, 2021.
Manuscript submitted to ACM38 Kuang and Xie, et al.
[261] Yaoyuan Liang, Xin Wang, Xuguang Duan, and Wenwu Zhu. Multi-modal contextual graph neural network for text visual question answering. In
2020 25th International Conference on Pattern Recognition (ICPR) , pages 3491â€“3498. IEEE, 2021.
[262] Chin-Yew Lin. Rouge: A package for automatic evaluation of summaries. In Text summarization branches out , pages 74â€“81, 2004.
[263] Junyang Lin, An Yang, Yichang Zhang, Jie Liu, Jingren Zhou, and Hongxia Yang. Interbert: Vision-and-language interaction for multi-modal
pretraining. arXiv preprint arXiv:2003.13198 , 2020.
[264] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr DollÃ¡r, and C Lawrence Zitnick. Microsoft coco:
Common objects in context. In European conference on computer vision , pages 740â€“755. Springer, 2014.
[265] Tsung-Yu Lin, Aruni RoyChowdhury, and Subhransu Maji. Bilinear cnn models for fine-grained visual recognition. In Proceedings of the IEEE
international conference on computer vision , pages 1449â€“1457, 2015.
[266] Alexander H Liu, Wei-Ning Hsu, Michael Auli, and Alexei Baevski. Towards end-to-end unsupervised speech recognition. arXiv preprint
arXiv:2204.02492 , 2022.
[267] Fuxiao Liu, Tianrui Guan, Zongxia Li, Lichang Chen, Yaser Yacoob, Dinesh Manocha, and Tianyi Zhou. Hallusionbench: You see what you think?
or you think what you see? an image-context reasoning benchmark challenging for gpt-4v (ision), llava-1.5, and other multi-modality models.
arXiv preprint arXiv:2310.14566 , 2023.
[268] Haokun Liu, Derek Tam, Mohammed Muqeeth, Jay Mohta, Tenghao Huang, Mohit Bansal, and Colin A Raffel. Few-shot parameter-efficient
fine-tuning is better and cheaper than in-context learning. Advances in Neural Information Processing Systems , 35:1950â€“1965, 2022.
[269] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning, 2023.
[270] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems , 36, 2024.
[271] Hugo Liu and Push Singh. Conceptnetâ€”a practical commonsense reasoning tool-kit. BT technology journal , 22(4):211â€“226, 2004.
[272] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer
using shifted windows. In Proceedings of the IEEE/CVF International Conference on Computer Vision , pages 10012â€“10022, 2021.
[273] Zhun Liu, Ying Shen, Varun Bharadhwaj Lakshminarasimhan, Paul Pu Liang, Amir Zadeh, and Louis-Philippe Morency. Efficient low-rank
multimodal fusion with modality-specific factors. arXiv preprint arXiv:1806.00064 , 2018.
[274] Dengsheng Lu and Qihao Weng. A survey of image classification methods and techniques for improving classification performance. International
journal of Remote sensing , 28(5):823â€“870, 2007.
[275] Jiasen Lu, Jianwei Yang, Dhruv Batra, and Devi Parikh. Hierarchical question-image co-attention for visual question answering. Advances in neural
information processing systems , 29, 2016.
[276] Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language
tasks. Advances in neural information processing systems , 32, 2019.
[277] Jiasen Lu, Vedanuj Goswami, Marcus Rohrbach, Devi Parikh, and Stefan Lee. 12-in-1: Multi-task vision and language representation learning. In
Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 10437â€“10446, 2020.
[278] Pan Lu, Lei Ji, Wei Zhang, Nan Duan, Ming Zhou, and Jianyong Wang. R-vqa: learning visual relation facts with semantic attention for visual
question answering. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining , pages 1880â€“1889,
2018.
[279] Pan Lu, Hongsheng Li, Wei Zhang, Jianyong Wang, and Xiaogang Wang. Co-attending free-form regions and detections with multi-modal
multiplicative feature embedding for visual question answering. In Proceedings of the AAAI Conference on Artificial Intelligence , volume 32, 2018.
[280] Pan Lu, Liang Qiu, Jiaqi Chen, Tanglin Xia, Yizhou Zhao, Wei Zhang, Zhou Yu, Xiaodan Liang, and Song-Chun Zhu. Iconqa: A new benchmark for
abstract diagram understanding and visual language reasoning. In NeurIPS Datasets and Benchmarks , 2021.
[281] Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. Learn to
explain: Multimodal reasoning via thought chains for science question answering. In Sanmi Koyejo, S. Mohamed, A. Agarwal, Danielle Belgrave,
K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022,
NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022 , 2022.
[282] Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao.
Mathvista: Evaluating math reasoning in visual contexts with gpt-4v, bard, and other large multimodal models. In ICLR , 2024.
[283] Pan Lu, Baolin Peng, Hao Cheng, Michel Galley, Kai-Wei Chang, Ying Nian Wu, Song-Chun Zhu, and Jianfeng Gao. Chameleon: Plug-and-play
compositional reasoning with large language models. Advances in Neural Information Processing Systems , 36, 2024.
[284] Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and Pontus Stenetorp. Fantastically ordered prompts and where to find them: Overcoming
few-shot prompt order sensitivity. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) ,
pages 8086â€“8098, 2022.
[285] Chao Ma, Chunhua Shen, Anthony Dick, Qi Wu, Peng Wang, Anton van den Hengel, and Ian Reid. Visual question answering with memory-
augmented networks. In Proceedings of the IEEE conference on computer vision and pattern recognition , pages 6975â€“6984, 2018.
[286] Jie Ma, Pinghui Wang, Dechen Kong, Zewei Wang, Jun Liu, Hongbin Pei, and Junzhou Zhao. Robust visual question answering: Datasets, methods,
and future challenges. IEEE Transactions on Pattern Analysis and Machine Intelligence , pages 1â€“20, 2024. doi: 10.1109/TPAMI.2024.3366154.
[287] Lin Ma, Zhengdong Lu, and Hang Li. Learning to answer questions from image using convolutional neural network. In Thirtieth AAAI Conference
on Artificial Intelligence , 2016.
Manuscript submitted to ACMNatural Language Understanding and Inference with MLLM in Visual Question Answering: A Survey 39
[288] Zhiliang Ma and Shilong Liu. A review of 3d reconstruction techniques in civil engineering and their applications. Advanced Engineering Informatics ,
37:163â€“174, 2018.
[289] Mateusz Malinowski and Mario Fritz. A multi-world approach to question answering about real-world scenes based on uncertain input. Advances
in neural information processing systems , 27, 2014.
[290] Mateusz Malinowski, Marcus Rohrbach, and Mario Fritz. Ask your neurons: A neural-based approach to answering questions about images. In
Proceedings of the IEEE international conference on computer vision , pages 1â€“9, 2015.
[291] Christopher D Manning, Mihai Surdeanu, John Bauer, Jenny Rose Finkel, Steven Bethard, and David McClosky. The stanford corenlp natural
language processing toolkit. In Proceedings of 52nd annual meeting of the association for computational linguistics: system demonstrations , pages
55â€“60, 2014.
[292] Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and Roozbeh Mottaghi. Ok-vqa: A visual question answering benchmark requiring external
knowledge. In Proceedings of the IEEE/cvf conference on computer vision and pattern recognition , pages 3195â€“3204, 2019.
[293] Kenneth Marino, Xinlei Chen, Devi Parikh, Abhinav Gupta, and Marcus Rohrbach. Krisp: Integrating implicit and symbolic knowledge for
open-domain knowledge-based vqa. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 14111â€“14121,
2021.
[294] Minesh Mathew, Dimosthenis Karatzas, and CV Jawahar. Docvqa: A dataset for vqa on document images. In Proceedings of the IEEE/CVF winter
conference on applications of computer vision , pages 2200â€“2209, 2021.
[295] Minesh Mathew, Viraj Bagal, RubÃ¨n Tito, Dimosthenis Karatzas, Ernest Valveny, and CV Jawahar. Infographicvqa. In Proceedings of the IEEE/CVF
Winter Conference on Applications of Computer Vision , pages 1697â€“1706, 2022.
[296] Daniel D. McCracken and Donald G. Golden. Simplified Structured COBOL with Microsoft/MicroFocus COBOL . John Wiley & Sons, Inc., New York,
NY, USA, 1990. ISBN 0471514071.
[297] Walaa Medhat, Ahmed Hassan, and Hoda Korashy. Sentiment analysis algorithms and applications: A survey. Ain Shams engineering journal , 5(4):
1093â€“1113, 2014.
[298] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. Distributed representations of words and phrases and their compositionality.
Advances in neural information processing systems , 26, 2013.
[299] Shervin Minaee, Yuri Y Boykov, Fatih Porikli, Antonio J Plaza, Nasser Kehtarnavaz, and Demetri Terzopoulos. Image segmentation using deep
learning: A survey. IEEE transactions on pattern analysis and machine intelligence , 2021.
[300] Ron Mokady, Amir Hertz, and Amit H Bermano. Clipcap: Clip prefix for image captioning. arXiv preprint arXiv:2111.09734 , 2021.
[301] Sape Mullender, editor. Distributed systems (2nd Ed.) . ACM Press/Addison-Wesley Publishing Co., New York, NY, USA, 1993. ISBN 0-201-62427-3.
[302] E. Mumford. Managerial expert systems and organizational change: some critical research issues. In Critical issues in information systems research
(incoll) , pages 135â€“155. John Wiley & Sons, Inc., New York, NY, USA, 1987. ISBN 0-471-91281-6. URL http://portal.acm.org/citation.cfm?id=54905.
54911.
[303] Jonghwan Mun, Paul Hongsuck Seo, Ilchae Jung, and Bohyung Han. Marioqa: Answering questions by watching gameplay videos. In Proceedings
of the IEEE International Conference on Computer Vision , pages 2867â€“2875, 2017.
[304] Seil Na, Sangho Lee, Jisung Kim, and Gunhee Kim. A read-write memory network for movie story understanding. In Proceedings of the IEEE
International Conference on Computer Vision , pages 677â€“685, 2017.
[305] Vinod Nair and Geoffrey E Hinton. Rectified linear units improve restricted boltzmann machines. In Icml, 2010.
[306] Hyeonseob Nam, Jung-Woo Ha, and Jeonghee Kim. Dual attention networks for multimodal reasoning and matching. In Proceedings of the IEEE
conference on computer vision and pattern recognition , pages 299â€“307, 2017.
[307] Medhini Narasimhan and Alexander G Schwing. Straight to the facts: Learning knowledge base retrieval for factual visual question answering. In
Proceedings of the European conference on computer vision (ECCV) , pages 451â€“468, 2018.
[308] Medhini Narasimhan, Svetlana Lazebnik, and Alexander Schwing. Out of the box: Reasoning with graph convolution nets for factual visual
question answering. Advances in neural information processing systems , 31, 2018.
[309] A. Natarajan, M. Motani, B. de Silva, K. Yap, and K. C. Chua. Investigating network architectures for body sensor networks. In G. Whitcomb and
P. Neece, editors, Network Architectures , pages 322â€“328, Dayton, OH, 2007. Keleuven Press.
[310] Jiquan Ngiam, Aditya Khosla, Mingyu Kim, Juhan Nam, Honglak Lee, and Andrew Y Ng. Multimodal deep learning. In ICML , 2011.
[311] Duy-Kien Nguyen and Takayuki Okatani. Improved fusion of visual and language representations by dense symmetric co-attention for visual
question answering. In Proceedings of the IEEE conference on computer vision and pattern recognition , pages 6087â€“6096, 2018.
[312] F. Nielson. Program transformations in a denotational setting. ACM Trans. Program. Lang. Syst. , 7(3):359â€“379, July 1985.
[313] Hyeonwoo Noh and Bohyung Han. Training recurrent answering units with joint loss minimization for vqa. arXiv preprint arXiv:1606.03647 , 2016.
[314] Hyeonwoo Noh, Paul Hongsuck Seo, and Bohyung Han. Image question answering using convolutional neural network with dynamic parameter
prediction. In Proceedings of the IEEE conference on computer vision and pattern recognition , pages 30â€“38, 2016.
[315] Will Norcliffe-Brown, Stathis Vafeias, and Sarah Parisot. Learning conditioned graph structures for interpretable visual question answering.
Advances in neural information processing systems , 31, 2018.
[316] Dave Novak. Solder man. In ACM SIGGRAPH 2003 Video Review on Animation theater Program: Part I - Vol. 145 (July 27â€“27, 2003) , page 4, New
York, NY, March 21, 2008 2003. ACM Press. doi: 99.9999/woot07-S422. URL http://video.google.com/videoplay?docid=6528042696351994555.
Manuscript submitted to ACM40 Kuang and Xie, et al.
[317] Sai Vidyaranya Nuthalapati, Ramraj Chandradevan, Eleonora Giunchiglia, Bowen Li, Maxime Kayser, Thomas Lukasiewicz, and Carl Yang.
Lightweight visual question answering using scene graphs. In Proceedings of the 30th ACM International Conference on Information & Knowledge
Management , pages 3353â€“3357, 2021.
[318] Barack Obama. A more perfect union. Video, March 2008. URL http://video.google.com/videoplay?docid=6528042696351994555.
[319] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive coding. arXiv preprint arXiv:1807.03748 , 2018.
[320] Xichen Pan, Peiyu Chen, Yichen Gong, Helong Zhou, Xinbing Wang, and Zhouhan Lin. Leveraging uni-modal self-supervised learning for
multimodal audio-visual speech recognition. arXiv preprint arXiv:2203.07996 , 2022.
[321] Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. Librispeech: an asr corpus based on public domain audio books. In 2015
IEEE international conference on acoustics, speech and signal processing (ICASSP) , pages 5206â€“5210. IEEE, 2015.
[322] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic evaluation of machine translation. In Proceedings of
the 40th annual meeting of the Association for Computational Linguistics , pages 311â€“318, 2002.
[323] Aaron Parisi, Yao Zhao, and Noah Fiedel. TALM: tool augmented language models. CoRR , abs/2205.12255, 2022.
[324] Devshree Patel, Ratnam Parikh, and Yesha Shastri. Recent advances in video question answering: A review of datasets and methods. In International
Conference on Pattern Recognition , pages 339â€“356. Springer, 2021.
[325] Jeffrey Pennington, Richard Socher, and Christopher D Manning. Glove: Global vectors for word representation. In Proceedings of the 2014
conference on empirical methods in natural language processing (EMNLP) , pages 1532â€“1543, 2014.
[326] Cleon Pereira JÃºnior, Luiz Rodrigues, Newarney Costa, Valmir Macario Filho, and Rafael Mello. Can vlm understand childrenâ€™s handwriting? an
analysis on handwritten mathematical equation recognition. In International Conference on Artificial Intelligence in Education , pages 321â€“328.
Springer, 2024.
[327] Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. Deepwalk: Online learning of social representations. In Proceedings of the 20th ACM SIGKDD
international conference on Knowledge discovery and data mining , pages 701â€“710, 2014.
[328] Pierre Perruchet and Annie Vinter. Parser: A model for word segmentation. Journal of memory and language , 39(2):246â€“263, 1998.
[329] Charles J. Petrie. New algorithms for dependency-directed backtracking (masterâ€™s thesis). Masterâ€™s thesis, University of Texas at Austin, Austin,
TX, USA, 1986.
[330] Charles J. Petrie. New algorithms for dependency-directed backtracking (masterâ€™s thesis). Technical report, Austin, TX, USA, 1986.
[331] Pouya Pezeshkpour, Liyan Chen, and Sameer Singh. Embedding multimodal relational data for knowledge base completion. In Proceedings of the
2018 Conference on Empirical Methods in Natural Language Processing , pages 3208â€“3218, Brussels, Belgium, October-November 2018. Association
for Computational Linguistics. doi: 10.18653/v1/D18-1359. URL https://aclanthology.org/D18-1359.
[332] Bryan A Plummer, Liwei Wang, Chris M Cervantes, Juan C Caicedo, Julia Hockenmaier, and Svetlana Lazebnik. Flickr30k entities: Collecting
region-to-phrase correspondences for richer image-to-sentence models. In Proceedings of the IEEE international conference on computer vision ,
pages 2641â€“2649, 2015.
[333] Poker-Edge.Com. Stats and analysis, March 2006. URL http://www.poker-edge.com/stats.php.
[334] Daniel Povey, Arnab Ghoshal, Gilles Boulianne, Lukas Burget, Ondrej Glembek, Nagendra Goel, Mirko Hannemann, Petr Motlicek, Yanmin Qian,
Petr Schwarz, et al. The kaldi speech recognition toolkit. In IEEE 2011 workshop on automatic speech recognition and understanding , number CONF.
IEEE Signal Processing Society, 2011.
[335] Jay Pujara, Hui Miao, Lise Getoor, and William Cohen. Knowledge graph identification. In International semantic web conference , pages 542â€“557.
Springer, 2013.
[336] Di Qi, Lin Su, Jia Song, Edward Cui, Taroon Bharti, and Arun Sacheti. Imagebert: Cross-modal pre-training with large-scale weak-supervised
image-text data. arXiv preprint arXiv:2001.07966 , 2020.
[337] Yuxi Qian, Yuncong Hu, Ruonan Wang, Fangxiang Feng, and Xiaojie Wang. Question-driven graph fusion network for visual question answering.
arXiv preprint arXiv:2204.00975 , 2022.
[338] Chen Qu, Hamed Zamani, Liu Yang, W Bruce Croft, and Erik Learned-Miller. Passage retrieval for outside-knowledge visual question answering.
InProceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval , pages 1753â€“1757, 2021.
[339] Francis Quek, David McNeill, Robert Bryll, Susan Duncan, Xin-Feng Ma, Cemil Kirbas, Karl E McCullough, and Rashid Ansari. Multimodal human
discourse: gesture and speech. ACM Transactions on Computer-Human Interaction (TOCHI) , 9(3):171â€“193, 2002.
[340] R Core Team. R: A language and environment for statistical computing, 2019. URL https://www.R-project.org/.
[341] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin,
Jack Clark, et al. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning , pages
8748â€“8763. PMLR, 2021.
[342] Kiran Ramnath and Mark Hasegawa-Johnson. Seeing is knowing! fact-based visual question answering using knowledge graph embeddings. arXiv
preprint arXiv:2012.15484 , 2020.
[343] Brian K. Reid. A high-level approach to computer document formatting. In Proceedings of the 7th Annual Symposium on Principles of Programming
Languages , pages 24â€“31, New York, January 1980. ACM.
[344] Mengye Ren, Ryan Kiros, and Richard Zemel. Exploring models and data for image question answering. Advances in neural information processing
systems , 28, 2015.
Manuscript submitted to ACMNatural Language Understanding and Inference with MLLM in Visual Question Answering: A Survey 41
[345] Mengye Ren, Ryan Kiros, and Richard Zemel. Image question answering: A visual semantic embedding model and a new dataset. Proc. Advances in
Neural Inf. Process. Syst , 1(2):5, 2015.
[346] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. Advances
in neural information processing systems , 28, 2015.
[347] Fazlollah M Reza. An introduction to information theory . Courier Corporation, 1994.
[348] Jonathan Roberts, Timo LÃ¼ddecke, Rehan Sheikh, Kai Han, and Samuel Albanie. Charting new territories: Exploring the geographic and geospatial
capabilities of multimodal llms. arXiv preprint arXiv:2311.14656 , 2023.
[349] Cyril Robin and Simon Lacroix. Multi-robot target detection and tracking: taxonomy and survey. Autonomous Robots , 40(4):729â€“760, 2016.
[350] Daniel Rose, Vaishnavi Himakunthala, Andy Ouyang, Ryan He, Alex Mei, Yujie Lu, Michael Saxon, Chinmay Sonar, Diba Mirza, and William Yang
Wang. Visual chain of thought: Bridging logical gaps with multimodal infillings. CoRR , abs/2305.02317, 2023.
[351] Elliott M Ross. Signal sorting and amplification through g protein-coupled receptors. Neuron , 3(2):141â€“152, 1989.
[352] Edward Rosten, Reid Porter, and Tom Drummond. Faster and better: A machine learning approach to corner detection. IEEE transactions on pattern
analysis and machine intelligence , 32(1):105â€“119, 2008.
[353] Bernard Rous. The enabling of digital libraries. Digital Libraries , 12(3), July 2008. To appear.
[354] Octavian Rusu, Ionela Halcu, Oana Grigoriu, Giorgian Neculoiu, Virginia Sandulescu, Mariana Marinescu, and Viorel Marinescu. Converting
unstructured and semi-structured data into knowledge. In 2013 11th RoEduNet International Conference , pages 1â€“4. IEEE, 2013.
[355] Nelson Ruwa, Qirong Mao, Liangjun Wang, Jianping Gou, and Ming Dong. Mood-aware visual question answering. Neurocomputing , 330:305â€“316,
2019.
[356] Mehdi Saeedi, Morteza Saheb Zamani, and Mehdi Sedighi. A library-based synthesis methodology for reversible logic. Microelectron. J. , 41(4):
185â€“194, April 2010.
[357] Mehdi Saeedi, Morteza Saheb Zamani, Mehdi Sedighi, and Zahra Sasanian. Synthesis of reversible circuit using cycle-based approach. J. Emerg.
Technol. Comput. Syst. , 6(4), December 2010.
[358] Kuniaki Saito, Andrew Shin, Yoshitaka Ushiku, and Tatsuya Harada. Dualnet: Domain-invariant network for visual question answering. In 2017
IEEE International Conference on Multimedia and Expo (ICME) , pages 829â€“834. IEEE, 2017.
[359] S.L. Salas and Einar Hille. Calculus: One and Several Variable . John Wiley and Sons, New York, 1978.
[360] Victor Sanh, Albert Webson, Colin Raffel, Stephen H Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Teven Le Scao,
Arun Raja, et al. Multitask prompted training enables zero-shot task generalization. In ICLR 2022-Tenth International Conference on Learning
Representations , 2022.
[361] Adam Santoro, Sergey Bartunov, Matthew Botvinick, Daan Wierstra, and Timothy Lillicrap. Meta-learning with memory-augmented neural
networks. In International conference on machine learning , pages 1842â€“1850. PMLR, 2016.
[362] Michael Schlichtkrull, Thomas N Kipf, Peter Bloem, Rianne van den Berg, Ivan Titov, and Max Welling. Modeling relational data with graph
convolutional networks. In European semantic web conference , pages 593â€“607. Springer, 2018.
[363] Idan Schwartz, Alexander Schwing, and Tamir Hazan. High-order attention models for visual question answering. Advances in Neural Information
Processing Systems , 30, 2017.
[364] Joseph Scientist. The fountain of youth, August 2009. Patent No. 12345, Filed July 1st., 2008, Issued Aug. 9th., 2009.
[365] Meet Shah, Xinlei Chen, Marcus Rohrbach, and Devi Parikh. Cycle-consistency for robust visual question answering. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition , pages 6649â€“6658, 2019.
[366] Sanket Shah, Anand Mishra, Naganand Yadati, and Partha Pratim Talukdar. Kvqa: Knowledge-aware visual question answering. In Proceedings of
the AAAI conference on artificial intelligence , volume 33, pages 8876â€“8884, 2019.
[367] Zhenwei Shao, Zhou Yu, Meng Wang, and Jun Yu. Prompting large language models with answer heuristics for knowledge-based visual question
answering. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 14974â€“14983, 2023.
[368] Linda G Shapiro, George C Stockman, et al. Computer vision , volume 3. Prentice Hall New Jersey, 2001.
[369] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. Conceptual captions: A cleaned, hypernymed, image alt-text dataset for
automatic image captioning. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) ,
pages 2556â€“2565, 2018.
[370] Sheng Shen, Liunian Harold Li, Hao Tan, Mohit Bansal, Anna Rohrbach, Kai-Wei Chang, Zhewei Yao, and Kurt Keutzer. How much can clip benefit
vision-and-language tasks? In International Conference on Learning Representations , 2021.
[371] Ying Shen, Ning Ding, Hai-Tao Zheng, Yaliang Li, and Min Yang. Modeling relation paths for knowledge graph completion. IEEE Transactions on
Knowledge and Data Engineering , 33(11):3607â€“3617, 2020.
[372] Ying Shen, Min Yang, Yaliang Li, Dong Wang, Haitao Zheng, and Daoyuan Chen. Knowledge-based reasoning network for relation detection. IEEE
Transactions on Neural Networks and Learning Systems , 2021.
[373] Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang. Hugginggpt: Solving ai tasks with chatgpt and its friends
in hugging face. Advances in Neural Information Processing Systems , 36, 2024.
[374] Aman Shenoy and Ashish Sardana. Multilogue-net: A context-aware RNN for multi-modal emotion detection and sentiment analysis in conversation.
InSecond Grand-Challenge and Workshop on Multimodal Language (Challenge-HML) , pages 19â€“28, Seattle, USA, July 2020. Association for
Computational Linguistics. doi: 10.18653/v1/2020.challengehml-1.3. URL https://aclanthology.org/2020.challengehml-1.3.
Manuscript submitted to ACM42 Kuang and Xie, et al.
[375] Bowen Shi, Wei-Ning Hsu, and Abdelrahman Mohamed. Robust self-supervised audio-visual speech recognition. arXiv preprint arXiv:2201.01763 ,
2022.
[376] Jiaxin Shi, Hanwang Zhang, and Juanzi Li. Explainable and explicit visual reasoning over scene graphs. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pages 8376â€“8384, 2019.
[377] Yang Shi, Tommaso Furlanello, Sheng Zha, and Animashree Anandkumar. Question type guided attention in visual question answering. In
Proceedings of the European Conference on Computer Vision (ECCV) , pages 151â€“166, 2018.
[378] Yusuxke Shibata, Takuya Kida, Shuichi Fukamachi, Masayuki Takeda, Ayumi Shinohara, Takeshi Shinohara, and Setsuo Arikawa. Byte pair
encoding: A text compression scheme that accelerates pattern matching. 1999.
[379] Kevin J Shih, Saurabh Singh, and Derek Hoiem. Where to look: Focus regions for visual question answering. In Proceedings of the IEEE conference
on computer vision and pattern recognition , pages 4613â€“4621, 2016.
[380] Mustafa Shukor, Alexandre Rame, Corentin Dancette, and Matthieu Cord. Beyond task performance: Evaluating and reducing the flaws of large
multimodal models with in-context learning. arXiv preprint arXiv:2310.00647 , 2023.
[381] Nathan Silberman, Derek Hoiem, Pushmeet Kohli, and Rob Fergus. Indoor segmentation and support inference from rgbd images. In European
conference on computer vision , pages 746â€“760. Springer, 2012.
[382] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556 , 2014.
[383] Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, and Marcus Rohrbach. Towards vqa models that
can read. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages 8317â€“8326, 2019.
[384] Gurkirt Singh, Stephen Akrigg, Manuele Di Maio, Valentina Fontana, Reza Javanmard Alitappeh, Suman Saha, Kossar Jeddisaravi, Farzad Yousefi,
Jacob Culley, Tom Nicholson, et al. Road: The road event awareness dataset for autonomous driving. arXiv preprint arXiv:2102.11585 , 2021.
[385] Harsimran Jit Singh, Gourav Bathla, Munish Mehta, Gunjan Chhabra, and Pardeep Singh. Visual questions answering developments, applications,
datasets and opportunities: A state-of-the-art survey. In 2023 International Conference on Sustainable Computing and Data Communication Systems
(ICSCDS) , pages 778â€“785, 2023.
[386] Jasdeep Singh, Vincent Ying, and Alex Nutkiewicz. Attention on attention: Architectures for visual question answering (vqa). arXiv preprint
arXiv:1803.07724 , 2018.
[387] Amit Singhal et al. Modern information retrieval: A brief overview. IEEE Data Eng. Bull. , 24(4):35â€“43, 2001.
[388] Stan W. Smith. An experiment in bibliographic mark-up: Parsing metadata for xml export. In Reginald N. Smythe and Alexander Noble, editors,
Proceedings of the 3rd. annual workshop on Librarians and Computers , volume 3 of LAC â€™10 , pages 422â€“431, Milan Italy, 2010. Paparazzi Press. doi:
99.9999/woot07-S422. URL http://dx.doi.org/99.0000/woot07-S422.
[389] Joon Son Chung, Andrew Senior, Oriol Vinyals, and Andrew Zisserman. Lip reading sentences in the wild. In Proceedings of the IEEE conference on
computer vision and pattern recognition , pages 6447â€“6456, 2017.
[390] J. Song, P. Zeng, L. Gao, and H. T. Shen. From pixels to objects: Cubic visual attention for visual question answering. In Twenty-Seventh International
Joint Conference on Artificial Intelligence IJCAI-18 , 2018.
[391] Asad Z. Spector. Achieving application requirements. In Sape Mullender, editor, Distributed Systems , pages 19â€“33. ACM Press, New York, NY, 2nd.
edition, 1990. doi: 10.1145/90417.90738. URL http://doi.acm.org/10.1145/90417.90738.
[392] Yash Srivastava, Vaishnav Murali, Shiv Ram Dubey, and Snehasis Mukherjee. Visual question answering using deep learning: A survey and
performance analysis. In International Conference on Computer Vision and Image Processing , pages 75â€“86. Springer, 2021.
[393] Matteo Stefanini, Marcella Cornia, Lorenzo Baraldi, Silvia Cascianelli, Giuseppe Fiameni, and Rita Cucchiara. From show to tell: A survey on deep
learning-based image captioning. IEEE transactions on pattern analysis and machine intelligence , 45(1):539â€“559, 2022.
[394] W Su, X Zhu, Y Cao, B Li, L Lu, F Wei, and VL-BERT Dai J. Pre-training of generic visual-linguistic representations. In Proceedings of the 8th
International Conference on Learning Representations , pages 1â€“14, 2020.
[395] Weijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu, Furu Wei, and Jifeng Dai. Vl-bert: Pre-training of generic visual-linguistic representations. arXiv
preprint arXiv:1908.08530 , 2019.
[396] Zhou Su, Chen Zhu, Yinpeng Dong, Dongqi Cai, Yurong Chen, and Jianguo Li. Learning visual knowledge memory networks for visual question
answering. In Proceedings of the IEEE conference on computer vision and pattern recognition , pages 7736â€“7745, 2018.
[397] Sainbayar Sukhbaatar, Jason Weston, Rob Fergus, et al. End-to-end memory networks. Advances in neural information processing systems , 28, 2015.
[398] Rui Sun, Xuezhi Cao, Yan Zhao, Junchen Wan, Kun Zhou, Fuzheng Zhang, Zhongyuan Wang, and Kai Zheng. Multi-modal knowledge graphs for
recommender systems. In Proceedings of the 29th ACM international conference on information & knowledge management , pages 1405â€“1414, 2020.
[399] Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to sequence learning with neural networks. Advances in neural information processing
systems , 27, 2014.
[400] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew
Rabinovich. Going deeper with convolutions. In Proceedings of the IEEE conference on computer vision and pattern recognition , pages 1â€“9, 2015.
[401] Ganchao Tan, Daqing Liu, Meng Wang, and Zheng-Jun Zha. Learning to discretely compose reasoning module networks for video captioning.
arXiv preprint arXiv:2007.09049 , 2020.
[402] Hao Tan and Mohit Bansal. Lxmert: Learning cross-modality encoder representations from transformers. arXiv preprint arXiv:1908.07490 , 2019.
[403] Kaihua Tang, Hanwang Zhang, Baoyuan Wu, Wenhan Luo, and Wei Liu. Learning to compose dynamic tree structures for visual contexts. In
Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages 6619â€“6628, 2019.
Manuscript submitted to ACMNatural Language Understanding and Inference with MLLM in Visual Question Answering: A Survey 43
[404] Makarand Tapaswi, Yukun Zhu, Rainer Stiefelhagen, Antonio Torralba, Raquel Urtasun, and Sanja Fidler. Movieqa: Understanding stories in
movies through question-answering. In Proceedings of the IEEE conference on computer vision and pattern recognition , pages 4631â€“4640, 2016.
[405] Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai,
Anja Hauth, et al. Gemini: a family of highly capable multimodal models. arXiv preprint arXiv:2312.11805 , 2023.
[406] Damien Teney, Lingqiao Liu, and Anton van Den Hengel. Graph-structured representations for visual question answering. In Proceedings of the
IEEE conference on computer vision and pattern recognition , pages 1â€“9, 2017.
[407] Damien Teney, Peter Anderson, Xiaodong He, and Anton Van Den Hengel. Tips and tricks for visual question answering: Learnings from the 2017
challenge. In Proceedings of the IEEE conference on computer vision and pattern recognition , pages 4223â€“4232, 2018.
[408] Christopher Thomas and Adriana Kovashka. Emphasizing complementary samples for non-literal cross-modal retrieval. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 4632â€“4641, 2022.
[409] Bart Thomee, David A Shamma, Gerald Friedland, Benjamin Elizalde, Karl Ni, Douglas Poland, Damian Borth, and Li-Jia Li. Yfcc100m: The new
data in multimedia research. Communications of the ACM , 59(2):64â€“73, 2016.
[410] Harry Thornburg. Introduction to bayesian statistics, March 2001. URL http://ccrma.stanford.edu/~jos/bayes/bayes.html.
[411] Anthony Meng Huat Tiong, Junnan Li, Boyang Li, Silvio Savarese, and Steven C. H. Hoi. Plug-and-play VQA: zero-shot VQA by conjoining large
pretrained models with zero training. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang, editors, Findings of the Association for Computational
Linguistics: EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022 , pages 951â€“967. Association for Computational Linguistics, 2022.
[412] Du Tran, Lubomir Bourdev, Rob Fergus, Lorenzo Torresani, and Manohar Paluri. Learning spatiotemporal features with 3d convolutional networks.
InProceedings of the IEEE international conference on computer vision , pages 4489â€“4497, 2015.
[413] Athanasios Tsanas, Max A Little, Patrick E McSharry, Jennifer Spielman, and Lorraine O Ramig. Novel speech signal processing algorithms for
high-accuracy classification of parkinsonâ€™s disease. IEEE transactions on biomedical engineering , 59(5):1264â€“1271, 2012.
[414] Maria Tsimpoukelli, Jacob L Menick, Serkan Cabi, SM Eslami, Oriol Vinyals, and Felix Hill. Multimodal few-shot learning with frozen language
models. Advances in Neural Information Processing Systems , 34:200â€“212, 2021.
[415] TUG. Institutional members of the T EX users group, 2017. URL http://wwtug.org/instmem.html.
[416] Matthew Turk. Multimodal interaction: A review. Pattern recognition letters , 36:189â€“195, 2014.
[417] A. Tzamaloukas and J. J. Garcia-Luna-Aceves. Channel-hopping multiple access. Technical Report I-CA2301, Department of Computer Science,
University of California, Berkeley, CA, 2000.
[418] Jasper RR Uijlings, Koen EA Van De Sande, Theo Gevers, and Arnold WM Smeulders. Selective search for object recognition. International journal
of computer vision , 104(2):154â€“171, 2013.
[419] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Åukasz Kaiser, and Illia Polosukhin. Attention is all
you need. Advances in neural information processing systems , 30, 2017.
[420] Boris Veytsman. acmartâ€”Class for typesetting publications of ACM, 2017. URL http://www.ctan.org/pkg/acmart.
[421] Denny VrandeÄiÄ‡ and Markus KrÃ¶tzsch. Wikidata: a free collaborative knowledgebase. Communications of the ACM , 57(10):78â€“85, 2014.
[422] Bo Wang, Youjiang Xu, Yahong Han, and Richang Hong. Movie question answering: Remembering the textual cues for layered visual contents. In
Proceedings of the AAAI Conference on Artificial Intelligence , volume 32, 2018.
[423] Peng Wang, Qi Wu, Chunhua Shen, Anthony Dick, and Anton Van Den Henge. Explicit knowledge-based reasoning for visual question answering.
InProceedings of the 26th International Joint Conference on Artificial Intelligence , page 1290â€“1296. AAAI Press, 2017. ISBN 9780999241103.
[424] Peng Wang, Qi Wu, Chunhua Shen, Anthony Dick, and Anton Van Den Hengel. Fvqa: Fact-based visual question answering. IEEE transactions on
pattern analysis and machine intelligence , 40(10):2413â€“2427, 2017.
[425] Peng Wang, Qi Wu, Chunhua Shen, Anthony R Dick, and Anton van den Hengel. Explicit knowledge-based reasoning for visual question answering.
InIJCAI , 2017.
[426] Wenhai Wang, Zhe Chen, Xiaokang Chen, Jiannan Wu, Xizhou Zhu, Gang Zeng, Ping Luo, Tong Lu, Jie Zhou, Yu Qiao, et al. Visionllm: Large
language model is also an open-ended decoder for vision-centric tasks. Advances in Neural Information Processing Systems , 36, 2024.
[427] Wenhui Wang, Hangbo Bao, Li Dong, and Furu Wei. Vlmo: Unified vision-language pre-training with mixture-of-modality-experts. arXiv preprint
arXiv:2111.02358 , 2021.
[428] Wenhui Wang, Hangbo Bao, Li Dong, Johan Bjorck, Zhiliang Peng, Qiang Liu, Kriti Aggarwal, Owais Khan Mohammed, Saksham Singhal, Subhojit
Som, et al. Image as a foreign language: Beit pretraining for all vision and vision-language tasks. arXiv preprint arXiv:2208.10442 , 2022.
[429] Xiaozhi Wang, Ziqi Wang, Xu Han, Wangyi Jiang, Rong Han, Zhiyuan Liu, Juanzi Li, Peng Li, Yankai Lin, and Jie Zhou. Maven: A massive general
domain event detection dataset. arXiv preprint arXiv:2004.13590 , 2020.
[430] Yanan Wang, Jianming Wu, Kazuaki Furumai, Shinya Wada, and Satoshi Kurihara. Vae-based adversarial multimodal domain transfer for video-level
sentiment analysis. IEEE Access , 2022.
[431] Yanling Wang, Jing Zhang, Shasha Guo, Hongzhi Yin, Cuiping Li, and Hong Chen. Decoupling representation learning and classification for
gnn-based anomaly detection. In Proceedings of the 44th international ACM SIGIR conference on research and development in information retrieval ,
pages 1239â€“1248, 2021.
[432] Zirui Wang, Jiahui Yu, Adams Wei Yu, Zihang Dai, Yulia Tsvetkov, and Yuan Cao. Simvlm: Simple visual language model pretraining with weak
supervision. In International Conference on Learning Representations , 2021.
Manuscript submitted to ACM44 Kuang and Xie, et al.
[433] Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, and Quoc V Le. Finetuned language
models are zero-shot learners. In International Conference on Learning Representations , 2021.
[434] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits
reasoning in large language models. Advances in Neural Information Processing Systems , 35:24824â€“24837, 2022.
[435] Elizabeth M. Wenzel. Three-dimensional virtual acoustic displays. In Multimedia interface design (incoll) , pages 257â€“288. ACM, New York, NY, USA,
1992. ISBN 0-201-54981-6. doi: 10.1145/146022.146089. URL http://portal.acm.org/citation.cfm?id=146022.146089.
[436] Renato Werneck, JoÃ£o Setubal, and Arlindo da ConceicÃ£o. (old) finding minimum congestion spanning trees. J. Exp. Algorithmics , 5:11, 2000. ISSN
1084-6654. doi: http://doi.acm.org/10.1145/351827.384253.
[437] Renato Werneck, JoÃ£o Setubal, and Arlindo da ConceicÃ£o. (new) finding minimum congestion spanning trees. J. Exp. Algorithmics , 5, December
2000. ISSN 1084-6654. doi: 10.1145/351827.384253. URL http://portal.acm.org/citation.cfm?id=351827.384253.
[438] Chenfei Wu, Jinlai Liu, Xiaojie Wang, and Xuan Dong. Chain of reasoning for visual question answering. Advances in Neural Information Processing
Systems , 31, 2018.
[439] Chenfei Wu, Jinlai Liu, Xiaojie Wang, and Ruifan Li. Differential networks for visual question answering. In Proceedings of the AAAI Conference on
Artificial Intelligence , volume 33, pages 8997â€“9004, 2019.
[440] Chenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong Wang, Zecheng Tang, and Nan Duan. Visual chatgpt: Talking, drawing and editing with
visual foundation models. CoRR , abs/2303.04671, 2023.
[441] Chenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong Wang, Zecheng Tang, and Nan Duan. Visual chatgpt: Talking, drawing and editing with
visual foundation models. arXiv preprint arXiv:2303.04671 , 2023.
[442] Jialin Wu, Jiasen Lu, Ashish Sabharwal, and Roozbeh Mottaghi. Multi-modal answer validation for knowledge-based vqa. In Proceedings of the
AAAI Conference on Artificial Intelligence , volume 36, pages 2712â€“2721, 2022.
[443] Qi Wu, Chunhua Shen, Lingqiao Liu, Anthony Dick, and Anton Van Den Hengel. What value do explicit high level concepts have in vision to
language problems? In Proceedings of the IEEE conference on computer vision and pattern recognition , pages 203â€“212, 2016.
[444] Qi Wu, Peng Wang, Chunhua Shen, Anthony Dick, and Anton Van Den Hengel. Ask me anything: Free-form visual question answering based on
knowledge from external sources. In Proceedings of the IEEE conference on computer vision and pattern recognition , pages 4622â€“4630, 2016.
[445] Qi Wu, Damien Teney, Peng Wang, Chunhua Shen, Anthony Dick, and Anton Van Den Hengel. Visual question answering: A survey of methods
and datasets. Computer Vision and Image Understanding , 163:21â€“40, 2017.
[446] Xiaoqian Wu, Yong-Lu Li, Jianhua Sun, and Cewu Lu. Symbol-llm: leverage language models for symbolic system in visual human activity
reasoning. Advances in Neural Information Processing Systems , 36, 2024.
[447] Zhibiao Wu and Martha Palmer. Verbs semantics and lexical selection. In Proceedings of the 32nd Annual Meeting on Association for Computational
Linguistics , page 133â€“138, USA, 1994. Association for Computational Linguistics. doi: 10.3115/981732.981751. URL https://doi.org/10.3115/981732.
981751.
[448] Yijia Xiao, Edward Sun, Tianyu Liu, and Wei Wang. Logicvista: Multimodal llm logical reasoning benchmark in visual contexts. arXiv preprint
arXiv:2407.04973 , 2024.
[449] Caiming Xiong, Stephen Merity, and Richard Socher. Dynamic memory networks for visual and textual question answering. In International
conference on machine learning , pages 2397â€“2406. PMLR, 2016.
[450] BinChen Xu, Lu Ma, Liang Zhang, HaoHai Li, Qi Kang, and MengChu Zhou. An adaptive wordpiece language model for learning chinese word
embeddings. In 2019 IEEE 15th International Conference on Automation Science and Engineering (CASE) , pages 812â€“817. IEEE, 2019.
[451] Danfei Xu, Yuke Zhu, Christopher B Choy, and Li Fei-Fei. Scene graph generation by iterative message passing. In Proceedings of the IEEE conference
on computer vision and pattern recognition , pages 5410â€“5419, 2017.
[452] Dejing Xu, Zhou Zhao, Jun Xiao, Fei Wu, Hanwang Zhang, Xiangnan He, and Yueting Zhuang. Video question answering via gradually refined
attention over appearance and motion. In Proceedings of the 25th ACM international conference on Multimedia , pages 1645â€“1653, 2017.
[453] Huijuan Xu and Kate Saenko. Ask, attend and answer: Exploring question-guided spatial attention for visual question answering. In European
conference on computer vision , pages 451â€“466. Springer, 2016.
[454] Huijuan Xu and Kate Saenko. Dual attention network for visual question answering. In ECCV 2016 2nd Workshop on Storytelling with Images and
Videos (VisStory) , 2016.
[455] Hongwei Xue, Yupan Huang, Bei Liu, Houwen Peng, Jianlong Fu, Houqiang Li, and Jiebo Luo. Probing inter-modality: Visual parsing with
self-attention for vision-and-language pre-training. Advances in Neural Information Processing Systems , 34:4514â€“4528, 2021.
[456] Hongyang Xue, Zhou Zhao, and Deng Cai. Unifying the video and question attentions for open-ended video question answering. IEEE Transactions
on Image Processing , 26(12):5656â€“5666, 2017.
[457] Antoine Yang, Antoine Miech, Josef Sivic, Ivan Laptev, and Cordelia Schmid. Just ask: Learning to answer questions from millions of narrated
videos. In Proceedings of the IEEE/CVF International Conference on Computer Vision , pages 1686â€“1697, 2021.
[458] Antoine Yang, Antoine Miech, Josef Sivic, Ivan Laptev, and Cordelia Schmid. Zero-shot video question answering via frozen bidirectional language
models. arXiv preprint arXiv:2206.08155 , 2022.
[459] Chao Yang, Mengqi Jiang, Bin Jiang, Weixin Zhou, and Keqin Li. Co-attention network with question type for visual question answering. IEEE
Access , 7:40771â€“40781, 2019.
Manuscript submitted to ACMNatural Language Understanding and Inference with MLLM in Visual Question Answering: A Survey 45
[460] Jianwei Yang, Hao Zhang, Feng Li, Xueyan Zou, Chunyuan Li, and Jianfeng Gao. Set-of-mark prompting unleashes extraordinary visual grounding
in gpt-4v. arXiv preprint arXiv:2310.11441 , 2023.
[461] Kaicheng Yang, Hua Xu, and Kai Gao. Cm-bert: Cross-modal bert for text-audio sentiment analysis. In Proceedings of the 28th ACM international
conference on multimedia , pages 521â€“528, 2020.
[462] Min Yang, Junhao Liu, Lei Chen, Zhou Zhao, Xiaojun Chen, and Ying Shen. An advanced deep generative framework for temporal link prediction
in dynamic networks. IEEE transactions on cybernetics , 50(12):4946â€“4957, 2019.
[463] Min Yang, Chengming Li, Ying Shen, Qingyao Wu, Zhou Zhao, and Xiaojun Chen. Hierarchical human-like deep neural networks for abstractive
text summarization. IEEE Transactions on Neural Networks and Learning Systems , 32(6):2744â€“2757, 2020.
[464] Rui Yang, Lin Song, Yanwei Li, Sijie Zhao, Yixiao Ge, Xiu Li, and Ying Shan. Gpt4tools: Teaching large language model to use tools via self-instruction.
Advances in Neural Information Processing Systems , 36, 2024.
[465] Xu Yang, Chongyang Gao, Hanwang Zhang, and Jianfei Cai. Auto-parsing network for image captioning and visual question answering. In
Proceedings of the IEEE/CVF International Conference on Computer Vision , pages 2197â€“2207, 2021.
[466] Zekun Yang, Noa Garcia, Chenhui Chu, Mayu Otani, Yuta Nakashima, and Haruo Takemura. Bert representations for video question answering. In
Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision , pages 1556â€“1565, 2020.
[467] Zhenguo Yang, Jiale Xiang, Jiuxiang You, Qing Li, and Wenyin Liu. Event-oriented visual question answering: The e-vqa dataset and benchmark.
IEEE Transactions on Knowledge and Data Engineering , 35(10):10210â€“10223, 2023.
[468] Zhengyuan Yang, Zhe Gan, Jianfeng Wang, Xiaowei Hu, Yumao Lu, Zicheng Liu, and Lijuan Wang. An empirical study of GPT-3 for few-shot
knowledge-based VQA. In Thirty-Sixth AAAI Conference on Artificial Intelligence, AAAI 2022, Thirty-Fourth Conference on Innovative Applications of
Artificial Intelligence, IAAI 2022, The Twelveth Symposium on Educational Advances in Artificial Intelligence, EAAI 2022 Virtual Event, February 22 -
March 1, 2022 , pages 3081â€“3089. AAAI Press, 2022.
[469] Zhengyuan Yang, Zhe Gan, Jianfeng Wang, Xiaowei Hu, Yumao Lu, Zicheng Liu, and Lijuan Wang. An empirical study of gpt-3 for few-shot
knowledge-based vqa. In Proceedings of the AAAI Conference on Artificial Intelligence , volume 36, pages 3081â€“3089, 2022.
[470] Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Ehsan Azarnasab, Faisal Ahmed, Zicheng Liu, Ce Liu, Michael Zeng, and Lijuan Wang.
MM-REACT: prompting chatgpt for multimodal reasoning and action. CoRR , abs/2303.11381, 2023.
[471] Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Ehsan Azarnasab, Faisal Ahmed, Zicheng Liu, Ce Liu, Michael Zeng, and Lijuan Wang.
Mm-react: Prompting chatgpt for multimodal reasoning and action. arXiv preprint arXiv:2303.11381 , 2023.
[472] Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R Salakhutdinov, and Quoc V Le. Xlnet: Generalized autoregressive pretraining for
language understanding. Advances in neural information processing systems , 32, 2019.
[473] Zhuoqian Yang, Zengchang Qin, Jing Yu, and Yue Hu. Scene graph reasoning with prior visual relationship for visual question answering. arXiv
preprint arXiv:1812.09681 , 2018.
[474] Zichao Yang, Xiaodong He, Jianfeng Gao, Li Deng, and Alex Smola. Stacked attention networks for image question answering. In Proceedings of
the IEEE conference on computer vision and pattern recognition , pages 21â€“29, 2016.
[475] Qinghao Ye, Haiyang Xu, Jiabo Ye, Ming Yan, Anwen Hu, Haowei Liu, Qi Qian, Ji Zhang, Fei Huang, and Jingren Zhou. mplug-owl2: Revolutionizing
multi-modal large language model with modality collaboration. CoRR , abs/2311.04257, 2023.
[476] Yunan Ye, Zhou Zhao, Yimeng Li, Long Chen, Jun Xiao, and Yueting Zhuang. Video question answering via attribute-augmented attention network
learning. In Proceedings of the 40th International ACM SIGIR conference on Research and Development in Information Retrieval , pages 829â€“832, 2017.
[477] Kexin Yi, Jiajun Wu, Chuang Gan, Antonio Torralba, Pushmeet Kohli, and Josh Tenenbaum. Neural-symbolic vqa: Disentangling reasoning from
vision and language understanding. Advances in neural information processing systems , 31, 2018.
[478] Haoxuan You, Rui Sun, Zhecan Wang, Long Chen, Gengyu Wang, Hammad A Ayyubi, Kai-Wei Chang, and Shih-Fu Chang. Idealgpt: Iteratively
decomposing vision and language reasoning via large language models. arXiv preprint arXiv:2305.14985 , 2023.
[479] Fei Yu, Jiji Tang, Weichong Yin, Yu Sun, Hao Tian, Hua Wu, and Haifeng Wang. Ernie-vil: Knowledge enhanced vision-language representations
through scene graphs. In Proceedings of the AAAI Conference on Artificial Intelligence , volume 35, pages 3208â€“3216, 2021.
[480] Jing Yu, Zihao Zhu, Yujing Wang, Weifeng Zhang, Yue Hu, and Jianlong Tan. Cross-modal knowledge reasoning for knowledge-based visual
question answering. Pattern Recognition , 108:107563, 2020.
[481] Licheng Yu, Eunbyung Park, Alexander C Berg, and Tamara L Berg. Visual madlibs: Fill in the blank image generation and question answering.
arXiv preprint arXiv:1506.00278 , 2015.
[482] Youngjae Yu, Hyungjin Ko, Jongwook Choi, and Gunhee Kim. End-to-end concept word detection for video captioning, retrieval, and question
answering. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pages 3165â€“3173, 2017.
[483] Youngjae Yu, Jongseok Kim, and Gunhee Kim. A joint sequence fusion model for video question answering and retrieval. In Proceedings of the
European Conference on Computer Vision (ECCV) , pages 471â€“487, 2018.
[484] Zhou Yu, Jun Yu, Jianping Fan, and Dacheng Tao. Multi-modal factorized bilinear pooling with co-attention learning for visual question answering.
InProceedings of the IEEE international conference on computer vision , pages 1821â€“1830, 2017.
[485] Zhou Yu, Jun Yu, Chenchao Xiang, Jianping Fan, and Dacheng Tao. Beyond bilinear: Generalized multimodal factorized high-order pooling for
visual question answering. IEEE transactions on neural networks and learning systems , 29(12):5947â€“5959, 2018.
[486] Zhou Yu, Jun Yu, Yuhao Cui, Dacheng Tao, and Qi Tian. Deep modular co-attention networks for visual question answering. In Proceedings of the
IEEE/CVF conference on computer vision and pattern recognition , pages 6281â€“6290, 2019.
Manuscript submitted to ACM46 Kuang and Xie, et al.
[487] Zhaoquan Yuan, Siyuan Sun, Lixin Duan, Changsheng Li, Xiao Wu, and Changsheng Xu. Adversarial multimodal network for movie story question
answering. IEEE Transactions on Multimedia , 23:1744â€“1756, 2020.
[488] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, Cong Wei,
Botao Yu, Ruibin Yuan, Renliang Sun, Ming Yin, Boyuan Zheng, Zhenzhu Yang, Yibo Liu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen.
Mmmu: A massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. In Proceedings of CVPR , 2024.
[489] Amir Zadeh, Rowan Zellers, Eli Pincus, and Louis-Philippe Morency. Mosi: multimodal corpus of sentiment intensity and subjectivity analysis in
online opinion videos. arXiv preprint arXiv:1606.06259 , 2016.
[490] Amir Zadeh, Paul Pu Liang, and Louis-Philippe Morency. Foundations of multimodal co-learning. Information Fusion , 64:188â€“193, 2020.
[491] AmirAli Bagher Zadeh, Paul Pu Liang, Soujanya Poria, Erik Cambria, and Louis-Philippe Morency. Multimodal language analysis in the wild:
Cmu-mosei dataset and interpretable dynamic fusion graph. In Proceedings of the 56th Annual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers) , pages 2236â€“2246, 2018.
[492] Kuo-Hao Zeng, Tseng-Hung Chen, Ching-Yao Chuang, Yuan-Hong Liao, Juan Carlos Niebles, and Min Sun. Leveraging video descriptions to learn
video question answering. In Thirty-First AAAI Conference on Artificial Intelligence , 2017.
[493] Xingchen Zeng, Haichuan Lin, Yilin Ye, and Wei Zeng. Advancing multimodal large language models in chart question answering with visualization-
referenced instruction tuning. IEEE Transactions on Visualization and Computer Graphics , 2024.
[494] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pre-training. In IEEE/CVF International
Conference on Computer Vision, ICCV 2023, Paris, France, October 1-6, 2023 , pages 11941â€“11952. IEEE, 2023.
[495] Dongxiang Zhang, Rui Cao, and Sai Wu. Information fusion in visual question answering: A survey. Information Fusion , 52:268â€“280, 2019.
[496] Ge Zhang, Xinrun Du, Bei Chen, Yiming Liang, Tongxu Luo, Tianyu Zheng, Kang Zhu, Yuyang Cheng, Chunpu Xu, Shuyue Guo, et al. Cmmmu: A
chinese massive multi-discipline multimodal understanding benchmark. arXiv preprint arXiv:2401.11944 , 2024.
[497] Huibin Zhang, Zhengkun Zhang, Yao Zhang, Jun Wang, Yufan Li, Ning jiang, Xin wei, and Zhenglu Yang. Modeling temporal-modal entity graph
for procedural multimodal machine comprehension. Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics
(Volume 1: Long Papers):1179â€“1189, 2022.
[498] Jiawei Zhang, Tianyu Pang, Chao Du, Yi Ren, Bo Li, and Min Lin. Benchmarking large multimodal models against common corruptions. In NAACL ,
2024.
[499] Lei Zhang, Shuai Wang, and Bing Liu. Deep learning for sentiment analysis: A survey. Wiley Interdisciplinary Reviews: Data Mining and Knowledge
Discovery , 8(4):e1253, 2018.
[500] Pengchuan Zhang, Xiujun Li, Xiaowei Hu, Jianwei Yang, Lei Zhang, Lijuan Wang, Yejin Choi, and Jianfeng Gao. Vinvl: Revisiting visual
representations in vision-language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 5579â€“5588,
2021.
[501] Qi Zhang, Zhen Lei, Zhaoxiang Zhang, and Stan Z Li. Context-aware attention network for image-text retrieval. In Proceedings of the IEEE/CVF
conference on computer vision and pattern recognition , pages 3536â€“3545, 2020.
[502] Renrui Zhang, Dongzhi Jiang, Yichi Zhang, Haokun Lin, Ziyu Guo, Pengshuo Qiu, Aojun Zhou, Pan Lu, Kai-Wei Chang, Peng Gao, et al. Mathverse:
Does your multi-modal llm truly see the diagrams in visual math problems? arXiv preprint arXiv:2403.14624 , 2024.
[503] Zhuosheng Zhang, Aston Zhang, Mu Li, Hai Zhao, George Karypis, and Alex Smola. Multimodal chain-of-thought reasoning in language models.
CoRR , abs/2302.00923, 2023.
[504] Haozhe Zhao, Zefan Cai, Shuzheng Si, Xiaojian Ma, Kaikai An, Liang Chen, Zixuan Liu, Sheng Wang, Wenjuan Han, and Baobao Chang. MMICL:
empowering vision-language model with multi-modal in-context learning. CoRR , abs/2309.07915, 2023.
[505] Xiaojuan Zhao, Yan Jia, Aiping Li, Rong Jiang, and Yichen Song. Multi-source knowledge fusion: a survey. World Wide Web , 23(4):2567â€“2592, 2020.
[506] Zhou Zhao, Qifan Yang, Deng Cai, Xiaofei He, Yueting Zhuang, Zhou Zhao, Qifan Yang, Deng Cai, Xiaofei He, and Yueting Zhuang. Video question
answering via hierarchical spatio-temporal attention networks. In IJCAI , volume 2, page 8, 2017.
[507] Zhou Zhao, Xinghua Jiang, Deng Cai, Jun Xiao, Xiaofei He, and Shiliang Pu. Multi-turn video question answering via multi-stream hierarchical
attention context network. In IJCAI , volume 2018, page 27th, 2018.
[508] Zhou Zhao, Shuwen Xiao, Zehan Song, Chujie Lu, Jun Xiao, and Yueting Zhuang. Open-ended video question answering via multi-modal
conditional adversarial networks. IEEE Transactions on Image Processing , 29:3859â€“3870, 2020.
[509] Chen Zheng, Quan Guo, and Parisa Kordjamshidi. Cross-modality relevance for reasoning on language and vision. arXiv preprint arXiv:2005.06035 ,
2020.
[510] Zilong Zheng, Wenguan Wang, Siyuan Qi, and Song-Chun Zhu. Reasoning visual dialogs with structural and partial observations. In Proceedings
of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 6669â€“6678, 2019.
[511] Ming Zhong, Yang Liu, Yichong Xu, Chenguang Zhu, and Michael Zeng. Dialoglm: Pre-trained model for long dialogue understanding and
summarization. In Proceedings of the AAAI Conference on Artificial Intelligence , volume 36, pages 11765â€“11773, 2022.
[512] Bolei Zhou, Yuandong Tian, Sainbayar Sukhbaatar, Arthur Szlam, and Rob Fergus. Simple baseline for visual question answering. arXiv preprint
arXiv:1512.02167 , 2015.
[513] G. Zhou, J. Lu, C.-Y. Wan, M. D. Yarvis, and J. A. Stankovic. Body Sensor Networks . MIT Press, Cambridge, MA, 2008.
[514] Gang Zhou, Yafeng Wu, Ting Yan, Tian He, Chengdu Huang, John A. Stankovic, and Tarek F. Abdelzaher. A multifrequency mac specially designed for
wireless sensor network applications. ACM Trans. Embed. Comput. Syst. , 9(4):39:1â€“39:41, April 2010. ISSN 1539-9087. doi: 10.1145/1721695.1721705.
Manuscript submitted to ACMNatural Language Understanding and Inference with MLLM in Visual Question Answering: A Survey 47
URL http://doi.acm.org/10.1145/1721695.1721705.
[515] Luowei Zhou, Hamid Palangi, Lei Zhang, Houdong Hu, Jason Corso, and Jianfeng Gao. Unified vision-language pre-training for image captioning
and vqa. In Proceedings of the AAAI Conference on Artificial Intelligence , volume 34, pages 13041â€“13049, 2020.
[516] Xinzhe Zhou and Yadong Mu. Question-guided semantic dual-graph visual reasoning with novel answers. In Proceedings of the 2021 International
Conference on Multimedia Retrieval , pages 411â€“419, 2021.
[517] Yiyi Zhou, Tianhe Ren, Chaoyang Zhu, Xiaoshuai Sun, Jianzhuang Liu, Xinghao Ding, Mingliang Xu, and Rongrong Ji. Trar: Routing the attention
spans in transformer for visual question answering. In Proceedings of the IEEE/CVF International Conference on Computer Vision , pages 2074â€“2084,
2021.
[518] Chen Zhu, Yanpeng Zhao, Shuaiyi Huang, Kewei Tu, and Yi Ma. Structured attentions for visual question answering. In Proceedings of the IEEE
International Conference on Computer Vision , pages 1291â€“1300, 2017.
[519] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced
large language models. arXiv preprint arXiv:2304.10592 , 2023.
[520] Yuke Zhu, Oliver Groth, Michael Bernstein, and Li Fei-Fei. Visual7w: Grounded question answering in images. In Proceedings of the IEEE conference
on computer vision and pattern recognition , pages 4995â€“5004, 2016.
[521] Yuke Zhu, Joseph J Lim, and Li Fei-Fei. Knowledge acquisition for visual question answering via iterative querying. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition , pages 1154â€“1163, 2017.
[522] Zihao Zhu, Jing Yu, Yujing Wang, Yajing Sun, Yue Hu, and Qi Wu. Mucko: Multi-layer cross-modal knowledge reasoning for fact-based visual
question answering. In Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence , IJCAIâ€™20, 2021. ISBN 9780999241165.
[523] Alex Zhuang, Eddy Zhou, Quanquan Li, Rowan Dempster, Alikasim Budhwani, Mohammad Al-Sharman, Derek Rayside, and William Melek.
Radacs: Towards higher-order reasoning using action recognition in autonomous vehicles. arXiv preprint arXiv:2209.14408 , 2022.
[524] Yueting Zhuang, Dejing Xu, Xin Yan, Wenzhuo Cheng, Zhou Zhao, Shiliang Pu, and Jun Xiao. Multichannel attention refinement for video question
answering. ACM Transactions on Multimedia Computing, Communications, and Applications (TOMM) , 16(1s):1â€“23, 2020.
[525] Maryam Ziaeefard and Freddy Lecue. Towards knowledge-augmented visual question answering. In Proceedings of the 28th International Conference
on Computational Linguistics , pages 1863â€“1873, 2020.
[526] Yeyun Zou and Qiyu Xie. A survey on vqa: Datasets and approaches. In 2020 2nd International Conference on Information Technology and Computer
Application (ITCA) , pages 289â€“297. IEEE, 2020.
Received 13 January 2023; revised 24 June 2024; revised 18 October 2024
Manuscript submitted to ACM",1.9641289710998535,"['Jiayi Kuang', 'Jingyou Xie', 'Haohao Luo', 'Ronghao Li', 'Zhe Xu', 'Xianfeng Cheng', 'Yinghui Li', 'Xika Lin', 'Ying Shen']",https://www.semanticscholar.org/paper/4e5f4277cc06406a46b89ddb168fa63605a53a92,semanticscholar
A Review of Human Emotion Synthesis Based on Generative Technology,"Human emotion synthesis is a crucial aspect of affective computing. It involves using computational methods to mimic and convey human emotions through various modalities, with the goal of enabling more natural and effective human-computer interactions. Recent advancements in generative models, such as Autoencoders, Generative Adversarial Networks, Diffusion Models, Large Language Models, and Sequence-to-Sequence Models, have significantly contributed to the development of this field. However, there is a notable lack of comprehensive reviews in this field. To address this problem, this paper aims to address this gap by providing a thorough and systematic overview of recent advancements in human emotion synthesis based on generative models. Specifically, this review will first present the review methodology, the emotion models involved, the mathematical principles of generative models, and the datasets used. Then, the review covers the application of different generative models to emotion synthesis based on a variety of modalities, including facial images, speech, and text. It also examines mainstream evaluation metrics. Additionally, the review presents some major findings and suggests future research directions, providing a comprehensive understanding of the role of generative technology in the nuanced domain of emotion synthesis.","JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 1
A Review of Human Emotion Synthesis Based
on Generative Technology
Fei Ma*, Yukan Li*, Yifan Xie*, Ying He, Yi Zhang, Hongwei Ren, Zhou Liu, Wei Y ao, Fuji Ren, Senior
Member, IEEE , Fei Richard Yu, Fellow, IEEE , Shiguang Ni
Abstract â€”Human emotion synthesis is a crucial aspect of affective computing. It involves using computational methods to mimic and
convey human emotions through various modalities, with the goal of enabling more natural and effective human-computer interactions.
Recent advancements in generative models, such as Autoencoders, Generative Adversarial Networks, Diffusion Models, Large
Language Models, and Sequence-to-Sequence Models, have significantly contributed to the development of this field. However, there
is a notable lack of comprehensive reviews in this field. To address this problem, this paper aims to address this gap by providing a
thorough and systematic overview of recent advancements in human emotion synthesis based on generative models. Specifically, this
review will first present the review methodology, the emotion models involved, the mathematical principles of generative models, and
the datasets used. Then, the review covers the application of different generative models to emotion synthesis based on a variety of
modalities, including facial images, speech, and text. It also examines mainstream evaluation metrics. Additionally, the review presents
some major findings and suggests future research directions, providing a comprehensive understanding of the role of generative
technology in the nuanced domain of emotion synthesis.
Index Terms â€”Emotion Synthesis, Generative Technology, Autoencoder, Generative Adversarial Network, Diffusion Model, Large
Language Model, Sequence-to-Sequence Model
âœ¦
1 I NTRODUCTION
AFFECTIVE computing is an interdisciplinary research
field that aims to endow computers with the ability
to recognize, understand, express, and respond to human
emotions [1], [2]. It integrates theories and methods from
multiple disciplines such as computer science, psychology,
and cognitive science, attempting to reveal the essence of
human emotions and apply it to human-computer inter-
action and intelligent systems. The core goal of affective
computing is to enable computers to perceive, understand,
and express emotions like humans, thereby achieving more
natural and friendly human-computer interaction [3], [4],
[5], [6].
â€¢(Corresponding Author: Shiguang Ni. *: Equal Contribution.)
â€¢Fei Ma, Yifan Xie, Yi Zhang, and Zhou Liu are with Guangdong Labora-
tory of Artificial Intelligence and Digital Economy (SZ), Shenzhen, China.
E-mail: mafei@gml.ac.cn, xieyifan@stu.xjtu.edu.cn, zhangyi@gml.ac.cn,
liuzhou@gml.ac.cn.
â€¢Yukan Li, Wei Yao, and Shiguang Ni are with Tsinghua Shenzhen
International Graduate School, Tsinghua University, Shenzhen, China.
E-mail: liyukan23@mails.tsinghua.edu.cn, viviayao@sz.tsinghua.edu.cn,
ni.shiguang@sz.tsinghua.edu.cn.
â€¢Ying He is with College of Computer Science and Software Engineering,
Shenzhen University, Shenzhen, China. E-mail: heying@szu.edu.cn.
â€¢Hongwei Ren is with MICS Thrust, The Hong Kong Univer-
sity of Science and Technology (GZ), Guangzhou, China. E-mail:
hren066@connect.hkust-gz.edu.cn.
â€¢Fuji Ren is with School of Computer Science and Engineering, Univer-
sity of Electronic Science and Technology of China, Chengdu, China,
and also with Shenzhen Institute for Advanced Study, University of
Electronic Science and Technology of China, Shenzhen, China. E-mail:
renfuji@uestc.edu.cn.
â€¢Fei Richard Yu is with College of Computer Science and Software
Engineering, Shenzhen University, Shenzhen, China, and also with
School of Information Technology, Carleton University, Canada. E-mail:
richard.yu@ieee.org.
Manuscript received April 19, 2005; revised August 26, 2015.Emotion synthesis [7] is an important branch of affective
computing, which aims to enable computers to generate
emotional expressions similar to human emotions. This
ability can be realized through various common modalities,
such as facial images, speech, and text. To achieve emotion
synthesis, researchers have proposed a series of traditional
methods by analyzing the characteristics of human emo-
tional expressions and establishing mathematical models.
These models are then used to generate speech and facial
expressions with specific emotions using computers [8], [9].
Artificial intelligence has made remarkable advances in
synthesizing human emotions, marking a significant break-
through in the field. In particular, generative technology
has greatly improved the effect and application scope of
emotion synthesis [10], [11], [12]. Compared with traditional
methods, these new models can automatically learn the
characteristics of emotional expressions from massive data
without relying on manually designed rules and models
[13], [14], [15], [16]. With their powerful generation capa-
bilities, generative models can generate emotional samples
that are highly similar to real data and more flexible, greatly
expanding the research boundaries in the field of emotion
synthesis. For example, some researchers use Autoencoders
(AEs) [17] to generate speech with emotions. By modifying
this structure, they can extract speaker embeddings, isolate
timbre information, and control the flow of emotional at-
tributes [18]. Other researchers use Generative Adversarial
Networks (GANs) [19] to generate facial images with spe-
cific emotions. By controlling the input of the generative
model, they can generate faces expressing different emo-
tions such as happiness, sadness, and anger [20]. In recent
years, Diffusion Models (DMs) [21] and Large Language
Models (LLMs) [22] have also been widely used in emo-arXiv:2412.07116v1  [cs.LG]  10 Dec 2024JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 2
tion synthesis tasks. Some researchers use DMs to enhance
image and audio processing by employing a reconstruction
Module, which leverages noising and denoising in latent
spaces [23]. Other researchers use LLMs to generate em-
pathic conversational texts by training language models on
empathic conversations and injecting emotional information
into response generation [24]. However, to the best of our
knowledge, there is a conspicuous absence of a systematic
review that specifically focuses on generative technology for
human emotion synthesis within this burgeoning field.
This study examines how generative AI models syn-
thesize human emotions, addressing current gaps in re-
search through a systematic analysis. The overall schematic
diagram is illustrated in Fig. 1. Specifically, this paper
will firstly introduce several generative models and their
underlying mathematical principles to help readers better
understand the technical background of this field, includ-
ing AEs, GANs, DMs, LLMs, and Sequence-to-Sequence
(Seq2Seq) models [25]. AEs learn compressed data represen-
tations through encoder-decoder reconstruction, enabling
dimensionality reduction and feature extraction. GANs use
adversarial training between a generator and discrimina-
tor to produce realistic data samples. DMs generate data
by learning to reverse a noise diffusion process, often
avoiding vanishing gradients. LLMs leverage transformer
architectures and massive text datasets for human language
processing and generation. Seq2Seq models use encoder-
decoder architectures to map input sequences to output
sequences, facilitating tasks like translation and synthesis.
Then, this review summarizes the commonly used human
emotion synthesis datasets from unimodal to cross-modal.
Each dataset has annotated emotion labels according to
its purpose, covering the simplest positive and negative
emotion labels [26] to complex 32 categories of compound
emotion labels [27].
Subsequently, we categorize human emotion synthesis
into three subfields: facial emotion synthesis, speech emo-
tion synthesis, and textual emotion synthesis. The taxonomy
of this survey is shown in Fig. 2. Facial emotion synthesis in-
volves modifying facial information in computer-generated
faces to create more realistic and diverse emotions. This area
of research intersects with face reenactment [28], [29], face
manipulation [30], and talking head generation [31], [32].
Speech emotion synthesis involves altering the emotional
attributes of speech segments or generating new speech
that conveys specific emotions by manipulating acoustic
features. This section will cover various studies on Voice
Conversion [33], [34], Text-to-Speech (TTS) [35], and speech
manipulation tasks [36]. Textual emotion synthesis refers to
using computational techniques to infuse textual content
with different emotions or sentiments, thereby enhancing
its expressiveness. This section will focus on text emotion
transfer and empathetic dialogue generation.
Finally, this review summarize the prevalent evaluation
metrics employed in the task, present key findings from
multiple perspectives, and offer insights into future research
directions based on the aforementioned systematic analysis.
The main findings include: (i) Generative models have made
significant progress in emotion synthesis across multiple
modalities, including facial images, speech, and text. (ii) In
the past, GAN-based methods have demonstrated strongcapability in facial emotion synthesis, excelling at capturing
subtle nuances in expressions. However, DMs have now
emerged as a more promising alternative, offering superior
control, stronger adaptability across different modalities.
(iii) Speech emotion synthesis has benefited from the adap-
tation of GANs and Seq2Seq models, with further improve-
ments through AEs and DMs to enhance emotional depth
and prosodic control. (iv) Textual emotion synthesis has
increasingly leveraged LLMs and Seq2Seq architectures, us-
ing sentiment control and emotional valence modulation to
produce emotionally resonant content, although challenges
remain in balancing emotional expressiveness with conver-
sational coherence. (v) Both subjective and objective metrics
are essential for evaluating emotion synthesis models, with
future research focusing on refining both to better capture
emotional subtleties and align with human judgments.
Looking ahead, there are still many directions worth ex-
ploring, which include: (i) Combining different generative
models like GANs, Seq2Seqs, AEs, DMs, and LLMs can en-
hance emotion synthesis by leveraging the strengths of each
model for more accurate and realistic outputs. (ii) Exploring
new modalities like gestures, electroencephalogram (EEG)
[37], and electrocardiogram (ECG) [38], as well as cross-
modal models, expanding the potential for immersive and
interactive emotional experiences. (iii) Real-time emotion
generation on edge devices like smartphones and wearables
can enable personalized, adaptive emotional interactions,
with applications in healthcare, retail, and more. (iv) Emo-
tion synthesis can transform digital entertainment and film-
making by enabling more authentic emotional expressions
in virtual characters, enhancing storytelling, and allowing
real-time emotional adjustments in films based on audience
feedback. This research provides a framework for under-
standing how generative models replicate human emotions,
offering insights to guide future developments in the field.
Overall, the main contributions of this paper include:
â€¢To the best of our knowledge, this review provides
the first systematic overview of human emotion syn-
thesis based on generative technology.
â€¢By analyzing more than 230 related papers, this
review gives a taxonomy of generative technology-
based human emotion synthesis in different modali-
ties.
â€¢We summarize commonly used datasets and eval-
uation metrics for human emotion synthesis across
different modalities.
â€¢Finally, we discuss the current research status of
human emotion synthesis based on generation tech-
nology and present a future outlook.
The remainder of this paper is structured as follows:
Sections 3 - 6 describe the gaps in existing review research,
the mainstream emotion models, mathematical principles
for generating models, and the commonly used datasets,
Sections 7 - 9 introduce the latest human emotion synthesis
works in the three modalities of face images, speech, and
text, Section 10 and 11 summarize the common evaluation
metrics in the field and discuss the current state of research
and development trends, and Section 12 provides the con-
clusion. A list of abbreviations is given in Table 1.JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 3
Images
Speech
Text
Images
Speech
Text
â€œThis book was 
very boring. â€â€œThis book was 
very excellent! â€
Input
Synthesize
AE GAN
DM
LLM
 Seq2Seq
Genrative ModelsFacial Emotion 
SynthesisSpeech Emotion 
Synthesis
Textual Emotion 
Synthesis
Fig. 1. Schematic Diagram of Generation Technology for Human Emo-
tion Synthesis.
(1) Schematic diagram of facial emotion synthesis is sourced from https://www.
semanticscholar.org/reader/2ebb1cb387a1761b20b97c7c3038a7ab119b54d7,
(2) Schematic diagram of speech emotion synthesis is sourced from https:
//huggingface.co/learn/audio-course/chapter6/pre-trained_models,
(3) Schematic diagram of speech emotion synthesis is sourced from https://blog.
reachsumit.com/posts/2020/12/generating-empathetic-responses/.
(4) Schematics of GAN and VAE are sourced from https://www.researchgate.
net/figure/Comparison-of-two-categories-of-generative-models_fig2_
378846405,
(5) Seq2Seq schematic is sourced from https://www.geeksforgeeks.org/
self-attention-in-nlp/,
(6) Diffusion model schematic is sourced from https://bobondemon.github.io/
2024/07/18/%E7%B4%80%E9%8C%84-Evidence-Lower-BOund-ELBO-%E7%
9A%84%E4%B8%89%E7%A8%AE%E7%94%A8%E6%B3%95/,
(7) LLM schematic is sourced from https://events.afcea.org/Augusta24/
Custom/Handout/Speaker0_Session11206_1.pdf.
TABLE 1
Main Acronyms.
Acronym Full Form
AAE Adversarial Autoencoder
ACC Accuracy
AE Autoencoder
AIGC Artificial Intelligence Generated Content
AU Action Unit
AUD AU-intensity Discriminator
BLEU Bilingual Evaluation Understudy
CAAE Conditional Adversarial Autoencoder
CGAN Conditional GAN
CWT Continuous Wavelet Transform
DM Diffusion Model
FID FrÃ©chet Inception Distance
F0 RMSE F0 Root Mean Square Error
GAN Generative Adversarial Network
GPT Generative Pre-trained Transformer
MCD Mel Cepstral Distortion
MFCC Mel-Frequency Cepstral Coefficient
MOS Mean Opinion Score
PPL Perplexity
PSNR Peak Signal-to-Noise Ratio
RNN Recurrent Neural Network
Seq2Seq Sequence-to-Sequence
SMOS Similarity Mean Opinion Score
SSIM Structural Similarity Index
TTS Text-to-Speech
T5 Text-to-Text Transfer Transformer
VAE Variational Autoencoder
VC Voice Conversion
2 R EVIEW METHODOLOGY
To compile this extensive review on human emotion synthe-
sis based on generative models, a systematic and rigorous
A Review of Human 
Emotion Synthesis Based 
on Generative TechnologyFace Reenactment GAN-based [90-92, 94-96, 122]
Talking Head 
GenerationAE-based [132]
GAN-based [116-118]
DM-based [23, 120, 133-134, 136]
Seq2Seq-based [131]Face ManipulationAE-based [114]
GAN-based [20, 30, 97-113, 123-128]
Voice Conversion
Seq2Seq-based [151-153]AE-based [143-145, 150, 170]
GAN-based [76, 137-139, 141-142, 147-149]
DM-based [146]
Text-to-Speech
Seq2Seq-based [81, 154, 156-159, 162-165, 174-178]GAN-based [160, 173]
DM-based [155, 161, 166, 171, 180-181]
Speech 
Manipulation
Seq2Seq-based [36, 167-168, 182]AE-based [18]
GAN-based [79]
Text Emotion 
Transfer
Seq2Seq-based [26, 185, 190, 192, 215]AE-based [189, 191]
LLM-based [183-184, 186, 212]
Empathetic 
Dialogue 
Generation
Seq2Seq-based [198-200, 202-205, 223]AE-based [200-202, 206, 219]
GAN-based [208-209, 211, 223]
DM-based [210]
LLM-based [24, 193-197, 217-218, 224-225]Speech Emotion 
SynthesisFacial Emotion 
Synthesis
Textual Emotion 
SynthesisFig. 2. Taxonomy of This Survey.
approach was followed to ensure the comprehensiveness
and relevance of the literature. The overall screening pro-
cess is shown in Fig. 3. Initially, we conducted compre-
hensive searches across key academic databases, including
IEEE Xplore, ScienceDirect, and Google Scholar. The search
strategy combined general and modality-specific keywords
related to generative models and emotion synthesis. Ex-
amples of search terms included â€œfacial emotion synthe-
sisâ€ + â€œgenerative models,â€ â€œspeech emotion synthesisâ€ +
â€œgenerative models,â€ â€œemotional face reenactment,â€ and
â€œemotional voice conversion,â€ etc., aiming to cover a wide
array of studies in these domains. To ensure the inclusion
of the most recent advancements, the search focused on
papers published between 2017 and 2024, providing a com-
prehensive overview of developments in this timeframe. To
further broaden the scope, we included terms referencing
specific generative model architectures, such as AE, GAN,
DM, LLM, and Seq2Seq. The initial search yielded more than
270 papers.
Furthermore, we established strict inclusion criteria for
the selection process: (1) Peer-reviewed papers publishedJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 4
Screening Process:
Inclusion Criteria, Title, Abstract, Full-TextLiterature Search:
 IEEE Xplore, ScienceDirect, Google Scholar
Search Parameters:
Keywords, Timeframe (2017-2024)
Initial Retrieval:
More than 270 papers
Data Processing & Quality Assessment
Final Selection & Outcome:
A Complete and Comprehensive Review
Fig. 3. A Comprehensive Review Methodology.
up to November 2024, including both journal and confer-
ence papers; (2) Research focusing on the application of
generative models to human emotion synthesis in various
modalities; (3) Studies in which extensive experiments were
conducted and fully evaluated.
After the initial retrieval, a two-step filtering process was
applied to ensure the focus remained on emotion synthesis.
Studies whose primary aim was not related to emotional
generation or did not involve the use of generative models
were excluded. Furthermore, we eliminated papers with
incremental contributions to maintain the diversity of the
sources reviewed. The final set of selected works provides
a detailed overview of the current and impactful advance-
ments in the field.
By following this structured methodology, we ensured
the thoroughness and relevance of the selected studies,
offering a timely and well-rounded perspective on the
role of generative technology in human emotion synthesis
across multiple modalities. This process ultimately enabled
a focused analysis of both seminal works and the latest
advancements, contributing to a deepened understanding
of generative models in affective computing.
3 E MOTION MODEL
Emotion is commonly understood as a complex and ever-
changing state of mind and body, which can be triggered
by various interactions [39], perceptions, or thoughts [40].
It encompasses a wide range of experiences, cognitive eval-
uations, behavioral responses, physiological reactions [41],
and communicative expressions. In the realm of human
cognition, emotions play a crucial role in decision-making
[42], shaping our perceptions, and guiding our interactions
with others [43].
As illustrated in Fig. 4, the study of emotions has re-
sulted in the development of different theoretical models,
which can be primarily categorized into discrete emotions
theory and multidimensional emotion theory [44]. In the
Fig. 4. Plutchik Wheel (left) and 2D Emotion Model (right).
(1) Plutchik Wheel is sourced from https://en.wikipedia.org/wiki/Robert_
Plutchik#/media/File:Plutchik-wheel.svg.
(2) Diagram of 2D emotion model is modified from [44].
TABLE 2
Differences between Our Work and Other Reviews.
ReferenceRelated to
generative modelsRelated to
emotion synthesisFocus on
multiple modalities
Our work âœ“ âœ“ âœ“
Kammoun et al. [52] âœ“*
Liu et al. [53] âœ“*
Triantafyllopoulos et al. [54] âœ“ âœ“
Wali et al. [55] âœ“* âœ“
Zhang et al. [56] âœ“ âœ“
De et al. [57] âœ“*
Hajarolasvadi et al. [9] âœ“* âœ“ âœ“
*: The article only relates to GAN.
most basic discrete emotion framework, emotions are sim-
ply categorized as positive or negative, also known as polar-
ity [45], [46]. Within this framework, the term ""emotion"" is
often replaced with ""sentiment,"" which sometimes includes
a neutral category as well. However, this sentiment cate-
gorization is considered too simplistic for certain contexts.
Therefore, the more detailed discrete emotion theory catego-
rizes basic emotions that are universally recognized across
cultures into six or eight types [47], [48], [49]. On the other
hand, the multidimensional emotions theory suggests that
emotions can be viewed along a continuous spectrum, often
defined by dimensions such as 2D (valence and arousal)
[50] or 3D (valence, arousal, and dominance) [51]. These
theoretical perspectives provide valuable insights into the
complex nature of human emotions and serve as founda-
tional principles for emotion synthesis.
By considering emotions in generative tasks, machines
not only understand and process information, but also
become attuned to the emotional dimensions of human
experience. This enriches the human-machine interaction
landscape and opens up new avenues for the development
of empathetic and intuitive AI developments.
4 D IFFERENCE
The current state of research in affective computing pri-
marily focuses on areas such as human sentiment analysis
[10], emotion detection, and emotion recognition [12]. These
tasks have a long-standing tradition and are considered
significant when viewed in a broader context. For instance,
Saxena et al. [58] conducted a survey on emotion recognition
methods, examining facial, physiological, speech, and text
approaches. They highlighted key techniques like Stationary
Wavelet Transform and Particle Swarm Optimization. In
another study, Nandwani et al. [59] analyzed sentiment
analysis and emotion detection methods, discussing theJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 5
transition from lexicon-based to deep learning techniques.
They addressed challenges and emphasized the need for
advanced and versatile models to improve accuracy and
adaptability across different domains and languages. Canal
et al. [60] presented a systematic literature review on fa-
cial emotion recognition from images. They categorized
the techniques into classical and neural network-based ap-
proaches, highlighting the slightly higher accuracy of clas-
sical methods compared to neural networks, despite the
latterâ€™s generalization capabilities.
Generative models currently constitute one of the main-
stream directions in artificial intelligence research. However,
most of the existing reviews only focus on the synthesis of a
single modality, such as facial images [52], [53], speech [54],
[55], and text [56], [57], and many of them ignore the emo-
tional aspects of the synthesis process. The most relevant
work to ours is [9]. This paper provides a thorough review
of GANs in synthesizing human emotions, with a focus
on facial expressions, speech, and cross-modal synthesis.
It details various GAN architectures, their applications in
emotion synthesis, challenges faced, and future directions.
By evaluating numerous studies, it highlights how GANs
enhance emotion recognition accuracy, offer data augmen-
tation, and create realistic, diverse emotional samples across
modalities. However, the key distinction between our sur-
vey and the aforementioned one lies in the fact that ours
exclusively focuses on emotion synthesis. In addition, this
review introduces a series of generation models other than
GANs, such as the Seq2Seq model in the field of emotional
speech synthesis, and LLMs in text emotion synthesis.
In summary, the differences are shown in Table 2. Our
survey is based on a clear definition of human emotion
synthesis, focusing on the application of generative models
in this emerging field, as well as the latest research develop-
ments in various sub-fields.
5 G ENERATIVE MODEL
The generative model [61] refers to a model that can be
described as generating data, belonging to a type of proba-
bility model. In machine learning, it can model data directly
or establish conditional probability distributions between
variables via Bayesâ€™ theorem, allowing the generation of
new data not present in the training set. In this section,
we explain the mathematics of different generative models,
including AE, GAN, DM, LLM, and Seq2Seq.
5.1 Auto-Encoder (AE)
AEs [17] are neural network models used in generative tasks
to efficiently learn data representations. These models com-
press input data into simplified patterns, then reconstruct
it by preserving key features while minimizing errors in
the reproduction process. As a variant of AE, Variational
Auto-Encoders (VAEs) introduced by Kingma and Welling
in 2013 [62], offer a principled approach to learning latent
data representations to account for data uncertainty and
variability, making them well-suited for generative tasks.
The training of VAEs is guided by the maximization of the
Evidence Lower BOund (ELBO), which can be expressed as
follows:
ELBO =EqÏ•(z|x)[logpÎ¸(x|z)]âˆ’DKL[qÏ•(z|x)âˆ¥p(z)],(1)where the encoder, qÏ•(z|x), maps the input data xto a dis-
tribution over the latent space characterized by parameters
Ï•. Typically, this distribution is assumed to be Gaussian,
with the encoder outputting the mean and variance of
the distribution. The latent variable z, sampled from this
distribution, is then fed into the decoder, pÎ¸(x|z), which
attempts to reconstruct the input data, where Î¸denotes the
parameters of the decoder. In the Equation (1), the first term
is the expected log-likelihood of the data given the latent
variables, encouraging accurate reconstruction of the data.
The second component measures how closely the encoded
data patterns match an expected statistical distribution p(z),
using the Kullback-Leibler divergence formula. By optimiz-
ing the ELBO, VAEs learn to balance the trade-off between
fidelity in data reconstruction and adherence to a structured
latent space.
5.2 Generative Adversarial Network (GAN)
GANs [19] are distinguished by their unique training
methodology, which leverages the concept of adversarial
learning, setting up a dynamic competition between two
distinct neural networks: the generator and the discrimina-
tor. The generator network, G, aims to map latent space
vectors, drawn from a prior distribution pz(z), to data
space, effectively generating new data samples that mimic
the distribution of real data, pdata(x). In contrast, the dis-
criminator network, D, is trained to distinguish between
samples drawn from the real data distribution and those
produced by the generator. The competition between net-
works steadily improves their performance, ultimately en-
abling the generator to create lifelike outputs. The training
of GANs is formulated as a min-max game, which can
be formally represented by the following value function
V(G, D):
min
Gmax
DV(D, G) =Exâˆ¼pdata (x)[logD(x)]+
Ezâˆ¼pz(z)[log(1 âˆ’D(G(z)))],(2)
where the first term represents the expected log-probability
that the discriminator correctly identifies real data sam-
ples as real. The second term represents the expected log-
probability that the discriminator correctly identifies fake
samples (generated by G) as fake. Training GANs involves
alternating between optimizing Dto maximize V(D, G)for
fixed G(improving Dâ€™s accuracy in distinguishing real from
fake samples) and optimizing Gto minimize V(D, G)for
fixed D(improving Gâ€™s ability to generate realistic samples).
This adversarial training process continues until a state of
equilibrium is reached where Ggenerates samples indis-
tinguishable from real data by D. The adversarial training
mechanism of GANs has proven to be highly effective for
generating complex, high-dimensional data.
5.3 Diffusion Model (DM)
DMs [21] have emerged as a class of powerful generative
models that synthesize data by gradually refining random
noise into structured patterns. The fundamental principle
behind DMs involves two phases: a forward (noising) phase
and a reverse (denoising) phase. In the forward phase, step
by step, the model gradually adds random noise to the dataJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 6
until the original information becomes completely obscured.
This process is described by a Markov chain that gradually
corrupts the original data distribution, x0, into a tractable
noise distribution, xT, over Tsteps. Mathematically, this
can be expressed through a sequence of conditional proba-
bilities:
q(xt|xtâˆ’1) =N(xt;p
1âˆ’Î²txtâˆ’1, Î²tI), (3)
where Î²trepresents the variance of the noise added at each
step, and Iis the identity matrix. The sequence of Î²tvalues
is predefined to control the noise level at each step, ensuring
a smooth transition from data to noise. The reverse phase
aims to learn the reverse process, modeling the conditional
distribution of xtâˆ’1given xt, effectively denoising the data.
The model, typically parameterized by a neural network, is
trained to approximate the reverse conditional probabilities:
pÎ¸(xtâˆ’1|xt) =N(xtâˆ’1;ÂµÎ¸(xt, t),Î£Î¸(xt, t)), (4)
where the parameters ÂµÎ¸and Î£Î¸are functions learned
during training, with Î¸representing the modelâ€™s parameters.
The model learns by minimizing the gap between real and
generated data distributions, using either statistical bounds
or direct probability optimization. Despite their computa-
tional intensity, DMs have emerged as a cornerstone in
generative modeling due to their impressive performance
in generating high-quality samples.
5.4 Large Language Model (LLM)
Large Language Models are AI systems trained on massive
text datasets, using billions of parameters to learn language
patterns. Their deep understanding of language enables
human-like text comprehension and generation, transform-
ing how computers process natural language. The advent
of Transformer architecture, developed by Vaswani et al. in
2017 [22], marked a significant breakthrough in language
modeling.
The training objective of LLMs can generally be de-
scribed as learning a conditional probability distribu-
tion over sequences. Given an input sequence X=
(x1, x2, . . . , x T), the model maximizes the likelihood of
generating each token based on prior tokens, represented
as:
p(X) =TY
t=1p(xt|x<t) (5)
where p(xt|x<t)is the probability of token xtgiven its con-
textx<t. A key feature of this process is the attention mech-
anism, which allows the model to dynamically focus on
different parts of the context at each step. In self-attention,
the probability of generating each token is influenced by
a weighted sum of the context, with attention weights Î±tj
computed as:
Î±tj=exp(qtÂ·kjâˆšdk)
PT
k=1exp(qtÂ·kkâˆšdk)(6)
and the new token representation ztis given by:
zt=TX
j=1Î±tjvj (7)By analyzing relationships between tokens, the attention
system helps the model understand context and produce
coherent, relevant text.
So far, several LLMs have gained prominence, includ-
ing Generative Pre-trained Transformer (GPT) [63] series,
Bidirectional Encoder Representations from Transformers
(BERT) [64], eXtreme Language Understanding Network
(XLNet) [65], and Text-to-Text Transfer Transformer (T5)
[66], etc. These models have been pre-trained on vast
amounts of text data and can effectively fine-tune for spe-
cific tasks, such as language translation, sentiment analysis,
and text generation [67], [68].
5.5 Sequence-to-Sequence (Seq2Seq) Model
Seq2Seq model [25] is a neural network architecture de-
signed for tasks involving sequential input-output pairs.
It follows an encoder-decoder structure, where each com-
ponent is typically implemented with recurrent neural net-
works (RNNs) or Transformers in later models.
One defining feature of Seq2Seq models is their focus
on sequential data, which differentiates them from models
like GANs or VAEs that do not inherently account for
sequential dependencies. Unlike general language models,
Seq2Seq models specialize in transforming one sequence
into another, effectively capturing both immediate and dis-
tant patterns in the data.
In the Seq2Seq model, the encoder processes the input
sequence (x1, x2, . . . , x T)to produce a context vector c,
often represented by the encoderâ€™s final hidden state:
c=hT (8)
where hTencapsulates the input sequenceâ€™s essential infor-
mation. The decoder then uses this context vector to gen-
erate the output sequence (y1, y2, . . . , y Tâ€²)one element at a
time, conditioned on cand previously generated outputs:
st=g(ytâˆ’1, stâˆ’1, c) (9)
p(yt|y<t, x) =softmax (Wst) (10)
where stis the hidden state at time t, and Wis a weight
matrix for calculating the output distribution.
Seq2Seq models are effective for handling variable-
length input and output sequences, making them well-
suited for applications like translation, summarization, and
question answering, where coherent sequence transforma-
tion is required. The differences between Seq2Seq models
and other generative models are shown in Table 3.
6 D ATABASES
The performance of human emotion synthesis tasks based
on generative models is closely tied to the quality and
richness of the utilized datasets. To be specific, the diver-
sity and scope of the datasets play a crucial role in the
modelâ€™s ability to generalize across various emotional states,
cultural contexts, and individual differences. The structure
and content of emotion databases directly shape how re-
searchers design and build emotion synthesis models. The
structure, annotation scheme, and inherent biases of the
datasets influence the choice of model architecture, loss
functions, and training strategies. Furthermore, the size andJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 7
TABLE 3
Relationship Between Seq2Seq and Other Generative Models.
Model TypeBelongs to
Seq2Seq?Description
LLMsPartially
belongsSome LLMs (e.g., T5, BART) are
implementations of Seq2Seq,
but LLMs cover a broader
range.
AEsDoes not
belongSimilar to Seq2Seq in architec-
ture, but with different goals
and tasks.
GANsDoes not
belongCompletely different model
type with unrelated goals and
structures.
DMsDoes not
belongEntirely different generative
models, unrelated to Seq2Seq.
quality of the datasets influence the choice between end-to-
end learning approaches and modular architectures, with
large, high-quality datasets enabling end-to-end learning of
emotion synthesis, while smaller or noisier datasets might
necessitate the use of pre-trained components or transfer
learning techniques. Based on these emotional datasets of
different modalities, such as facial images, speech, and text,
the designed models can imitate human emotional expres-
sions with high precision from different aspects. Table 4
summarizes the common datasets used in the field of human
emotion synthesis, providing a comprehensive overview of
the available resources for researchers and practitioners in
this domain.
7 F ACIAL EMOTION SYNTHESIS
Facial emotion synthesis is a crucial research field within
human emotion synthesis, aiming to generate faces that ex-
press specified emotions. This technology holds significant
academic value in computer graphics and computer vision,
while also demonstrating great promise for applications
in virtual reality (VR), gaming, and interactive computer
systems. Based on existing works, we can broadly categorize
facial emotion synthesis into three main approaches: face
reenactment (Section 7.1), talking head generation (Section
7.3), and facial manipulation (Section 7.2). The related works
are illustrated in Table 5.
7.1 Face Reenactment
Face reenactment focuses on transferring facial expressions
from a source actor to a target face, preserving the identity
of the target while adopting the emotional expressions of the
source. This technique is particularly useful in applications
like film dubbing, virtual avatars, and privacy-preserving
video conferencing.
In facial reenactment, there are some tasks that empha-
size emotional attributes in the face. For example, in [90],
Tripathy et al. introduced ICface, a GAN-based face anima-
tor that manipulated facial expressions in a given image.
The animation process was guided by interpretable control
signals, such as head pose angles and Action Units (AU)
values, which were derived from various sources, allowing
for selective emotion transfer. Zeng et al. [91] proposed
DAE-GAN, which employed two deforming autoencoders
to separate identity and pose in unlabeled videos, reducingthe need for manual annotation. It realized emotional trans-
fer between different identities with varied poses using con-
ditional generation and disentangled features. Strizhkova
et al. [92] proposed a novel method for emotion editing in
head reenactment videos by manipulating the latent space
of a pre-trained GAN. This technique disentangled emotion,
identity, and pose within the latent space, allowing for the
direct modification of emotions in the reenactment videos
without affecting the personâ€™s identity or the speech-related
facial expressions. Groth et al. [93] designed a new method
to achieve emotion mapping by generating correctly rec-
ognized expressions, using video reenactments to influence
the intensity of the emotion. In [94], Ali et al. utilized two
encoders to separately capture expression from a source and
identity from a target image, merging these features to cre-
ate expressive images, enhanced by innovative consistency
losses for both expression and identity features. In Fig. 5,
Xue et al. presented LSGAN [95], employing a transforma-
tive generator that combined target expression labels with
specific facial region features to produce clear and distinct
facial expressions in images. Shao et al. [96] utilized dual
parallel generators and wavelet-based discriminators for
facial expression translation, enhancing realism by focusing
on key areas with an attention mechanism and capturing
expression details across scales without the bidirectional
translation interference seen in single-generator models.
Fig. 5. A mask-based GAN [95] for face reenactment. The system
included four main components. A Semantic Mask Generator (SMG)
produced masks for specific facial regions (eyes, mouth, cheeks). Then
these masks were encoded into latent codes through an Adversarial
Autoencoder (AAE). A Transformative Generator (TG) used these codes
along with target expression labels to generate new facial expressions,
with an AU-intensity Discriminator (AUD) that assessed their quality and
intensity.
7.2 Face Manipulation
Face manipulation involves editing specific attributes of a
face, such as changing expressions, age, hairstyles, etc., to
generate different versions of the same person. It can be
seen that this has a completely different goal compared to
face reenactment which transfers the expressions and move-
ments of a source face to a target face. Researchers in this
field focus on the alteration of specific facial attributes while
preserving the remaining attributes unchanged, under the
condition of explicit predefined facial information, thereby
effecting changes in emotional expression. Works in this
field are mainly based on GAN and its variants [20], [30],
[97], [98], [99], [100], [101], [102], [103], [104], [105], [106]. ForJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 8
TABLE 4
Common Datasets for Human Emotion Synthesis with Generative Models.
Database Year Modalities Samples Subjects Category
Oulu-CASIA [69] 2011 visual 480 sequences 80 happiness, surprise, sadness, anger, fear, disgust
RaFD [70] 2010 visual 8040 images 49 sad, neutral, angry, contemptuous, disgusted, surprised, fearful, happy
CK+ [71] 2010 visual 593 images 123 happiness, sadness, surprise, fear, anger, disgust, neutral, contempt
CFEE [72] 2014 visual 229 images 230 happiness, surprise, sadness, anger, fear, disgust
AffectNet [73] 2017 visual 450,000 images / happiness, sadness, surprise, fear, anger, disgust, neutral, contempt
DISFA [74] 2013 visual 130,788 images 27 continuous annotation of graded changes in spontaneous facial expression of emotion
EmotioNet [75] 2016 visual 1,000,000 images / 23 basic or compound emotion categories(happy, sad, fearful, angrily surprised, sadly angry, etc.)
ESD [76] 2021 audio 350 utterances 20 happy, sad, neutral, angry, surprise
EmoV-DB [77] 2018 audio 7590 utterances 5 neutral, amused, angry, sleepy, disgust
Emo-DB [78] 2005 audio 800 sentences 10 neutral, anger, fear, joy, sadness, disgust, boredom
MEmoSD [79] 2019 audio / 4 angry, happy, neutral, sad
CaFE [80] 2018 audio 936 samples 12 neutral, sadness, happiness, anger, fear, disgust, surprise
KES [81] 2019 audio 21,000 speeches 1 neutral, happy, sad, angry, surprised, fearful, disgusted
ETOD [81] 2019 audio 6000 speeches 13 neutral, happy, sad, angry
YELP review [26] / text 6,990,280 reviews / positive, negative
AMAZON review [82] / text / / positive, negative
EmpatheticDialogue [27] 2019 text 24,850 conversations 810 32 emotion labels (suprised, excited, angry, proud, sad, annoyed, grateful, etc.)
MojiTalk [83] 2018 text 662,159 conversations / 64 emoji labels
MEAD [84] 2020 visual + audio 281,400 clips 60 angry, disgust, contempt, fear, happy, sad, surprise, neutral
CREMA-D [85] 2014 visual + audio 7442 utterances 91 happiness, surprise, sadness, anger, fear, disgust
EmoVoxCeleb [86] 2018 visual + audio 153,500 tracks 1251 neutral, happiness, surprise, sadness, anger, disgust, fear, contempt
SAVEE [87] 2008 visual + audio 480 utterances 4 neutral, anger, disgust, fear, happiness, sadness, surprise
RAVDESS [88] 2018 visual + audio 7356 videos 24 Happiness, Sadness, Surprise, Fear, Anger, Disgust, Neutral, Contempt
IEMOCAP [89] 2008 visual + audio + text 10,039 samples 10 categorical and continuous annotations
instance, in [30], Xie et al. proposed a novel approach, using
GAN equipped with consistency preservation and feature
entropy regularization techniques. This innovation achieved
improved results in attribute translation, particularly in
preserving consistency and reducing feature entropy. Song
et al. [107] utilized facial geometry to guide expression
creation but required a neutral face image, making the pro-
cess more complex. It consisted of two GANs for changing
and removing expressions. In [108], Kong te al. proposed
a dual-path GAN for emotion synthesis and introduced a
learning strategy based on a separation discriminator to
train it more efficiently. In [109], Tang et al. introduced
structured latent space and perceptual loss to achieve fine-
grained expression manipulation while preserving identity
and global facial shape. Patashnik et al. [110] innovatively
utilized text-driven manipulation techniques, in conjunction
with the extraordinary visual concept encoding abilities
of CLIP and StyleGAN, to specify desired attributes like
different emotional facial expressions. Liu et al. [111] first
proposed a novel general framework, EvoGAN, that com-
bined an evolutionary algorithm (EA) and GAN to work as
a whole, which generated face images with more compound
expressions. In [112], Tang et al. proposed a novel ECGAN
for generating faces with different emotions based on the
input expression attribute vector. And in [113], the innova-
tion in Sola et al.â€™s work was the use of ECGAN, a novel
expression-conditioned GAN, to incorporate expression in-
formation into unmasking processes, resulting in improved
naturalness and expression fidelity of generated faces. In
[114], Lindt et al. updated the Conditional Adversarial Au-
toencoder (CAAE) [115] framework to manipulate facial ex-
pressions in images based on continuous two-dimensional
emotion labels, representing valence and arousal.
7.3 Talking Head Generation
Talking head generation aims to create realistic, animated
facial models that can speak and emote based on input
audio or text. This approach is particularly valuable in creat-
ing virtual assistants, digital newscasters, and personalized
content for educational or entertainment purposes.
Fig. 6. A DM-based talking head generation model from [23]. This archi-
tecture started by extracting conditions from multiple modalities: audio,
visual, and textual inputs. Specifically, the emotion intensity block used
a pretrained CLIP text encoder to convert text prompts into embeddings
that represented the underlying emotional content. These embeddings,
reflecting nuanced emotional states and their strengths, were then inte-
grated into the DMâ€™s denoising process.
A series of works that incorporate emotion synthesis
into talking head generation have been explored. For ex-
ample, Eskimez et al. [116] presented a novel system for
generating talking faces, achieving independent control of
emotional expressions by disregarding the emotions ex-
pressed in the input speech audio and instead conditioning
the face generation on an independent categorical emotion
variable. In [117], Vougioukas et al. developed a special-
ized GAN that uses three discriminators to create detailed
facial expressions that match a speakerâ€™s emotional state.
As illustrated in Fig. 6, Zhang et al. presented EmoTalker
[23], a framework for emotionally editable talking face gen-
eration, utilizing a diffusion model and a novel Emotion
Intensity Block, and integrating a custom dataset to enhance
emotion interpretation. In [118], Zeng et al. presented ET-
GAN, an end-to-end system for generating talking faces
with tailored expressions from guiding videos, identity im-
ages, and arbitrary audio, utilizing multiple encoders for
identity, expression, and audio-lip synchronization, along-
side advanced frame and spatial-temporal discriminators.
Gan et al. [119] generated synchronized emotion coeffi-
cients and emotion-driven facial images through flow-based
and vector-quantized models, while using lightweight emo-
tion prompts or CLIP supervision for model control andJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 9
adaptation. In [120], Tan et al. used the regularized flow
model to generate emotion-related expression coefficients,
mapped the input emotion coefficients to the latent space,
and generated diverse expression dynamics based on audio
and emotion labels, while achieving high-fidelity restoration
of emotional details through a vector quantization image
generator. In another work [121], they used orthogonal
basis decomposition to decompose facial dynamics into
independent latent spaces of lip shape, head posture, and
emotional expression, allowing the model to adjust the
relevant expression coefficients in each independent latent
space.JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 10
TABLE 5
Literature on Generative Models for Face Reenactment, Face Manipulation, and Talking Head Generation, in Facial Emotion Synthesis.
Reference Year Model Dataset Performance
Ali et al. [94] 2019 TER-GAN Oulu-CASIA User study
Shao et al. [96] 2021 WP2-GAN RaFD/CFEE Accuracy: 89.47, 87.97/FID: 41.74, 24.91/SSIM: 0.6818, 0.6659
Tripathy et al. [90] 2020 GAN VoxCeleb Image Quality Assesment scores: 25.02, 33.08
Zeng et al. [91] 2020 DAE-GAN VoxCeleb1/RaFD SSIM: 0.65, 0.73/FID: 60.8, 13.8/User Study: 0.61
Strizhkova et al. [92] 2021 GAN MUG/MEAD ECS: 0.88, 0.58/FID: 19.7, 25.5/ACD: 0.10, 0.13
Groth et al. [93] 2020 Encoder-Decoder MoCap Ratings figure for perceived intensity and sincerity
Xue et al. [95] 2024 LSGAN RaFD/DISFA FID: 31.121/PSNR: 23.417/SSIM: 0.858
Hu et al. [122] 2023 2CET-GAN CFEE/RaFD FID: 31.1/IS: 1.55/Expression Transfer Score: 3.37
Choi et al. [123] 2018 StarGAN CelebA/RaFD Classification error: 2.12%
Liu et al. [111] 2022 EvoGAN EmotioNet/RaFD User study
Xie et al. [30] 2023 GAN FFHQ/RaFD/LFW FID: 8.32-16.05/Accuracy: 97.42-98.61
Tang et al. [109] 2020 EGGAN RaFD/MMI MSE: 0.24/PCC: 0.66
Patashnik et al. [110] 2021 StyleGAN FFHQ User study
Sola et al. [113] 2023 ECGAN RAFDB Accuracy: 78.54/SSIM: 0.52-0.64/FID: 244.75-668.68/Perceptual Loss: 1.21-1.46
Zhu et al. [124] 2019 UGAN CFEE FID: 44.8
Ding et al. [125] 2018 ExprGAN Oulu-CASIA User study
Tesei et al. [126] 2019 CCycleGAN FER2013 FID figure
Tang et al. [112] 2019 ECGAN AR/Yale/JAFFE/FERG/3DFE AMT Score: 35.32 VGG Score: 78.13, 80.32
Wang et al. [127] 2019 Comp-GAN CelebA/ F2ED Accuracy figure
Lindt et al. [114] 2019 CAAE AffectNet RMSE: 0.528, 0.607/SAGR: 0.567, 0.483/CCC: 0.312, 0.210
Kong et al. [108] 2021 DualPathGAN EmoVoxCeleb/VoxCeleb2 PSNR: 32.56/SSIM: 0.953/FID: 9.20
Doukas et al. [97] 2021 HeadGAN VoxCeleb FID: 50.9/FCD: 334/CSIM: 0.716
Zhao et al. [98] 2021 PAttGAN DISFA/DISFACat ICC: 0.89, 0.88/MAE: 0.24, 0.28/MSE: 0.22, 0.29/FID: 27.32, 21.77
Xia et al. [99] 2021 LGP-GAN RaFD IS: 1.31/FID: 11.88
Wang et al. [100] 2019 GAN EmotioNet/RaFD User study
Akram et al. [101] 2023 SARGAN KDEF/CFEE/RaFD ACD: 0.306/FVS: 94.18Â±1.11/FID: 53.95
Zhang et al. [102] 2021 SwitchGAN RaFD FID: 17.4/Accuracy: 98.32
Akram et al. [103] 2024 US-GAN KDEF/RaFD/CFEE ACD: 0.2832/FVS: 94.19Â±1.11/User study: 43% improvement(expression plausibility)
Azari et al. [104] 2024 StyleGAN CelebA/FFHQ LPIPS: 0.07/FID: 7.86/ID: 0.88
Apolito et al. [105] 2021 GAN AffectNet Smoothness score:0.33-0.38/ERE: 0.020, 0.018/FED: 0.71
Peng et al. [106] 2019 ApprGAN Bosphorus/CK+/MUG Correlation coefficients: 0.972, 0.953, 0.975/Normalised distances: 2.061, 1.879, 1.835
Pumarola et al. [20] 2018 GAN EmotioNet/RaFD User study
Ding et al. [125] 2018 ExprGAN Oulu-CASIA User study
Song et al. [107] 2018 G2-GAN CK+/Oulu-CASIA User study
Xu et al. [128] 2024 StyleGAN MEAD/RAVDESS Realism: 0.40/Emotion similarity: 0.36/Mouth shape similarity: 0.43
Eskimez et al. [116] 2021 GAN CREMA-D PSNR: 30.91/SSIM: 0.85/Accuracy: 55.3
Zhang et al. [23] 2024 Diffusion model MEAD/CREMA-D/FED CSIMD: 0.67, 0.51/Accuracy: 84.76, 75.13
Vougioukas et al. [117] 2020 GAN CREMA-D PSNR: 23.565/SSIM: 0.70/ACD: 1.40Â·10âˆ’4
Tan et al. [129] 2023 Encoder-Decoder CFD/MEAD/CREMA-D Accuracy: 65.20/PSNR: 29.38, 30.03/SSIM: 0.66, 0.68/M-LMD: 2.78, 3.03/F-LMD: 2.87, 3.16
Tan et al. [130] 2024 Encoder-Decoder MEAD SSIM: 0.795/FID: 23.207/M-LMD: 3.317/F-LMD: 2.696
Zhai et al. [131] 2023 Seq2Seq MEAD/CREMA-D SSIM: 0.82, 0.79/PSNR: 30.29, 30.55/M-LMD: 2.14, 1.16/LMD: 2.44, 1.46/FID: 19.59, 42.53/EP: 80.52, 85.64
Sheng et al. [132] 2023 DVAE MEAD/CREMA-D SSIM: 0.79, 0.92/LPIPS: 0.07, 0.08/LMD: 1.83, 1.34/LVD: 1.56, 0.84/CSIM: 0.88, 0.87/Emoacc: 0.87, 0.84
Gan et al. [119] 2023 Encoder-Decoder VoxCeleb2/MEAD PSNR: 21.75/SSIM: 0.68/FID: 19.69/M-LMD: 2.25/F-LMD: 2.47/Accuracy: 75.43
Tan et al. [120] 2024 Diffusion model HDTF/MEAD SSIM: 0.708, 0.689/FID: 15.165, 16.553/M-LMD: 1.643, 1.939/F-LMD: 1.958, 2.061/Accuracy: 71.53
Tan et al. [121] 2024 Encoder-Decoder HDTF/MEAD PSNR: 26.504, 22.771/SSIM: 0.845, 0.769/FID: 13.172, 15.548/M-LMD: 1.197, 1.102/F-LMD: 1.111, 1.060/Accuracy: 68.85
Ma et al. [133] 2023 Diffusion model MEAD/HDTF/Voxceleb2 SSIM: 0.86, 0.85, 0.69/CPBD: 0.16, 0.31, 0.30/F-LMD: 1.93, 1.80, 2.69/M-LMD: 2.91, 2.15, 2.72
Zhang et al. [134] 2023 Diffusion model MEAD/HDTF LPIPS: 0.169, 0.176/CPBD: 0.299, 0.280/F-LMD: 3.845, 3.948
Zeng et al. [118] 2020 ET-GAN CREMA-D/GRID PSNR: 23.981, 25.771/SSIM: 0.733, 0.810/FID: 76.92, 61.33
Sun et al. [135] 2023 StyleGAN MEAD/RAVDESS LIE: 0.739/CPBD: 0.247, 0.166/FED: 9.40/FID: 30.36/ID: 0.931/LSE-C: 7.11/LSE-D: 8.03
Wang et al. [136] 2024 Diffusion model MEAD/CREMA-D PSNR: 32.6131, 34.3401/CPBD: 0.3803, 0.5180/Emo-Acc: 74.57JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 11
8 S PEECH EMOTION SYNTHESIS
Speech emotion synthesis is a critical research field within
human emotion synthesis, focusing on the manipulation
and generation of acoustic features in speech signals to alter
or create specific emotional states. This area of study aims
to modify key vocal parameters such as volume, intonation,
pitch, speaking rate, and timbre to effectively convey a
desired emotional state in synthesized speech. Based on
existing works, we divide it into voice conversion (Section
8.1) ,text-to-speech (Section 8.2) and speech manipulation
(Section 8.3). Table 6 illustrates the overall papers about
speech emotion synthesis.
8.1 Voice Conversion
Voice conversion technology modifies a speakerâ€™s voice to
express different emotions while keeping the original words
intact. This enables emotional dubbing in films and games,
and helps create more natural-sounding AI assistants.
For instance, in [76], Zhou et al. leveraged VAW-GAN
and deep emotional features from speech emotion recog-
nition to describe emotional prosody. By using adjustable
emotional features to guide the decoder, this approach
could transform speech to express both familiar and new
emotions. In [137], they combined VAW-GAN and Con-
tinuous Wavelet Transform (CWT) for advanced spectrum
and prosody conversion. By incorporating CWT-based F0
modeling, the system uniquely enhanced the granularity
of prosody representation. In [138], they focused on disen-
tangling and re-composing emotional elements in speech,
innovatively employing CWT for detailed prosody analysis
and integrating F0 conditioning in the decoder to enhance
emotion conversion performance. Similarly, in [139], they
proposed a CycleGAN-based model [140] that did not
require parallel data. It utilized CWT to analyze F0 on
multiple scales, enabling detailed prosody modification. In
[141], Fu et al. also presented an improved CycleGAN-
based model that incorporated a transformer to augment
temporal dependencies and integrated curriculum learn-
ing and a fine-grained level discriminator, enhancing the
modelâ€™s ability to capture and convert emotional nuances
in speech more effectively. Rizos et al. [142] utilized class-
conditional GAN and an auxiliary domain classifier to gen-
erate emotional speech samples. In [143], Elgaar et al. in-
troduced a Factorized Hierarchical Variational Autoencoder
(FHVAE) for multi-speaker and multi-domain emotional
voice conversion, enhancing disentangled representation
and emotion conversion quality through novel algorithms
and loss functions. Gao et al. [144] used style transfer
autoencoders for emotional voice conversion without par-
allel data, leveraging disentangled representation learning
to modify emotion-related characteristics. In Fig. 7, Chen et
al. introduced a Tacotron2-based framework using emotion
disentangling modules [145] to achieve cross-speaker emo-
tion transfer by separating speaker identity from emotion. In
[146], Oh et al. proposed the DurFlex-EVC model, integrat-
ing a style autoencoder and unit aligner for advanced con-
trol and flexibility. It leveraged HuBERT features, denoising
diffusion models, and self-supervised learning to modify
emotional tones while maintaining linguistic content and
unique vocal traits. Moreover, there were also some worksrelated to emotional voice conversion based on StarGAN
[147], [148], [149], AE [150], and Seq2Seq [151], [152], [153].
Fig. 7. A two-stage model [145] for voice conversion. In the first stage,
the method separated emotional and content features from speech us-
ing an attention-based mechanism, incorporating inter-speech relation
to model emotional strength. The second stage employed a conversion
adaptation strategy, leveraging a multi-view consistency mechanism to
ensure that the emotional nuances were accurately transformed while
the core speech content remained intact. This framework facilitated
precise control over the emotional output of synthesized speech.
8.2 Text-to-Speech (TTS)
TTS aims to generate natural-sounding speech based on the
semantic content and context of the input text. This tech-
nology is particularly crucial for enabling more engaging
and human-like interactions with virtual assistants, audio-
book narrations, and personalized content delivery. Some
researchers have attempted to incorporate emotional factors
into TTS systems to synthesize speech with emotional ex-
pressiveness.
For example, in [154], Lei et al. introduced a unified
model for fine-grained emotional speech synthesis, obtain-
ing fine-grained emotion expressions with emotion descrip-
tors or phoneme-level manual labels. Li et al. [155] de-
veloped DiCLET-TTS, which improves emotional speech
synthesis by combining emotion separation techniques with
advanced probability-based decoding. In [156], Tits et al. ex-
plored adapting a Deep Convolutional TTS (DCTTS) model
to various emotions using minimal emotional data. In [157],
Schnell et al. leveraged WaveNet and emotion intensity
extraction using attention LSTM and transformer models,
demonstrating increased perceived emotion accuracy. Wu et
al [158] synthesized emotional speech with limited labeled
data, achieving comparable performance to fully supervised
models. In [159], Um et al. employed an inter-to-intra dis-
tance ratio algorithm and an effective interpolation tech-
nique to achieve nuanced emotion intensity control. In [81],
Im et al. proposed EmoQ-TTS, a system that synthesized
expressive emotional speech by conditioning phoneme-wise
emotion information with fine-grained emotion intensity,
using intensity pseudo-labels generated via distance-based
intensity quantization. Hortal et al. [160] combined Tacotron
2 with GANs to modulate prosody, allowing customiza-
tion of inferred speech with specified emotions. Guo et al.
[161] presented ""EmoDiff,"" a DM-based model that enabled
intensity-controllable emotional speech synthesis using a
soft-label guidance technique. Li et al. [162] utilized the
Tacotron framework, enhanced with emotion classifiers and
style loss, to generate expressive, controllable emotional
speech efficiently. In [163], Lei et al. introduced a novel
method integrating global-level, utterance-level, and local-
level modules to achieve precise emotion modeling andJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 12
transfer, allowing for versatile emotional expression in syn-
thesized speech. In Fig. 8, Li et al. created a system [164]
that extracts emotion patterns independent of the speakerâ€™s
voice, allowing emotions to be transferred between speakers
and adjusted in intensity. In [165], Li et al. developed a
technique that analyzes speech style at different scales,
capturing both broad and fine details to produce more con-
trollable and expressive synthetic voices. In [166], Tang et al.
introduced mix methods, enabling the manual combination
of noise at runtime to produce diverse emotional mixtures,
which was validated through evaluations demonstrating its
capability to generate speech with various mixed emotions.
Fig. 8. A Tacotron2-based TTS model from [164]. This system integrated
an Emotion Disentangling Module (EDM) that utilized dual encoders
to separate emotion-related features from speaker identity, refining the
emotion embedding for clearer expression without speaker leakage. An
identity controller maintained the target speakerâ€™s identity by incorporat-
ing speaker-specific information, enabling the synthesis of emotionally
expressive speech consistent with the target speakerâ€™s vocal character-
istics.
8.3 Speech Manipulation
Speech manipulation focuses on modifying various aspects
of speech signals, such as the speakerâ€™s identity or linguistic
content. It shares similarities with face manipulation in Sec-
tion 7.2, as both aim to alter or control specific attributes of
human-generated data, be it facial features or speech char-
acteristics. Recently, some researchers have begun exploring
how to manipulate the emotional attributes of speech.
For instance, in [79], Jia et al. presented ET-GAN, an in-
novative cross-language emotion transfer system for speech
synthesis, which was unique for not necessitating parallel
training data and utilized CycleGAN, achieving signifi-
cant improvements in emotional accuracy and naturalness
of synthetic speech. In [167], Matsumoto et al. utilized
WaveNet and auxiliary features like voiced, unvoiced, and
silence flags to generate speech-like emotional sounds with-
out linguistic content, enhancing emotional expressiveness
control. In [168], Wang et al. presented Emo-CampNet,
a text-based speech editing model. It integrated emotion
attributes and a context-aware mask prediction network,
employing generative adversarial training and data aug-
mentation to enhance emotional expressiveness and speaker
variability in edited speech. In [36], Inoue et al. introduced a
novel emotion editing technique in speech synthesis utiliz-
ing a hierarchical emotion distribution extractor within the
FastSpeech2 framework [169], enabling fine-grained, quan-
titative emotion control at phoneme, word, and utterance
levels for dynamic and nuanced speech generation.JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 13
TABLE 6
Literature on Generative Models for Voice Conversion, Text-to-Speech and Speech Manipulation, in Speech Emotion Synthesis.
Reference Year Model Dataset Performance
Zhou et al. [76] 2021 VAW-GAN ESD/IEMOCAP MCD: 4.127-4.916/MOS: 3.24, 2.94, 3.15/Preference test
Fu et al. [141] 2022 CycleGAN Japanese emotional speech dataset/ESD MOS figures/MCD: 5.65, 16.38/F0 RMSE: 44.91, 105.99
Zhou et al. [138] 2021 VAW-GAN EmoV-DB MCD: 4.085, 4.278/LSD: 5.681, 6.106/F0 RMSE: 61.712, 52.348/PCC: 0.893, 0.867/Preference test
Zhou et al. [139] 2020 CycleGAN Emotional speech corpus MCD: 10.23, 8.71/F0 RMSE: 65.05, 63.03/PCC: 0.76, 0.76/Preference test
Rizos et al. [142] 2020 StarGAN IEMOCAP Spectrogram representation/Human evalution score
Zhou et al. [137] 2020 VAW-GAN EmoV-DB/English emotional speech corpus/JL-Corpus MCD: 4.439, 4.683/LSD: 6.161, 6.275/PCC: 0.776, 0.691/MOS: 2.808
Gao et al. [144] 2018 AE IEMOCAP Emotion conversion MOS: 48%/Speaker similarity MOS: 3.55
Zhou et al. [151] 2022 Seq2Seq VCTK/ESD MOS figures/MCD: 4.13, 4.15, 4.25/DDUR: 0.24, 0.17, 0.31
Oh et al. [146] 2024 AE Diffusion model ESD nMOS: 3.70/sMOS: 3.63/eMOC: 72.97/UTMOS: 3.58/ECA: 91.58/SECS: 74.83
Elgaar et al. [143] 2020 VAE Collected dataset/IEMOCAP Subjective evalution figures
Kreuk et al. [152] 2022 Seq2Seq VCTK/EmoV MOS figures/eMOC figures
Du et al. [150] 2021 VAE ESD MCD figures/F0 RMSE figures/SV accuracy figures/MOS: 3.53, 3.58, 3.74/Preference test
Du et al. [147] 2021 StarGAN ESD MCD figures/MOS: 3.044/Preference test
Chen et al. [145] 2023 AE ESD MCD: 4.596/ACC: 0.830/RMSE: 0.117/Emotion similarity: 76.12%
Meftah et al. [148] 2023 StarGAN ESD MCD,F0 RMSE figures/Waveforms figures/Confusion matrices/Spectrograms figures
Shah et al. [149] 2023 StarGAN Hindi Emotional Speech Database MOS: 4.21Â±0.01/Similarity: 0.76/EmoAcc: 41.50/Preference test
Choi et al. [153] 2021 Seq2Seq Plain-to-emotional dataset MOS: 4.14/MCD: 5.778, 16.055/Emotion confusion matrices/Emotion similarity figures
Qi et al. [170] 2024 CVAE ESD MCD: 4.06/RMSE: 38.28/DDUR: 0.21/MSD: 19.61, 20.55, 21.54, 22.24
Zhang et al. [18] 2023 AE Multi-S60-E3/Child-S1-E6/VA-S2/Read-S40 MOS: 4.09Â±0.03, 3.76Â±0.05, 4.10Â±0.03 (Emotion Similarity,Speaker Similarity,Voice Quality)
Lei et al. [154] 2021 Seq2Seq Internal corpus MCD: 4.91, 5.03/F0 trajectories
Li et al. [155] 2023 Diffusion model 5 female monolingual speakers Emotion similarity DMOS: 3.90-4.04/Cosine Similarity: 0.25, 0.73
Tits et al. [156] 2020 Seq2Seq EmoV-DB MOS: 2.00, 2.10, 2.27, 3.59, 3.29 (amused, angry, disgusted, neutral, sleepy)
Schnell et al. [157] 2021 Seq2Seq SAVEE/IEMOCAP/WSJCAM0 Accuracy: 35.5, 28.9(total, emo)
Wu et al. [158] 2019 Seq2Seq Emotional speech corpus MCD: 2.64/F0 RMSE: 64.4/V/UV: 8.26/FFE: 21.81
Um et al. [159] 2020 Seq2Seq Korean male voice database MOS: 3.90Â±0.54/Recognition acurracy/Preference test
Im et al. [81] 2022 Seq2Seq KES/ETOD MOS: 3.72, 3.95/MCD: 4.81, 2.94/F0 RMSE: 53.15, 30.61/EmoAcc: 99.39, 86.85
Hortal et al. [160] 2021 Seq2Seq GAN LJ Speech/VESUS/CREMA-D/RAVDESS Accuracy
Guo et al. [161] 2023 Diffusion model ESD MOS: 4.13Â±0.10/MCD: 5.94/Classification accuracy/Preference test
Kang et al. [171] 2023 Diffusion model Multi-emotional dataset/ESD/LibriTTS Test ECA: 51.59, 38.89, 39.86, 32.57/MOS: 3.44, 3.31/SMOS: 3.22
Zhu et al. [172] 2019 Seq2Seq Emotional speech corpus Mel spectrograms/Pitch/PCA ordination diagram trajectories
Tang et al. [166] 2023 Diffusion model IEMOCAP/ESD MOS: 4.10, 3.92/SMOS: 4.02, 3.82/MCD: 5.29, 5.65
Lei et al. [173] 2022 GAN Internal corpus Emotion MOS: 4.04, 4.01, 3.86/Pearson Correlation: 0.776, 0.790, 0.759, 0.794/F0 curves
Lei et al. [163] 2022 Seq2Seq Emotional speech corpus MCD: 3.63/MOS: 4.02Â±0.119/CMOS: 0.520, 0.342/Preference: 58.1, 51.2/F0 curves
Li et al. [162] 2021 Seq2Seq Emotional speech corpus Peference test/Strength confusion matrices/pitch trajectories
Li et al. [164] 2022 Seq2Seq DB_1/AIC/DB_6 Emotion similarity DMOS: 3.71Â±0.066/Cosine simlarity: 0.28, 0.60
Li et al. [165] 2021 Seq2Seq Internal corpus MOS: 4.155Â±0.552, 4.136Â±0.701
Cai et al. [174] 2021 Seq2Seq IEMOCAP/BC2013-English/RECOLA MOS: 3.66 Accuracy: 78.75, 91.0
Wang et al. [175] 2023 Seq2Seq EmoV-DB MCD: 4.66/MOS: 3.76Â±0.03/Accuracy: 0.67, 0.67, 0.77/preference test
Zhou et al. [176] 2022 Seq2Seq VCTK/ESD MCD figures/PCC figures/MOS: 3.21-3.81/BWS test
Diatlova et al. [177] 2023 Seq2Seq ESD MOS: 4.37/NISQA: 4.1
Guan et al. [178] 2024 Seq2Seq MEAD-TTS MOS: 4.36, 3.95, 4.11, 3.77, 4.19, 3.93/SMOS: 4.61, 4.03 /MCD: 3.17, 6.69/EmoAcc: 0.659, 0.261, 0.636, 0.318
Zhou et al. [179] 2024 Encoder-Decoder ESD/LibriTTS MOS: 3.67/CMOS: 0.64, 0.71, 0.15, 0.92/preference test
Tang et al. [180] 2024 Diffusion model IEMOCAP MOS: 4.12/SMOS: 4.10/ERA: 0.749/EDER: 27.8
Jing et al. [181] 2024 Diffusion model ESD/MSP-Podcast MOS-Q: 3.88/MOS-S: 3.36, 3.22, 3.05, 3.34/preference test
Jia et al. [79] 2019 ET-GAN IEMOCAP/Emo-DB/CaFE/MEmoSD FAD,naturalness MOS figures/Preference test
Inoue et al. [36] 2024 Seq2Seq Blizzard/ESD MOS: 3.596Â±0.141/MCD: 4.348/Pitch Distortion: 1.151/Energy Distortion: 4.018/FD: 6.881/BWS test
Wang et al. [168] 2024 CampNet VCTK/ESD F0 curve/MCD: 3.078, 3.495, 3.528, 3.425, 3.332/Preference test/MOS figures
Matsumoto et al. [167] 2020 WaveNet JSUT corpus MOS figures/F0 distribution/Confusion matrices (Subject-perceived emotions)
Shi et al. [182] 2024 Encoder-Decoder ESD MOS: 3.98/SMOS: 4.15/EmoAcc: 99.31/MCD: 4.81JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 14
Fig. 9. A text emotion transfer model from [183]. During training, the
model restored a corrupted input by using a stable ""style vector"" derived
from the preceding sentence to condition the reconstruction process.
A new style vector allowed precise, targeted style modifications, or
""targeted restyling,"" during inference by adjusting the direction and
magnitude of the style shift relative to a baseline, using a few exemplar
sentences to define the desired output style.
9 T EXTUAL EMOTION SYNTHESIS
Textual emotion synthesis is a vital research field within
human emotion synthesis, focusing on the generation of
texts that possess specific emotional, sentiment, or em-
pathetic attributes. This area of study aims to create or
modify written content to convey desired emotional states,
sentiments, or empathetic responses, thereby enhancing the
expressiveness and impact of textual communication. Based
on existing works, we consider the following two types of
tasks: text emotion transfer (Section 9.1) and empathetic di-
alogue generation (Section 9.2). Table 7 shows the literature
about textual emotion synthesis.
9.1 Text Emotion Transfer
Text emotion transfer focuses on transforming the emotional
tone or sentiment of an existing text while preserving its
core semantic content. It allows for the modification of neu-
tral text into emotionally charged content, or the alteration
of one emotional state to another.
For example, in [184], Mohammadibaghmolaei et al. pro-
posed a text emotion transfer technique based on masked
language modeling and transfer learning, and a GPT-2
model underwent training to construct an initial sentence
based on its altered sequences, allowing the model to per-
form efficiently even with limited emotion-annotated data.
A novel text sentiment transfer methodology was proposed
by Li et al. [26] in which they employed a three-step
processâ€”Delete, Retrieve, and Generate. This approach,
powered by unsupervised learning and neural sequence-to-
sequence models, effectively altered sentiment while retain-
ing content. In [185], Jin et al. introduced the IMaT model
that constructed a pseudo-parallel corpus through semantic
alignment, then applied a sequence-to-sequence model for
attribute translation, refining this alignment across itera-
tions. Wu et al. [186] proposed a two-stage ""Mask and Infill""
methodology that significantly enhanced the performance
of non-parallel text sentiment transfer. Following the ""mask
and infill"" method, in [187], Malmi et al. introduced an
innovative unsupervised method using padded masked
language models (MLMs) for sentiment transfer, using a
padded MLM variant to avoid having to predetermine the
number of inserted tokens. In [188], Yang et al. presented a
technique leveraging language models as discriminators forunsupervised sentiment manipulation, enhancing stability
and content fidelity in generated text. In [189], Shen et al. de-
veloped a technique for text sentiment transfer without par-
allel data, utilizing refined alignment of latent representa-
tions, which effectively separated content from style, allow-
ing for sentiment modification by mapping sentences to a
style-independent content vector, then decoding this vector
into another style. Zhang et al. [190] employed GAN for sen-
timent transfer across different text domains, innovatively
combining adversarial and reinforcement learning with a
cross-domain sentiment transfer model, enhancing the abil-
ity to generate emotionally nuanced text while maintaining
domain-specific content. In Fig. 9, Riley et al. leveraged
T5 to extract a style vector from preceding sentences and
used it to provide extra conditioning for the decoder [183].
Huang et al. introduced a method [191] based on Cycle-
consistent Adversarial AutoEncoders, comprising LSTM au-
toencoders, adversarial style transfer networks, and cycle-
consistent constraints, innovating unsupervised text style
transfer. In [192], Luo et al. proposed the Seq2SentiSeq
model, incorporating sentiment intensity via a Gaussian
kernel in the decoder, enhancing sentiment control. They
trained the model using cycle reinforcement learning, which
maintains the original message while changing emotional
tone without requiring matched data pairs.
9.2 Empathetic Dialogue Generation
Empathetic dialogue generation is a crucial aspect of creat-
ing more human-like and emotionally intelligent conversa-
tional agents. It goes beyond simply generating contextually
relevant responses and focuses on incorporating emotional
understanding and support into the generated dialogue.
A lot of researchers focused on how to generate re-
sponses with specific emotional tendencies or more empa-
thy by LLMs. For example, in [193], Li et al. employed the
Emotional Chain-of-Thought (ECoT) technique, enhancing
Large Language Modelsâ€™ capability for nuanced emotional
text generation, focusing on human emotional intelligence
alignment. Yang et al. [194] provided a Hybrid Empathetic
Framework (HEF) that used SEMs as flexible enhancements
to LLMs, implementing a two-stage emotion prediction
strategy and an emotion cause perception strategy. In [195],
Sun et al. utilized the Chain of Emotion-aware Empathetic
prompting (CoNECT) for better context understanding and
emotional engagement. In [196], Lee et al. investigated GPT-
3â€™s capacity for empathetic dialogue generation, employ-
ing in-context learning in zero-shot and few-shot settings.
Casas et al. [24] introduced an empathic chatbot framework
utilizing transformer-based language models for generating
responses that recognized and adapted to the userâ€™s emo-
tional state. In [197], Chen et al. enhanced the empathetic
responses of ChatGLM-6B by fine-tuning it with a special-
ized dataset comprising over 2 million multi-turn empathy
conversation samples.
Apart from LLMs, there are some studies on emotional
or empathetic dialogue generation based on models like
Seq2Seq model and conditional variational autoencoder
(CVAE). For example, as shown in Fig. 10, Song et al. pro-
posed an emotional dialogue system, EmoDS [198], that en-
hanced a Seq2Seq framework with a lexicon-based attentionJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 15
Fig. 10. A empathetic dialogue generation system from [198]. The
system employed a bidirectional LSTM encoder to process the input text
into a vector representation, which initialized a decoder. This decoder,
guided by an emotion classifier and enriched with a lexicon-based
attention mechanism, integrated emotional lexicon words seamlessly
into the responses.
mechanism and an emotion classifier, generating dialogue
responses that expressed specified emotions, either explic-
itly or implicitly. In [199], Zhou et al. introduced an Emo-
tional Chatting Machine (ECM) that integrated internal and
external memory mechanisms along with emotion category
embeddings. In [200], Kong et al. proposed a model that
effectively combined CGANs with either standard Seq2Seq
or CVAE models to produce dialogue responses with the
specified sentiment. Li et al. [201] introduced a novel Dual-
View CVAE model that synthesized emotional dialogue,
changing the emotional expression of responses with higher
content relevance. In [202], Xu et al. proposed a framework
that incorporated multi-task learning and dual attention
mechanisms, effectively decoupling and processing content
and emotional information from the input. Asghar et al.
[203] suggested an enhanced Seq2Seq model that incorpo-
rated three emotional strategies for the input, training, and
inference processes, which was based on a designed dictio-
nary with Valence, Arousal, and Dominance (VAD) scores.
In [204], Colombo et al. employed a Seq2Seq architecture
augmented by emotion embeddings and a VAD lexicon
for word and sequence-level emotion modeling. It utilized
an affect regularizer to favor emotionally charged words
and an affect sampling method for generating emotionally
relevant diverse responses. Huang et al. [205] introduced a
novel approach to integrate emotions into dialogue genera-
tion by appending an emotion token to the dialogue input
or injecting the emotion directly into the decoder. In [206],
Lin et al. proposed a model integrating Transformer and
CVAE, with an emotion perception encoder and a BERT-
based emotion classification model to embed emotional
intelligence, enabling the generation of nuanced and con-
textually relevant empathetic responses.
In addition to the aforementioned methods, there have
been several research efforts exploring the use of GANs
to achieve empathetic dialogue generation. For example,
in [207], [208], Wang et al. developed SentiGAN, an inno-
vative structure featuring numerous generators alongsidea singular multi-class discriminator. This setup encouraged
each generator to concentrate on crafting text samples that
distinctly exhibited a designated sentiment label. Chen et
al. [209] utilized GAN with multiple classifiers to enhance
emotional dialogue production, with an emotion discrim-
inative model to align the generated dialogueâ€™s emotion
with the intended one. Bi et al. [210] utilized a diffusion
model-based approach to generate empathetic responses,
distinctive for its use of multi-grained control signals, incor-
porating communication mechanism, intent, and semantic
frame as control levels, enabling nuanced guidance over the
generated responses. In [211], Chen et al. proposed CTGAN
for emotional text generation by incorporating emotion la-
bels, allowing for the production of text that aligned with
specific emotional tones within variable-length outputs.JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 16
TABLE 7
Literature on Generative Models for Text Emotion Transfer, Empathetic Dialogue Generation, in Textual Emotion Synthesis.
Reference Year Model Dataset Performance
Mohammadibaghmolaei et al. [184] 2023 LLM ISEAR/TEC Transfer strength figures/Content preservation: 0.8403-0.8967/Fluency: 164.9118-220.4884
Li et al. [26] 2018 Seq2Seq YELP/CAPTIONS/AMAZON Human evalution/BLEU: 11.8, 17.1, 27.1/Classifier Accuracy: 95.4, 96.8, 70.3
Jin et al. [185] 2019 Seq2Seq YELP/FORMALITY Human evalution/Accuracy: 95.90, 72.07/BLEU: 22.46, 38.16/PPL: 14.89, 32.63
Wu et al. [186] 2019 LLM YELP/AMAZON Human evalution/ACC: 97.3, 84.5/BLEU: 15.9, 32.1
Malmi et al. [187] 2020 LaserTagger YELP BLEU: 14.5, 15.3/ACC: 40.9, 49.6
Yang et al. [188] 2018 Encoder-Decoder LM YELP ACC: 90.0, 85.4/BLEU: 22.3, 13.4/ PPLX: 48.4, 32.8/ PPLY: 61.6, 40.5
Shen et al. [189] 2017 AE YELP Accuracy:78.4/Human evaluation (sentiment: 62.6,fluency: 2.8,overall transfer quality: 41.5)
Zhang et al. [190] 2019 Seq2Seq GAN YELP/AMAZON Sentiment Transfer Strength: 81.7, 69.2/Cosine Similarity: 95.0, 92.2/Word Overlap: 83.9, 75.4
Luo et al. [192] 2019 Seq2Seq YELP BLEU: 32.5, 10.3/MAE: 0.13/MRRR: 0.78/PPL: 35.1/Avg human evaluation: 3.96
Reif et al. [212] 2022 LLM YELP/GYAFC Human evalution figures/ACC: 90.6/BLEU: 10.4/PPL: 79
Yi et al. [213] 2021 Encoder-Decoder YELP ACC: 90.8/BLEU: 26.3/Cos: 96/PPL: 109/GM: 14.83
Li et al. [214] 2020 Dual-Generator Network YELP/IMDb ACC: 88.0, 70.1/BLEU: 54.5, 70.2
Huang et al. [191] 2020 AAE YELP Tranfer: 86.9%/BLEU: 22.51/PPL: 21.6/RPPL: 57.0
Riley et al. [183] 2021 LLM AMAZON ACC: 73.7, 54.9/Content preservation: 34.7, 55.8/Sentiment: 2.5/Preservation: 2.6/Fluency: 4.0
Sancheti et al. [215] 2020 Seq2Seq YELP BLEU: 0.153,0.088/Accuracy: 0.922, 0.744/Human evaluation
Chawla et al. [216] 2020 Encoder-Decoder LM GYAFC/YELP/AMAZON ACC: 86.2, 68.9/BLEU: 14.1, 28.6
Li et al. [193] 2024 LLM IEMOCAP/DailyDialog/Empathetic-Dialogues/ESConv/PENS EGS: 36.02, 36.42, 36.21, 36.70, 33.48
Yang et al. [194] 2024 LLM EmpatheticDialogue dataset Acc: 45.63/Distinct-1: 42.23/Distinct-2: 80.08
Sun et al. [195] 2023 LLM EmpatheticDialogue dataset Human evaluation/PPL: 18.86/ACC: 53.44
Lee et al. [196] 2022 LLM EmpatheticDialogue dataset Human evaluation/EMOACC: 0.1683/Explorations: 0.4970/Interpretations: 0.2780
Casas et al. [24] 2021 LLM DD/ED dataset Emotion Reflection: 0.465/Emotional: 0.487/Empathy Score: 0.443/PPL: 149.8
Chen et al. [197] 2023 LLM SoulChat-Corpus/SMILECHAT BLEU: 33.78, 20.07,12.86, 8.52/ROUGE: 31.47, 8.92, 26.57/Emp: 1.84/Human evaluation
Liu et al. [217] 2022 LLM EmpatheticDialogues dataset Emotion accuracy: 0.5262/Perplexity: 13.57/Dist-1: 2.04/Dist-2: 11.68
Qian et al. [218] 2023 LLM EmpatheticDialogues dataset Dist-1: 2.96/Dist-2: 18.29/BERTscore: 0.8803, 0.8816, 0.8774/BLEU-2: 9.37/BLEU-4: 3.26/Human evaluation
Song et al. [198] 2019 Seq2Seq STC/NLPCC Embedding score: 0.634, 0.451, 0.435/BLUE: 1.73/Dist-1: 0.0113/Dist-2: 0.0867/emotion-a: 0.810/emotion-w: 0.687
Zhou et al. [199] 2018 Seq2Seq STC/NLPCC Perplexity: 61.8/Accuracy: 0.773/Human evalution
Kong et al. [200] 2019 CGAN Seq2Seq CAVE MojiTalk dataset PPL: 69.54/ Sentiment Acc: 78.8, 78.9/Quality: 3.9
Li et al. [201] 2021 CAVE NLPCC2017/Weibo dataset/MojiTalk dataset PPL: 25.70, 23.60, 33.7/Dist-1: 0.109, 0.017, 0.105/Dist-2: 0.400, 0.225, 0.517/Human evalution
Xu et al. [202] 2019 CVAE Seq2Seq NLPCC2017 PPL: 34.6/Accuracy: 0.637/Dist-1: 0.3315/Dist-2: 0.7900/Dist-3: 0.9023/Human evalution
Asghar et al. [203] 2018 Seq2Seq Cornell Movie Dialogs Corpus Syntactic Coherence: 1.76/Naturalness: 1.09/Emotional Appropriateness: 1.10
Colombo et al. [204] 2019 Seq2Seq OpenSubtitles2018/Cornell Dist-1: 0.0406, 0.0305/Dist-2: 0.2030, 0.1431/BLEU: 0.0140, 0.110/Hyper-parameter optimization
Huang et al. [205] 2018 Seq2Seq CBET/OpenSubtitles dataset Average Acc: 76.69, 75.91, 78.46
Lin et al. [206] 2022 CVAE EmpatheticDialogues dataset PPL: 19.6/Diversity: 0.0208, 0.1404, 0.3976/Human evaluation
Firdaus et al. [219] 2020 CVAE SEMD PPL: 34.8/Sentiment Acc:0.85/Emotion Acc: 0.80/Dist-1: 0.0203/Dist-2: 0.0520/Human evaluation
Majumder et al. [220] 2020 Encoder-Decoder EmpatheticDialogues dataset BLEU: 2.98/Human evaluation/Preference test
Sabour et al. [221] 2022 Encoder-Decoder EmpatheticDialogues dataset PPL: 35.60/Dist-1: 0.66/Dist-2: 2.99/Acc: 39.11/Human evaluation
Li et al. [222] 2019 Encoder-Decoder NLPCC2017 PPL: 62.2, 61, 61.4/Accuracy: 0.871, 0.870, 0.869/Human evaluation
Wang et al. [208] 2019 GAN MR/BR/CR/Emotional tweet conversation Sentiment Acc: 0.885, 0.841, 0.803/Novelty: 0.395, 0.427, 0.549/Diversity: 0.741, 0.713, 0.708/Human evaluation
Chen et al. [209] 2021 GAN NLPW/XHJ Acc: 0.701-0.870/Fluency score: 6.300-11.33/Human evaluation
Bi et al. [210] 2023 Diffusion model EmpatheticDialogues dataset BERTScore: 0.5205/MIScore: 626.92/Acc: 92.36, 84.24/F1-SF: 52.79/Dist-1, 2, 4: 2.84, 29.25, 73.45/Self-BLEU: 1.09
Li et al. [223] 2020 GAN Seq2Seq NLPCC2017 PPL: 62.8/Acc: 0.735/Distinct: 0.0287, 0.3062/Human evaluation
Chen et al. [211] 2020 GAN Yelp restaurant reviews/Amazon reviews/Film review/Obama speech Similarity: 0.1-0.2/Sentiment figures/Average Acc: 0.56
Gu et al. [224] 2024 LLM EmpatheticDialogues dataset Dist-1: 2.98/Dist-2: 18.38/B-2: 7.64/B-4: 2.57/PBERT: 87.29/RBERT: 88.04/FBERT:87.63/Human evaluation(Empaythy): 4.17
Chen et al. [225] 2024 LLM EmpatheticDialogues dataset Acc: 52.73/Dist-1: 2.96/Dist-2: 19.52/BLEU-2: 10.54/BLEU-4: 5.17JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 17
10 E VALUATION METRIC
In the field of human emotion synthesis, several commonly
used evaluation metrics are employed to assess the quality
of generated content across the three modalities of facial
images, speech, and text. These metrics provide a com-
prehensive assessment of the synthesized emotions from
various perspectives. While some evaluation indicators are
universal and applicable to all modalities, others are exclu-
sive to specific modalities, as illustrated in Table 8.
For common evaluation indicators, there are usually
two forms: user study and accuracy. User studies are a
qualitative evaluation method that assesses the quality of
generated content from aspects such as clarity, naturalness,
and emotional authenticity. These studies involve gathering
feedback and opinions from human participants to gauge
their perception and experience of the synthesized emo-
tions. For example, in tasks like face reenactment, where
emotional authenticity is critical, researchers conduct a ""Real
vs. Fake"" perceptual study on platforms like Amazon Me-
chanical Turk (AMT) to evaluate the outputs."" In the field
of TTS and voice conversion, researchers [146], [159], [166]
use subjective evaluation metrics like Mean Opinion Score
(MOS) and Similarity Mean Opinion Score (SMOS) to assess
naturalness and emotional similarity. Accuracy is another
important evaluation metric for human emotion synthesis.
For example, in face reenactment, researchers use classifiers
to assess generated facial images. Higher accuracy in ex-
pression recognition reflects higher accuracy in expression
translation by the models [96]. Similarly, [23] utilizes an
emotion classifier network from EVP [226] to measure the
emotion accuracy of face manipulation.
In addition to the aforementioned universal evaluation
metrics, different modalities in the field of emotion syn-
thesis have their own specific and widely used evaluation
indicators. These indicators target the unique attributes
and synthesis goals of each modality, providing a more
fine-grained and specialized assessment perspective. For
example, in facial emotion synthesis, PSNR (Peak Signal-
to-Noise Ratio) [107], [227] quantifies image quality by
comparing compressed images to their originals. It is cal-
culated using the logarithm of the ratio between maximum
pixel value and mean squared error, with higher values
indicating better quality. SSIM (Structural Similarity Index)
[96] improves upon PSNR by evaluating image similarity
based on luminance, contrast, and structure, focusing on
perceived quality and structural integrity. FID (FrÃ©chet In-
ception Distance) [91] measures the similarity between sets
of images by comparing feature vectors, with lower scores
indicating higher quality and diversity in generated images.
In speech emotion synthesis, MCD (Mel Cepstral Distortion)
[166], [228] objectively measures spectral similarity between
reference and generated mel-spectrograms, providing quan-
titative feedback on emotional voice synthesis accuracy. F0
RMSE (F0 Root Mean Square Error) [81], [158] evaluates
the accuracy of the fundamental frequency contour in syn-
thesized speech compared to the reference. Lower values
indicate higher pitch accuracy, contributing to perceived
naturalness and expressiveness. In textual emotion synthe-
sis, the BLEU (Bilingual Evaluation Understudy) score [26],
[186], borrowed from machine translation evaluation, canmeasure lexical similarity between generated and reference
texts, indicating how well the generated text maintains
desired linguistic properties while altering emotional tone.
PPL (Perplexity) [185] measures a language modelâ€™s predic-
tion accuracy, reflecting its ability to produce coherent and
fluent text. Lower PPL suggests the generated text more
closely mirrors human language patterns.
TABLE 8
Evaluation Metrics in Human Emotion Synthesis.
Modalities Metrics Description
CommonUser
studyAssesses the quality and realism of
synthesized emotion contents based on
scoring by human participants.
AccuracyMeasures the percentage of correct pre-
dictions in emotion classification tasks.
Facial
Emotion
SynthesisPSNRQuantifies the similarity between the
synthesized and original images, where
higher values indicate better quality.
SSIMAssesses the perceived visual quality
by comparing structural information be-
tween the original and generated im-
ages.
FIDEvaluates the distribution difference
between real and synthesized images
by comparing deep feature representa-
tions.
Speech
Emotion
SynthesisMCDMeasures the distortion between the
synthesized and reference speech in the
cepstral domain.
F0 RMSEQuantifies the difference in pitch be-
tween synthesized and real speech.
Textual
Emotion
SynthesisBLEUComputes the lexical similarity between
synthesized and reference text.
PPLMeasures the fluency of generated text,
with lower values indicating more pre-
dictable and coherent sentences.
11 D ISCUSSION
11.1 Major Findings
Existing methods in generative technology for human emo-
tion synthesis have made substantial progress across mul-
tiple modalities, including facial images, speech, and text.
Each modality benefits from distinct approaches and these
developments not only enhance the perceived emotional
intelligence of systems but also push the boundaries of how
machines can generate and interpret nuanced emotional
states.
In facial emotion synthesis, GAN-based methods used to
be the mainstream methods. However, in recent years, DM-
based methods have also gained considerable attention and
made significant progress. Specifically, some classical GAN-
based models, such as StarGAN and StyleGAN, excel at al-
tering the emotional expressions of faces but face challenges
when attempting to generate more subtle or complex emo-
tional states. Moreover, generating mixed emotionsâ€”that
is, facial expressions that convey a strong contrast of emo-
tions like happiness and sadnessâ€”still lacks finer control.
Comparatively, DM generates images through progressive
denoising, which avoids the problem of model collapse that
GANs may have when generating facial expressions. It alsoJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 18
shows strong adaptability in generating mixed emotions,
better capturing subtle expression changes and emotional
nuances, and can handle multiple combinations of emotions
in facial expressions.
Speech emotion synthesis achieved similar progress
through the adaptation of GANs and Seq2Seqs like Tacotron
[229], where emotional intonation is introduced into syn-
thesized speech. These models produce more natural and
expressive speech by adjusting vocal elements like pitch,
rhythm, and tone. However, recent research has also incor-
porated AEs, and DMs to further improve the emotional
depth and expressiveness of synthesized speech. Specifi-
cally, AEs are used to disentangle emotional features from
speaker identity, enabling more flexible emotion transfer
while preserving the naturalness of speech. DMs, with
their capacity for modeling complex data distributions, offer
promising results in generating emotional speech with high
fidelity and more controlled variations in prosody.
Textual emotion synthesis has increasingly leveraged
LLMs and Seq2Seq architectures, utilizing mechanisms like
sentiment control and emotional valence modulation to pro-
duce emotionally resonant content [230]. These systems are
often employed in applications such as empathetic chatbots
and emotionally responsive dialogue systems. Despite their
effectiveness, generating responses with emotional depth
that appropriately reflect varying levels of empathy, sym-
pathy, or other complex emotional tones based on user
inputs remains a challenge. Current models still struggle
with maintaining a balance between emotional expressive-
ness and conversational coherence, especially in response to
ambiguous or contextually nuanced inputs. Moreover, un-
derstanding the contextual triggers for emotional responses
and integrating them effectively into generative models will
be an ongoing research area.
In terms of evaluation metrics, quality assessment re-
mains a complex, multidimensional task that integrates
both subjective and objective indicators. Subjective metrics
typically involve human evaluation, which focus on cap-
turing the emotional authenticity, resonance, and overall
impression of the generated content. However, subjective
evaluations can be time-consuming and costly, and they are
influenced by individual biases and cultural differences. On
the other hand, objective metrics aim to quantify various
aspects of emotion synthesis using automated computa-
tional methods, such as accuracy, PSNR, etc. They provide
a scalable and reproducible way of assessment but may not
fully capture the emotional nuances and human-perceived
quality of the generated content. Future research directions
include developing more fine-grained subjective evaluation
methods to better capture the subtle differences in emotional
content and the complexity of human responses. At the
same time, improving objective metrics to more accurately
quantify various aspects of emotional expression and cor-
relating them with human judgments is also an important
area of research.
11.2 Future Perspectives
Advances in generative AI have revolutionized how we
simulate human emotions, creating more authentic and
nuanced emotional expressions. Looking ahead, several
promising avenues for further research can be explored:Firstly, combining the capabilities of different generative
models such as GANs, Seq2Seqs, AEs, DMs, and LLMs
holds promise for further enhancing the quality of gen-
erated outputs. Each model has its unique strengths and
limitations, and intelligently integrating them can compen-
sate for the shortcomings of individual models, enabling
more accurate and realistic emotion synthesis. By designing
innovative hybrid architectures that leverage the strengths
of each model, more powerful and comprehensive emotion
synthesis systems can be developed. These hybrid models
can seamlessly transition between different modalities, gen-
erating emotionally consistent and complementary outputs.
For instance, a model combining DMs with the insights of
Seq2Seq can generate high-quality emotional speech with
appropriate facial expressions and lip synchronization.
Secondly, the horizon of emotion synthesis is not limited
to common modalities like facial images, speech, and text.
As generative models continue to evolve, we may see mul-
timodal human emotion synthesis results [231], [232] in the
future, including modalities beyond those mentioned in this
paper, such as gesture [233] and physiological signals like
EEG and ECG. In addition, emerging cross-modal genera-
tive models, such as text-to-image (T2I), text-to-video (T2V),
and even text-to-3D, are poised to expand the creative and
interactive potential of emotion synthesis. T2I models can
generate imagery that resonates emotionally with written
narratives, producing visuals that reflect subtle emotional
undertones, while T2V models can bring stories to life
by translating emotional content into animated, visually
expressive scenes that engage audiences on a deeper emo-
tional level. Moreover, as the technology matures, the poten-
tial for converting image-based emotions into sound (e.g.,
generating soundscapes that mirror the mood of a visual
scene) opens up new dimensions for immersive experiences
in fields like VR [234] and interactive entertainment.
Thirdly, the integration of generative models with edge
devicesâ€”from server-based processing to smart termi-
nalsâ€”marks a pivotal shift in the accessibility and appli-
cation of emotion synthesis [235]. As the computational
power of edge devices continues to grow, there is a grow-
ing potential for real-time emotion generation and synthe-
sis directly on devices such as smartphones, wearables,
and VR headsets. This transition from centralized server
processing to edge computing opens up a wide range of
applications, enabling personalized, on-the-fly emotional
interactions. For instance, smart devices can analyze usersâ€™
facial expressions, voice tone, or even physiological sig-
nals, generating responsive emotional content that adapts
to the userâ€™s immediate emotional state, location, or context.
Additionally, as AI models become more efficient, smaller-
scale devices like wearables or even IoT sensors [236] can
incorporate emotion-aware interactions, enhancing user ex-
perience in a range of industriesâ€”from healthcare, where
emotion synthesis can assist in mental health monitoring
and intervention, to retail, where it can personalize con-
sumer experiences in-store or online. In this new paradigm,
generative models are poised to provide immediate, adap-
tive emotional content, offering users a deeper connection
to the digital world.
Fourthly, human emotion synthesis holds the poten-
tial to drive profound transformations across a variety ofJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 19
industries and can revolutionize domains such as digital
entertainment and filmmaking [237], [238]. In the realm of
digital entertainment, emotion synthesis lays the foundation
for highly immersive experiences, enabling virtual charac-
ters and environments to express emotions with a level
of authenticity rivaling that of human actors. By precisely
generating emotional nuances in facial expressions, vocal
tones, and body language, these technologies can elevate
interactive media to new heights. In film production, AI-
driven tools are already being employed to infuse emotional
depth into character performances, achieving more dynamic
and expressive storytelling that transcends the limitations
of traditional physical acting. Moreover, the combination
of AIGC video and emotion synthesis opens up new av-
enues for creation, empowering filmmakers to craft content
with emotionally evocative visual and auditory cues. This
presents opportunities for real-time emotional adjustments
within films, where charactersâ€™ emotional arcs can be dy-
namically altered based on audience feedback or narrative
shifts, further immersing viewers in the experience [239].
Through these advancements, the industry is entering a
new chapter where the integration of emotion synthesis will
unleash vast creative potential.
12 C ONCLUSION
This review presents a detailed investigation of current
generative technology for human emotion synthesis across
various modalities, including facial images, speech, and
text. It reveals how different genrative models, ranging
from well-established approaches like AEs and GANs to
emerging techniques such as DMs and LLMs, are capable of
generating complex emotional expressions with remarkable
depth and subtle nuances.
In Section 1, we introduce the background of human
emotion synthesis, a pivotal area of research within affective
computing. With the growing sophistication of generative
models, which possess advanced data modeling and multi-
modal generation capabilities, new avenues for emotion
synthesis are emerging. However, there is currently a lack
of comprehensive reviews on this subject. To address this
gap, we present the first survey of generative technology for
human emotion synthesis, building upon existing literature
and filling the void in current research. In Section 2, we
outline our rigorous literature screening strategy, which
allowed us to systematically collect and classify key research
in the field of generative models for emotion synthesis. In
Sections 3 to 6, we highlight the gaps between previous
reviews and our survey, demonstrating the novelty and sig-
nificance of our contribution. We also provide an overview
of emotion models and mathematics of generative models,
as well as commonly used datasets, offering a deeper under-
standing of the latest advancements in this interdisciplinary
field.
Sections 7 to 9 provide a detailed discussion of the latest
research on human emotion synthesis based on facial im-
ages, speech, and text. We categorize the specific tasks under
each modality, discuss the applications of different gener-
ative models, and summarize the performance of existing
works in comprehensive tables. We classify specific tasks
within each modality, analyze the distribution of differentmodels across these tasks, and discuss their strengths and
limitations in various contexts. For each modality, we sum-
marize the application models and performance of existing
works in comprehensive tables. Finally, in Sections 10 and
11, we summarize the commonly used evaluation metrics
for emotion synthesis and discuss the current state and
future development trends in this field, providing insights
into the challenges and opportunities faced by emotion
synthesis.
In summary, this review highlights the transformative
potential of existing generative models in shaping the future
of human emotion synthesis. These advancements will not
only enhance the granularity and authenticity of synthe-
sized emotions but also usher in a new era where machines
can resonate with the subtleties of human emotions, foster-
ing deeper and more empathetic connections.
REFERENCES
[1] R. W. Picard, Affective computing . MIT press, 2000.
[2] F. Ding, X. Kang, and F. Ren, â€œNeuro or symbolic? fine-tuned
transformer with unsupervised lda topic clustering for text senti-
ment analysis,â€ IEEE Transactions on Affective Computing , vol. 15,
no. 2, pp. 493â€“507, 2024.
[3] E. Yadegaridehkordi, N. F. B. M. Noor, M. N. B. Ayub, H. B.
Affal, and N. B. Hussin, â€œAffective computing in education: A
systematic review and future research,â€ Computers & education ,
vol. 142, p. 103649, 2019.
[4] F. Ren, Z. Liu, and X. Kang, â€œAn efficient framework for con-
structing speech emotion corpus based on integrated active learn-
ing strategies,â€ IEEE Transactions on Affective Computing , vol. 13,
no. 4, pp. 1929â€“1940, 2022.
[5] J. Deng and F. Ren, â€œA survey of textual emotion recognition and
its challenges,â€ IEEE Transactions on Affective Computing , vol. 14,
no. 1, pp. 49â€“67, 2023.
[6] Y. Zhou, X. Kang, and F. Ren, â€œPrompt consistency for multi-
label textual emotion detection,â€ IEEE Transactions on Affective
Computing , vol. 15, no. 1, pp. 121â€“129, 2024.
[7] M. SchrÃ¶der, â€œEmotional speech synthesis: A review,â€ in Seventh
European Conference on Speech Communication and Technology , 2001.
[8] K. Tokuda, Y. Nankaku, T. Toda, H. Zen, J. Yamagishi, and
K. Oura, â€œSpeech synthesis based on hidden markov models,â€
Proceedings of the IEEE , vol. 101, no. 5, pp. 1234â€“1252, 2013.
[9] N. Hajarolasvadi, M. A. Ramirez, W. Beccaro, and H. Demirel,
â€œGenerative adversarial networks in human emotion synthesis:
A review,â€ IEEE Access , vol. 8, pp. 218 499â€“218 529, 2020.
[10] S. Poria, E. Cambria, R. Bajpai, and A. Hussain, â€œA review
of affective computing: From unimodal analysis to multimodal
fusion,â€ Information fusion , vol. 37, pp. 98â€“125, 2017.
[11] Y. Cao, S. Li, Y. Liu, Z. Yan, Y. Dai, P . S. Yu, and L. Sun, â€œA compre-
hensive survey of ai-generated content (aigc): A history of gen-
erative ai from gan to chatgpt,â€ arXiv preprint arXiv:2303.04226 ,
2023.
[12] F. Ma, Y. Yuan, Y. Xie, H. Ren, I. Liu, Y. He, F. Ren, F. R. Yu, and
S. Ni, â€œGenerative technology for human emotion recognition: A
scoping review,â€ Information Fusion , p. 102753, 2024.
[13] L. Ruthotto and E. Haber, â€œAn introduction to deep generative
modeling,â€ GAMM-Mitteilungen , vol. 44, no. 2, p. e202100008,
2021.
[14] A. Gillioz, J. Casas, E. Mugellini, and O. A. Khaled, â€œOverview
of the transformer-based models for nlp tasks,â€ in 2020 15th
Conference on Computer Science and Information Systems (FedCSIS) .
IEEE, 2020, pp. 179â€“183.
[15] S. Feuerriegel, J. Hartmann, C. Janiesch, and P . Zschech, â€œGenera-
tive ai,â€ Business & Information Systems Engineering , vol. 66, no. 1,
pp. 111â€“126, 2024.
[16] Y. Xie, J. Wang, T. Feng, F. Ma, and Y. Li, â€œCcis-diff: A generative
model with stable diffusion prior for controlled colonoscopy
image synthesis,â€ arXiv preprint arXiv:2411.12198 , 2024.
[17] G. E. Hinton and R. R. Salakhutdinov, â€œReducing the dimension-
ality of data with neural networks,â€ Science , vol. 313, no. 5786,
pp. 504â€“507, 2006.JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 20
[18] G. Zhang, Y. Qin, W. Zhang, J. Wu, M. Li, Y. Gai, F. Jiang, and
T. Lee, â€œiemotts: Toward robust cross-speaker emotion transfer
and control for speech synthesis based on disentanglement be-
tween prosody and timbre,â€ IEEE/ACM Transactions on Audio,
Speech, and Language Processing , 2023.
[19] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-
Farley, S. Ozair, A. Courville, and Y. Bengio, â€œGenerative adver-
sarial nets,â€ in Advances in neural information processing systems ,
vol. 27, 2014.
[20] A. Pumarola, A. Agudo, A. M. Martinez, A. Sanfeliu, and
F. Moreno-Noguer, â€œGanimation: Anatomically-aware facial an-
imation from a single image,â€ in Proceedings of the European
Conference on Computer Vision (ECCV) , 2018, pp. 818â€“833.
[21] J. Ho, A. Jain, and P . Abbeel, â€œDenoising diffusion probabilistic
models,â€ in Advances in neural information processing systems ,
vol. 33, 2020, pp. 6840â€“6851.
[22] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.
Gomez, Å. Kaiser, and I. Polosukhin, â€œAttention is all you need,â€
inAdvances in neural information processing systems , vol. 30, 2017.
[23] B. Zhang, X. Zhang, N. Cheng, J. Yu, J. Xiao, and J. Wang,
â€œEmotalker: Emotionally editable talking face generation via
diffusion model,â€ arXiv preprint arXiv:2401.08049 , 2024.
[24] J. Casas, T. Spring, K. Daher, E. Mugellini, O. A. Khaled, and
P . CudrÃ©-Mauroux, â€œEnhancing conversational agents with em-
pathic abilities,â€ in Proceedings of the 21st ACM International
Conference on Intelligent Virtual Agents , 2021, pp. 41â€“47.
[25] I. Sutskever, O. Vinyals, and Q. V . Le, â€œSequence to sequence
learning with neural networks,â€ in Advances in neural information
processing systems , vol. 27, 2014.
[26] J. Li, R. Jia, H. He, and P . Liang, â€œDelete, retrieve, generate: A
simple approach to sentiment and style transfer,â€ in Proceedings of
the 2018 Conference of the North American Chapter of the Association
for Computational Linguistics: Human Language Technologies, Volume
1 (Long Papers) , 2018, pp. 1865â€“1874.
[27] H. Rashkin, E. M. Smith, M. Li, and Y.-L. Boureau, â€œTowards
empathetic open-domain conversation models: A new bench-
mark and dataset,â€ in Proceedings of the 57th Annual Meeting of
the Association for Computational Linguistics , 2019, pp. 5370â€“5381.
[28] S. Dhere, S. B. Rathod, S. Aarankalle, Y. Lad, and M. Gandhi,
â€œA review on face reenactment techniques,â€ in 2020 International
Conference on Industry 4.0 Technology (I4Tech) . IEEE, 2020, pp.
191â€“194.
[29] X. Luo, X. Zhang, Y. Xie, X. Tong, W. Yu, H. Chang, F. Ma,
and F. R. Yu, â€œCodeswap: Symmetrically face swapping based
on prior codebook,â€ in Proceedings of the 32nd ACM International
Conference on Multimedia , 2024, pp. 6910â€“6919.
[30] W. Xie, W. Lu, Z. Peng, and L. Shen, â€œConsistency preservation
and feature entropy regularization for gan-based face editing,â€
IEEE Transactions on Multimedia , 2023.
[31] R. Zhen, W. Song, Q. He, J. Cao, L. Shi, and J. Luo, â€œHuman-
computer interaction system: A survey of talking-head genera-
tion,â€ Electronics , vol. 12, no. 1, p. 218, 2023.
[32] M. Toshpulatov, W. Lee, and S. Lee, â€œTalking human face gen-
eration: A survey,â€ Expert Systems with Applications , vol. 219, p.
119678, 2023.
[33] B. Sisman, J. Yamagishi, S. King, and H. Li, â€œAn overview of voice
conversion and its challenges: From statistical modeling to deep
learning,â€ IEEE/ACM Transactions on Audio, Speech, and Language
Processing , vol. 29, pp. 132â€“157, 2020.
[34] K. Zhou, B. Sisman, R. Liu, and H. Li, â€œEmotional voice conver-
sion: Theory, databases and esd,â€ Speech Communication , vol. 137,
pp. 1â€“18, 2022.
[35] N. Kaur and P . Singh, â€œConventional and contemporary ap-
proaches used in text to speech synthesis: a review,â€ Artificial
Intelligence Review , vol. 56, pp. 5837â€“5880, 2023.
[36] S. Inoue, K. Zhou, S. Wang, and H. Li, â€œFine-grained quan-
titative emotion editing for speech generation,â€ arXiv preprint
arXiv:2403.02002 , 2024.
[37] W.-L. Zheng and B.-L. Lu, â€œInvestigating critical frequency bands
and channels for eeg-based emotion recognition with deep neural
networks,â€ IEEE Transactions on Autonomous Mental Development ,
vol. 7, no. 3, pp. 162â€“175, 2015.
[38] D. B. Geselowitz, â€œOn the theory of the electrocardiogram,â€
Proceedings of the IEEE , vol. 77, no. 6, pp. 857â€“876, 1989.
[39] S. Brave and C. Nass, â€œEmotion in human-computer interaction,â€
inThe human-computer interaction handbook . CRC Press, 2007, pp.
103â€“118.[40] D. Keltner and J. J. Gross, â€œFunctional accounts of emotions,â€
Cognition & Emotion , vol. 13, no. 5, pp. 467â€“480, 1999.
[41] I. B. Mauss and M. D. Robinson, â€œMeasures of emotion: A
reviews,â€ Cognition and emotion , pp. 109â€“137, 2010.
[42] J. S. Lerner, Y. Li, P . Valdesolo, and K. S. Kassam, â€œEmotion and
decision making,â€ Annual review of psychology , vol. 66, pp. 799â€“
823, 2015.
[43] B. Parkinson, A. H. Fischer, and A. S. Manstead, Emotion in social
relations: Cultural, group, and interpersonal processes . Psychology
press, 2005.
[44] S. K. Khare, V . Blanes-Vidal, E. S. Nadimi, and U. R. Acharya,
â€œEmotion recognition and artificial intelligence: A systematic
review (2014â€“2023) and research recommendations,â€ Information
Fusion , vol. 102019, 2023.
[45] S. Zhao, G. Ding, J. Han, and Y. Gao, â€œPersonality-aware person-
alized emotion recognition from physiological signals,â€ in IJCAI ,
2018, pp. 1660â€“1667.
[46] S. Zhao, A. Gholaminejad, G. Ding, Y. Gao, J. Han, and
K. Keutzer, â€œPersonalized emotion recognition by personality-
aware high-order learning of physiological signals,â€ ACM Trans-
actions on Multimedia Computing, Communications, and Applications
(TOMM) , vol. 15, no. 1s, pp. 1â€“18, 2019.
[47] P . Ekman, â€œAn argument for basic emotions,â€ Cognition & emo-
tion, vol. 6, no. 3-4, pp. 169â€“200, 1992.
[48] R. Plutchik and H. Kellerman, Theories of emotion . Academic
press, 2013, vol. 1.
[49] F. Ma, Y. Li, S. Ni, S.-L. Huang, and L. Zhang, â€œData aug-
mentation for audio-visual emotion recognition with an efficient
multimodal conditional gan,â€ Applied Sciences , vol. 12, no. 1, p.
527, 2022.
[50] G. F. Wilson and C. A. Russell, â€œReal-time assessment of men-
tal workload using psychophysiological measures and artificial
neural networks,â€ Human factors , vol. 45, no. 4, pp. 635â€“644, 2003.
[51] A. Mehrabian, â€œPleasure-arousal-dominance: A general frame-
work for describing and measuring individual differences in
temperament,â€ Current Psychology , vol. 14, pp. 261â€“292, 1996.
[52] A. Kammoun, R. Slama, H. Tabia, T. Ouni, and M. Abid, â€œGener-
ative adversarial networks for face generation: A survey,â€ ACM
Computing Surveys , vol. 55, no. 5, pp. 1â€“37, 2022.
[53] Y. Liu, Q. Li, Q. Deng, Z. Sun, and M.-H. Yang, â€œGan-based facial
attribute manipulation,â€ IEEE Transactions on Pattern Analysis and
Machine Intelligence , 2023.
[54] A. Triantafyllopoulos, B. W. Schuller, G. Ë™Iymen, M. Sezgin, X. He,
Z. Yang, P . Tzirakis, S. Liu, S. Mertes, E. AndrÃ© et al. , â€œAn
overview of affective speech synthesis and conversion in the deep
learning era,â€ Proceedings of the IEEE , 2023.
[55] A. Wali, Z. Alamgir, S. Karim, A. Fawaz, M. B. Ali, M. Adan,
and M. Mujtaba, â€œGenerative adversarial networks for speech
processing: A review,â€ Computer Speech & Language , vol. 72, p.
101308, 2022.
[56] H. Zhang, H. Song, S. Li, M. Zhou, and D. Song, â€œA survey of
controllable text generation using transformer-based pre-trained
language models,â€ ACM Computing Surveys , vol. 56, no. 3, pp.
1â€“37, 2023.
[57] G. H. D. Rosa and J. P . Papa, â€œA survey on text generation using
generative adversarial networks,â€ Pattern Recognition , vol. 119, p.
108098, 2021.
[58] A. Saxena, A. Khanna, and D. Gupta, â€œEmotion recognition and
detection methods: A comprehensive survey,â€ Journal of Artificial
Intelligence and Systems , vol. 2, no. 1, pp. 53â€“79, 2020.
[59] P . Nandwani and R. Verma, â€œA review on sentiment analysis and
emotion detection from text,â€ Social network analysis and mining ,
vol. 11, no. 1, p. 81, 2021.
[60] F. Z. Canal, T. R. MÃ¼ller, J. C. Matias, G. G. Scotton, A. R.
de Sa Junior, E. Pozzebon, and A. C. Sobieranski, â€œA survey on
facial emotion recognition techniques: A state-of-the-art literature
review,â€ Information Sciences , vol. 582, pp. 593â€“617, 2022.
[61] A. Oussidi and A. Elhassouny, â€œDeep generative models: Sur-
vey,â€ in 2018 International conference on intelligent systems and
computer vision (ISCV) . IEEE, 2018, pp. 1â€“8.
[62] D. P . Kingma and M. Welling, â€œAuto-encoding variational bayes,â€
arXiv preprint arXiv:1312.6114 , 2013.
[63] A. Radford, K. Narasimhan, T. Salimans, I. Sutskever et al. ,
â€œImproving language understanding by generative pre-training,â€
2018.JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 21
[64] J. D. M.-W. C. Kenton and L. K. Toutanova, â€œBert: Pre-training of
deep bidirectional transformers for language understanding,â€ in
Proceedings of NAACL-HLT , 2019, pp. 4171â€“4186.
[65] Z. Yang, Z. Dai, Y. Yang, J. Carbonell, R. R. Salakhutdinov, and
Q. V . Le, â€œXlnet: Generalized autoregressive pretraining for lan-
guage understanding,â€ in Advances in neural information processing
systems , vol. 32, 2019.
[66] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena,
Y. Zhou, W. Li, and P . J. Liu, â€œExploring the limits of transfer
learning with a unified text-to-text transformer,â€ Journal of ma-
chine learning research , vol. 21, no. 140, pp. 1â€“67, 2020.
[67] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever
et al. , â€œLanguage models are unsupervised multitask learners,â€
OpenAI blog , vol. 1, no. 8, p. 9, 2019.
[68] Y. Liu, H. Hou, F. Ma, S. Ni, and F. R. Yu, â€œMllm-ta: Leveraging
multimodal large language models for precise temporal video
grounding,â€ IEEE Signal Processing Letters , 2024.
[69] G. Zhao, X. Huang, M. Taini, S. Z. Li, and M. PietikÃ¤Inen, â€œFacial
expression recognition from near-infrared videos,â€ Image and
vision computing , vol. 29, no. 9, pp. 607â€“619, 2011.
[70] O. Langner, R. Dotsch, G. Bijlstra, D. H. Wigboldus, S. T. Hawk,
and A. V . Knippenberg, â€œPresentation and validation of the
radboud faces database,â€ Cognition and emotion , vol. 24, no. 8,
pp. 1377â€“1388, 2010.
[71] P . Lucey, J. F. Cohn, T. Kanade, J. Saragih, Z. Ambadar, and
I. Matthews, â€œThe extended cohn-kanade dataset (ck+): A com-
plete dataset for action unit and emotion-specified expression,â€
in2010 IEEE Computer Society Conference on Computer Vision and
Pattern Recognition Workshops (CVPRW) . IEEE, 2010, pp. 94â€“101.
[72] S. Du, Y. Tao, and A. M. Martinez, â€œCompound facial expressions
of emotion,â€ Proceedings of the National Academy of Sciences , vol.
111, no. 15, pp. E1454â€“E1462, 2014.
[73] A. Mollahosseini, B. Hasani, and M. H. Mahoor, â€œAffectnet: A
database for facial expression, valence, and arousal computing in
the wild,â€ IEEE Transactions on Affective Computing , vol. 10, no. 1,
pp. 18â€“31, 2017.
[74] S. M. Mavadati, M. H. Mahoor, K. Bartlett, P . Trinh, and J. F.
Cohn, â€œDisfa: A spontaneous facial action intensity database,â€
IEEE Transactions on Affective Computing , vol. 4, no. 2, pp. 151â€“
160, 2013.
[75] C. F. Benitez-Quiroz, R. Srinivasan, and A. M. Martinez, â€œEmo-
tionet: An accurate, real-time algorithm for the automatic anno-
tation of a million facial expressions in the wild,â€ in Proceedings
of the IEEE Conference on Computer Vision and Pattern Recognition ,
2016, pp. 5562â€“5570.
[76] K. Zhou, B. Sisman, R. Liu, and H. Li, â€œSeen and unseen emo-
tional style transfer for voice conversion with a new emotional
speech dataset,â€ in ICASSP 2021 - 2021 IEEE International Confer-
ence on Acoustics, Speech and Signal Processing (ICASSP) . IEEE,
2021, pp. 920â€“924.
[77] A. Adigwe, N. Tits, K. E. Haddad, S. Ostadabbas, and T. Du-
toit, â€œThe emotional voices database: Towards controlling the
emotion dimension in voice generation systems,â€ arXiv preprint
arXiv:1806.09514 , 2018.
[78] F. Burkhardt, A. Paeschke, M. Rolfes, W. F. Sendlmeier, and
B. Weiss, â€œA database of german emotional speech,â€ in Inter-
speech , vol. 5, 2005, pp. 1517â€“1520.
[79] X. Jia, J. Tai, H. Zhou, Y. Li, W. Zhang, H. Du, and
Q. Huang, â€œEt-gan: Cross-language emotion transfer based on
cycle-consistent generative adversarial networks,â€ arXiv preprint
arXiv:1905.11173 , 2019.
[80] P . Gournay, O. Lahaie, and R. Lefebvre, â€œA canadian french emo-
tional speech dataset,â€ in Proceedings of the 9th ACM Multimedia
Systems Conference , 2018, pp. 399â€“402.
[81] C.-B. Im, S.-H. Lee, S.-B. Kim, and S.-W. Lee, â€œEmoq-tts: Emotion
intensity quantization for fine-grained controllable emotional
text-to-speech,â€ in ICASSP 2022 - 2022 IEEE International Con-
ference on Acoustics, Speech and Signal Processing (ICASSP) . IEEE,
2022, pp. 6317â€“6321.
[82] R. He and J. McAuley, â€œUps and downs: Modeling the visual
evolution of fashion trends with one-class collaborative filtering,â€
inProceedings of the 25th International Conference on World Wide
Web, 2016, pp. 507â€“517.
[83] X. Zhou and W. Y. Wang, â€œMojitalk: Generating emotional re-
sponses at scale,â€ in Proceedings of the 56th Annual Meeting of the
Association for Computational Linguistics (Volume 1: Long Papers) ,
2018, pp. 1128â€“1137.[84] K. Wang, Q. Wu, L. Song, Z. Yang, W. Wu, C. Qian, R. He,
Y. Qiao, and C. C. Loy, â€œMead: A large-scale audio-visual dataset
for emotional talking-face generation,â€ in European Conference on
Computer Vision . Springer, 2020, pp. 700â€“717.
[85] H. Cao, D. G. Cooper, M. K. Keutmann, R. C. Gur, A. Nenkova,
and R. Verma, â€œCrema-d: Crowd-sourced emotional multimodal
actors dataset,â€ IEEE Transactions on Affective Computing , vol. 5,
no. 4, pp. 377â€“390, 2014.
[86] S. Albanie, A. Nagrani, A. Vedaldi, and A. Zisserman, â€œEmotion
recognition in speech using cross-modal transfer in the wild,â€ in
Proceedings of the 26th ACM International Conference on Multimedia ,
2018, pp. 292â€“301.
[87] S. Haq, P . J. Jackson, and J. Edge, â€œAudio-visual feature se-
lection and reduction for emotion classification,â€ in Proc. Int.
Conf. on Auditory-Visual Speech Processing (AVSPâ€™08) , 2008, pp.
Tangalooma, Australia.
[88] S. R. Livingstone and F. A. Russo, â€œThe ryerson audio-visual
database of emotional speech and song (ravdess): A dynamic,
multimodal set of facial and vocal expressions in north american
english,â€ PloS One , vol. 13, no. 5, p. e0196391, 2018.
[89] C. Busso, M. Bulut, C.-C. Lee, A. Kazemzadeh, E. Mower, S. Kim,
J. N. Chang, S. Lee, and S. S. Narayanan, â€œIemocap: Interactive
emotional dyadic motion capture database,â€ Language Resources
and Evaluation , vol. 42, pp. 335â€“359, 2008.
[90] S. Tripathy, J. Kannala, and E. Rahtu, â€œIcface: Interpretable and
controllable face reenactment using gans,â€ in Proceedings of the
IEEE/CVF Winter Conference on Applications of Computer Vision ,
2020, pp. 3385â€“3394.
[91] X. Zeng, Y. Pan, M. Wang, J. Zhang, and Y. Liu, â€œRealistic face
reenactment via self-supervised disentangling of identity and
pose,â€ in Proceedings of the AAAI Conference on Artificial Intelli-
gence , vol. 34, 2020, pp. 12 757â€“12 764.
[92] V . Strizhkova, Y. Wang, D. Anghelone, D. Yang, A. Dantcheva,
and F. BrÃ©mond, â€œEmotion editing in head reenactment videos
using latent space manipulation,â€ in 2021 16th IEEE International
Conference on Automatic Face and Gesture Recognition (FG 2021) .
IEEE, 2021, pp. 1â€“8.
[93] C. Groth, J.-P . Tauscher, S. Castillo, and M. Magnor, â€œAltering the
conveyed facial emotion through automatic reenactment of video
portraits,â€ in International Conference on Computer Animation and
Social Agents . Springer, 2020, pp. 128â€“135.
[94] K. Ali and C. E. Hughes, â€œAll-in-one: Facial expression transfer,
editing and recognition using a single network,â€ arXiv preprint
arXiv:1911.07050 , 2019.
[95] T. Xue, J. Yan, D. Zheng, and Y. Liu, â€œSemantic prior guided fine-
grained facial expression manipulation,â€ Complex & Intelligent
Systems , pp. 1â€“16, 2024.
[96] J. Shao and T. Bui, â€œWp2-gan: Wavelet-based multi-level gan
for progressive facial expression translation with parallel gen-
erators,â€ in Proc. British Mach. Vis. Conf. , 2021, pp. 1388â€“1.
[97] M. C. Doukas, S. Zafeiriou, and V . Sharmanska, â€œHeadgan: One-
shot neural head synthesis and editing,â€ in Proceedings of the
IEEE/CVF International Conference on Computer Vision , 2021, pp.
14 398â€“14 407.
[98] Y. Zhao, L. Yang, E. Pei, M. C. Oveneke, M. Alioscha-Perez, L. Li,
D. Jiang, and H. Sahli, â€œAction unit driven facial expression
synthesis from a single image with patch attentive gan,â€ in
Computer Graphics Forum , vol. 40. Wiley Online Library, 2021,
pp. 47â€“61.
[99] Y. Xia, W. Zheng, Y. Wang, H. Yu, J. Dong, and F.-Y. Wang, â€œLocal
and global perception generative adversarial network for facial
expression synthesis,â€ IEEE Transactions on Circuits and Systems
for Video Technology , vol. 32, no. 3, pp. 1443â€“1452, 2021.
[100] J. Wang, J. Zhang, Z. Lu, and S. Shan, â€œDft-net: Disentanglement
of face deformation and texture synthesis for expression editing,â€
in2019 IEEE International Conference on Image Processing (ICIP) .
IEEE, 2019, pp. 3881â€“3885.
[101] A. Akram and N. Khan, â€œSargan: Spatial attention-based resid-
uals for facial expression manipulation,â€ IEEE Transactions on
Circuits and Systems for Video Technology , 2023.
[102] X. Zhang, Y. Zhu, W. Chen, W. Liu, and L. Shen, â€œGated switch-
gan for multi-domain facial image translation,â€ IEEE Transactions
on Multimedia , vol. 24, pp. 1990â€“2003, 2021.
[103] A. Akram and N. Khan, â€œUs-gan: On the importance of ultimate
skip connection for facial expression synthesis,â€ Multimedia Tools
and Applications , vol. 83, no. 3, pp. 7231â€“7247, 2024.JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 22
[104] B. Azari and A. Lim, â€œEmostyle: One-shot facial expression
editing using continuous emotion parameters,â€ in Proceedings of
the IEEE/CVF Winter Conference on Applications of Computer Vision ,
2024, pp. 6385â€“6394.
[105] S. dâ€™Apolito, D. P . Paudel, Z. Huang, A. Romero, and L. V . Gool,
â€œGanmut: Learning interpretable conditional space for gamut of
emotions,â€ in Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition , 2021, pp. 568â€“577.
[106] Y. Peng and H. Yin, â€œApprgan: Appearance-based gan for facial
expression synthesis,â€ IET Image Processing , vol. 13, no. 14, pp.
2706â€“2715, 2019.
[107] L. Song, Z. Lu, R. He, Z. Sun, and T. Tan, â€œGeometry guided
adversarial facial expression synthesis,â€ in Proceedings of the 26th
ACM International Conference on Multimedia , 2018, pp. 627â€“635.
[108] J. Kong, H. Shen, and K. Huang, â€œDualpathgan: Facial reenacted
emotion synthesis,â€ IET Computer Vision , vol. 15, no. 7, pp. 501â€“
513, 2021.
[109] J. Tang, Z. Shao, and L. Ma, â€œFine-grained expression manip-
ulation via structured latent space,â€ in 2020 IEEE International
Conference on Multimedia and Expo (ICME) . IEEE, 2020, pp. 1â€“6.
[110] O. Patashnik, Z. Wu, E. Shechtman, D. Cohen-Or, and D. Lischin-
ski, â€œStyleclip: Text-driven manipulation of stylegan imagery,â€ in
Proceedings of the IEEE/CVF International Conference on Computer
Vision , 2021, pp. 2085â€“2094.
[111] F. Liu, H. Wang, J. Zhang, Z. Fu, A. Zhou, J. Qi, and Z. Li,
â€œEvogan: An evolutionary computation assisted gan,â€ Neurocom-
puting , vol. 469, pp. 81â€“90, 2022.
[112] H. Tang, W. Wang, S. Wu, X. Chen, D. Xu, N. Sebe, and Y. Yan,
â€œExpression conditional gan for facial expression-to-expression
translation,â€ in 2019 IEEE International Conference on Image Pro-
cessing (ICIP) . IEEE, 2019, pp. 4449â€“4453.
[113] S. Sola and D. Gera, â€œUnmasking your expression: Expression-
conditioned gan for masked face inpainting,â€ in Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition ,
2023, pp. 5907â€“5915.
[114] A. Lindt, P . Barros, H. Siqueira, and S. Wermter, â€œFacial expres-
sion editing with continuous emotion labels,â€ in 2019 14th IEEE
International Conference on Automatic Face & Gesture Recognition
(FG 2019) . IEEE, 2019, pp. 1â€“8.
[115] A. Makhzani, J. Shlens, N. Jaitly, I. Goodfellow, and B. Frey, â€œAd-
versarial autoencoders,â€ arXiv preprint arXiv:1511.05644 , 2015.
[116] S. E. Eskimez, Y. Zhang, and Z. Duan, â€œSpeech driven talking
face generation from a single image and an emotion condition,â€
IEEE Transactions on Multimedia , vol. 24, pp. 3480â€“3490, 2021.
[117] K. Vougioukas, S. Petridis, and M. Pantic, â€œRealistic speech-
driven facial animation with gans,â€ International Journal of Com-
puter Vision , vol. 128, no. 5, pp. 1398â€“1413, 2020.
[118] D. Zeng, H. Liu, H. Lin, and S. Ge, â€œTalking face generation with
expression-tailored generative adversarial network,â€ in Proceed-
ings of the 28th ACM International Conference on Multimedia , 2020,
pp. 1716â€“1724.
[119] Y. Gan, Z. Yang, X. Yue, L. Sun, and Y. Yang, â€œEfficient emotional
adaptation for audio-driven talking-head generation,â€ in Proceed-
ings of the IEEE/CVF International Conference on Computer Vision ,
2023, pp. 22 634â€“22 645.
[120] S. Tan, B. Ji, and Y. Pan, â€œFlowvqtalker: High-quality emotional
talking face generation through normalizing flow and quantiza-
tion,â€ in Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition , 2024, pp. 26 317â€“26 327.
[121] S. Tan, B. Ji, M. Bi, and et al., â€œEdtalk: Efficient disentan-
glement for emotional talking head synthesis,â€ arXiv preprint
arXiv:2404.01647 , 2024.
[122] X. Hu, N. Aldausari, and G. Mohammadi, â€œ2cet-gan: Pixel-level
gan model for human facial expression transfer,â€ in Proceedings
of the 1st International Workshop on Multimedia Content Generation
and Evaluation: New Methods and Practice , 2023, pp. 49â€“56.
[123] Y. Choi, M. Choi, M. Kim, J.-W. Ha, S. Kim, and J. Choo, â€œStar-
gan: Unified generative adversarial networks for multi-domain
image-to-image translation,â€ in Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition , 2018, pp. 8789â€“8797.
[124] D. Zhu, S. Liu, W. Jiang, C. Gao, T. Wu, Q. Wang, and G. Guo,
â€œUgan: Untraceable gan for multi-domain face translation,â€ arXiv
preprint arXiv:1907.11418 , 2019.
[125] H. Ding, K. Sricharan, and R. Chellappa, â€œExprgan: Facial expres-
sion editing with controllable expression intensity,â€ in Proceedings
of the AAAI Conference on Artificial Intelligence , vol. 32, 2018.[126] G. Tesei, â€œGenerating realistic facial expressions through condi-
tional cycle-consistent generative adversarial networks (ccycle-
gan),â€ 2019.
[127] W. Wang, Q. Sun, Y. Fu, T. Chen, C. Cao, Z. Zheng, G. Xu,
H. Qiu, Y.-G. Jiang, and X. Xue, â€œComp-gan: Compositional
generative adversarial network in synthesizing and recognizing
facial expression,â€ in Proceedings of the 27th ACM International
Conference on Multimedia , 2019, pp. 211â€“219.
[128] Z. Xu, T. Chen, Z. Yang et al. , â€œSelf-supervised emotion represen-
tation disentanglement for speech-preserving facial expression
manipulation,â€ in Proceedings of ACM Multimedia 2024 , 2024.
[129] S. Tan, B. Ji, and Y. Pan, â€œEmmn: Emotional motion memory
network for audio-driven emotional talking face generation,â€ in
Proceedings of the IEEE/CVF International Conference on Computer
Vision , 2023, pp. 22 146â€“22 156.
[130] â€”â€”, â€œStyle2talker: High-resolution talking head generation with
emotion style and art style,â€ in Proceedings of the AAAI Conference
on Artificial Intelligence , vol. 38, 2024, pp. 5079â€“5087.
[131] S. Zhai, M. Liu, Y. Li, Z. Gao, L. Zhu, and L. Nie, â€œTalking
face generation with audio-deduced emotional landmarks,â€ IEEE
Transactions on Neural Networks and Learning Systems , 2023.
[132] Z. Sheng, L. Nie, M. Zhang, X. Chang, and Y. Yan, â€œStochastic
latent talking face generation towards emotional expressions and
head poses,â€ IEEE Transactions on Circuits and Systems for Video
Technology , 2023.
[133] Y. Ma, S. Zhang, J. Wang, X. Wang, Y. Zhang, and Z. Deng,
â€œDreamtalk: When expressive talking head generation meets
diffusion probabilistic models,â€ arXiv preprint arXiv:2312.09767 ,
2023.
[134] C. Zhang, C. Wang, J. Zhang, H. Xu, G. Song, Y. Xie, L. Luo,
Y. Tian, X. Guo, and J. Feng, â€œDream-talk: Diffusion-based realis-
tic emotional audio-driven method for single image talking face
generation,â€ arXiv preprint arXiv:2312.13578 , 2023.
[135] Z. Sun, Y. H. Wen, T. Lv et al. , â€œContinuously controllable facial
expression editing in talking face videos,â€ IEEE Transactions on
Affective Computing , 2023.
[136] H. Wang, X. Jia, and X. Cao, â€œEat-face: Emotion-controllable
audio-driven talking face generation via diffusion model,â€ in Pro-
ceedings of the 2024 IEEE 18th International Conference on Automatic
Face and Gesture Recognition (FG) . IEEE, 2024, pp. 1â€“10.
[137] K. Zhou, B. Sisman, M. Zhang, and H. Li, â€œConverting anyoneâ€™s
emotion: Towards speaker-independent emotional voice conver-
sion,â€ arXiv preprint arXiv:2005.07025 , 2020.
[138] K. Zhou, B. Sisman, and H. Li, â€œVaw-gan for disentanglement
and recomposition of emotional elements in speech,â€ in 2021
IEEE Spoken Language Technology Workshop (SLT) . IEEE, 2021,
pp. 415â€“422.
[139] â€”â€”, â€œTransforming spectrum and prosody for emotional voice
conversion with non-parallel training data,â€ arXiv preprint
arXiv:2002.00198 , 2020.
[140] J.-Y. Zhu, T. Park, P . Isola, and A. A. Efros, â€œUnpaired image-to-
image translation using cycle-consistent adversarial networks,â€
inProceedings of the IEEE International Conference on Computer
Vision , 2017, pp. 2223â€“2232.
[141] C. Fu, C. Liu, C. T. Ishi, and H. Ishiguro, â€œAn improved cyclegan-
based emotional voice conversion model by augmenting tempo-
ral dependency with a transformer,â€ Speech Communication , vol.
144, pp. 110â€“121, 2022.
[142] G. Rizos, A. Baird, M. Elliott, and B. Schuller, â€œStargan for
emotional speech conversion: Validated by data augmentation
of end-to-end emotion recognition,â€ in ICASSP 2020-2020 IEEE
International Conference on Acoustics, Speech and Signal Processing
(ICASSP) . IEEE, 2020, pp. 3502â€“3506.
[143] M. Elgaar, J. Park, and S. W. Lee, â€œMulti-speaker and multi-
domain emotional voice conversion using factorized hierarchical
variational autoencoder,â€ in ICASSP 2020-2020 IEEE International
Conference on Acoustics, Speech and Signal Processing (ICASSP) .
IEEE, 2020, pp. 7769â€“7773.
[144] J. Gao, D. Chakraborty, H. Tembine, and O. Olaleye, â€œNonparallel
emotional speech conversion,â€ arXiv preprint arXiv:1811.01174 ,
2018.
[145] Y. Chen, L. Yang, Q. Chen, J.-H. Lai, and X. Xie, â€œAttention-based
interactive disentangling network for instance-level emotional
voice conversion,â€ arXiv preprint arXiv:2312.17508 , 2023.
[146] H.-S. Oh, S.-H. Lee, D.-H. Cho, and S.-W. Lee, â€œDurflex-evc:
Duration-flexible emotional voice conversion with parallel gen-
eration,â€ arXiv preprint arXiv:2401.08095 , 2024.JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 23
[147] Z. Du, B. Sisman, K. Zhou, and H. Li, â€œExpressive voice con-
version: A joint framework for speaker identity and emotional
style transfer,â€ in 2021 IEEE Automatic Speech Recognition and
Understanding Workshop (ASRU) . IEEE, 2021, pp. 594â€“601.
[148] A. H. Meftah, A. A. Alashban, Y. A. Alotaibi, and S.-A. Selouani,
â€œEnglish emotional voice conversion using stargan model,â€ IEEE
Access , 2023.
[149] N. Shah, M. Singh, N. Takahashi, and N. Onoe, â€œNonparallel
emotional voice conversion for unseen speaker-emotion pairs us-
ing dual domain adversarial network & virtual domain pairing,â€
inICASSP 2023-2023 IEEE International Conference on Acoustics,
Speech and Signal Processing (ICASSP) . IEEE, 2023, pp. 1â€“5.
[150] Z. Du, B. Sisman, K. Zhou, and H. Li, â€œDisentanglement of emo-
tional style and speaker identity for expressive voice conversion,â€
arXiv preprint arXiv:2110.10326 , 2021.
[151] K. Zhou, B. Sisman, R. Rana, B. W. Schuller, and H. Li, â€œEmotion
intensity and its control for emotional voice conversion,â€ IEEE
Transactions on Affective Computing , vol. 14, no. 1, pp. 31â€“48, 2022.
[152] F. Kreuk, A. Polyak, J. Copet, E. Kharitonov, T. A. Nguyen,
M. RiviÃ¨re, W.-N. Hsu, A. Mohamed, E. Dupoux, and Y. Adi,
â€œTextless speech emotion conversion using discrete & decom-
posed representations,â€ in Proceedings of the 2022 Conference on
Empirical Methods in Natural Language Processing , 2022, pp. 11 200â€“
11 214.
[153] H. Choi and M. Hahn, â€œSequence-to-sequence emotional voice
conversion with strength control,â€ IEEE Access , vol. 9, pp. 42 674â€“
42 687, 2021.
[154] Y. Lei, S. Yang, and L. Xie, â€œFine-grained emotion strength
transfer, control and prediction for emotional speech synthesis,â€
in2021 IEEE Spoken Language Technology Workshop (SLT) . IEEE,
2021, pp. 423â€“430.
[155] T. Li, C. Hu, J. Cong, X. Zhu, J. Li, Q. Tian, Y. Wang, and L. Xie,
â€œDiclet-tts: Diffusion model based cross-lingual emotion transfer
for text-to-speechâ€”a study between english and mandarin,â€
IEEE/ACM Transactions on Audio, Speech, and Language Processing ,
2023.
[156] N. Tits, K. E. Haddad, and T. Dutoit, â€œExploring transfer learn-
ing for low resource emotional tts,â€ in Intelligent Systems and
Applications: Proceedings of the 2019 Intelligent Systems Conference
(IntelliSys) Volume 1 . Springer, 2020, pp. 52â€“60.
[157] B. Schnell and P . N. Garner, â€œImproving emotional tts with an
emotion intensity input from unsupervised extraction,â€ in Proc.
11th ISCA Speech Synth. Workshop , 2021, pp. 60â€“65.
[158] P . Wu, Z. Ling, L. Liu, Y. Jiang, H. Wu, and L. Dai, â€œEnd-to-
end emotional speech synthesis using style tokens and semi-
supervised training,â€ in 2019 Asia-Pacific Signal and Information
Processing Association Annual Summit and Conference (APSIP A
ASC) . IEEE, 2019, pp. 623â€“627.
[159] S.-Y. Um, S. Oh, K. Byun, I. Jang, C. Ahn, and H.-G. Kang,
â€œEmotional speech synthesis with rich and granularized control,â€
inICASSP 2020-2020 IEEE International Conference on Acoustics,
Speech and Signal Processing (ICASSP) . IEEE, 2020, pp. 7254â€“7258.
[160] E. Hortal and R. B. Alarcia, â€œGantron: Emotional speech syn-
thesis with generative adversarial networks,â€ arXiv preprint
arXiv:2110.03390 , 2021.
[161] Y. Guo, C. Du, X. Chen, and K. Yu, â€œEmodiff: Intensity con-
trollable emotional text-to-speech with soft-label guidance,â€ in
ICASSP 2023-2023 IEEE International Conference on Acoustics,
Speech and Signal Processing (ICASSP) . IEEE, 2023, pp. 1â€“5.
[162] T. Li, S. Yang, L. Xue, and L. Xie, â€œControllable emotion transfer
for end-to-end speech synthesis,â€ in 2021 12th International Sym-
posium on Chinese Spoken Language Processing (ISCSLP) . IEEE,
2021, pp. 1â€“5.
[163] Y. Lei, S. Yang, X. Wang, and L. Xie, â€œMsemotts: Multi-scale
emotion transfer, prediction, and control for emotional speech
synthesis,â€ IEEE/ACM Transactions on Audio, Speech, and Language
Processing , vol. 30, pp. 853â€“864, 2022.
[164] T. Li, X. Wang, Q. Xie, Z. Wang, and L. Xie, â€œCross-speaker emo-
tion disentangling and transfer for end-to-end speech synthesis,â€
IEEE/ACM Transactions on Audio, Speech, and Language Processing ,
vol. 30, pp. 1448â€“1460, 2022.
[165] X. Li, C. Song, J. Li, Z. Wu, J. Jia, and H. Meng, â€œTowards multi-
scale style control for expressive speech synthesis,â€ arXiv preprint
arXiv:2104.03521 , 2021.
[166] H. Tang, X. Zhang, J. Wang, N. Cheng, and J. Xiao, â€œEmomix:
Emotion mixing via diffusion models for emotional speech syn-
thesis,â€ arXiv preprint arXiv:2306.00648 , 2023.[167] K. Matsumoto, S. Hara, and M. Abe, â€œControlling the strength of
emotions in speech-like emotional sound generated by wavenet,â€
inINTERSPEECH , 2020, pp. 3421â€“3425.
[168] T. Wang, J. Yi, R. Fu, J. Tao, Z. Wen, and C. Y. Zhang, â€œEmotion
selectable end-to-end text-based speech editing,â€ Artificial Intelli-
gence , vol. 329, p. 104076, 2024.
[169] Y. Ren, Y. Ruan, X. Tan, T. Qin, S. Zhao, Z. Zhao, and T.-Y.
Liu, â€œFastspeech: Fast, robust and controllable text to speech,â€
inAdvances in Neural Information Processing Systems , vol. 32, 2019.
[170] T. Qi, S. Wang, C. Lu et al. , â€œTowards realistic emotional voice
conversion using controllable emotional intensity,â€ arXiv preprint
arXiv:2407.14800 , 2024.
[171] M. Kang, W. Han, S. J. Hwang, and E. Yang, â€œZet-speech:
Zero-shot adaptive emotion-controllable text-to-speech synthe-
sis with diffusion and style-based models,â€ arXiv preprint
arXiv:2305.13831 , 2023.
[172] X. Zhu, S. Yang, G. Yang, and L. Xie, â€œControlling emotion
strength with relative attribute for end-to-end speech synthesis,â€
in2019 IEEE Automatic Speech Recognition and Understanding
Workshop (ASRU) . IEEE, 2019, pp. 192â€“199.
[173] Y. Lei, S. Yang, X. Zhu, L. Xie, and D. Su, â€œCross-speaker emotion
transfer through information perturbation in emotional speech
synthesis,â€ IEEE Signal Processing Letters , vol. 29, pp. 1948â€“1952,
2022.
[174] X. Cai, D. Dai, Z. Wu, X. Li, J. Li, and H. Meng, â€œEmotion
controllable speech synthesis using emotion-unlabeled dataset
with the assistance of cross-domain speech emotion recognition,â€
inICASSP 2021-2021 IEEE International Conference on Acoustics,
Speech and Signal Processing (ICASSP) . IEEE, 2021, pp. 5734â€“5738.
[175] S. Wang, J. GuÃ°nason, and D. Borth, â€œFine-grained emotional
control of text-to-speech: Learning to rank inter-and intra-class
emotion intensities,â€ in ICASSP 2023-2023 IEEE International Con-
ference on Acoustics, Speech and Signal Processing (ICASSP) . IEEE,
2023, pp. 1â€“5.
[176] K. Zhou, B. Sisman, R. Rana, B. W. Schuller, and H. Li, â€œSpeech
synthesis with mixed emotions,â€ IEEE Transactions on Affective
Computing , 2022.
[177] D. Diatlova and V . Shutov, â€œEmospeech: Guiding fastspeech2 to-
wards emotional text to speech,â€ arXiv preprint arXiv:2307.00024 ,
2023.
[178] W. Guan, Y. Li, T. Li et al. , â€œMm-tts: Multi-modal prompt based
style transfer for expressive text-to-speech synthesis,â€ in Proceed-
ings of the AAAI Conference on Artificial Intelligence , vol. 38, no. 16,
2024, pp. 18 117â€“18 125.
[179] K. Zhou, Y. Zhang, S. Zhao et al. , â€œEmotional dimension control
in language model-based text-to-speech: Spanning a broad spec-
trum of human emotions,â€ arXiv preprint arXiv:2409.16681 , 2024.
[180] H. Tang, X. Zhang, N. Cheng et al. , â€œEd-tts: Multi-scale emotion
modeling using cross-domain emotion diarization for emotional
speech synthesis,â€ in Proceedings of the IEEE International Confer-
ence on Acoustics, Speech and Signal Processing (ICASSP) . IEEE,
2024, pp. 12 146â€“12 150.
[181] X. Jing, K. Zhou, A. Triantafyllopoulos et al. , â€œEnhancing emo-
tional text-to-speech controllability with natural language guid-
ance through contrastive learning and diffusion models,â€ arXiv
preprint arXiv:2409.06451 , 2024.
[182] H. Shi, J. Wang, X. Zhang et al. , â€œRset: Remapping-based sorting
method for emotion transfer speech synthesis,â€ in Proceedings
of Asia-Pacific Web (APWeb) and Web-Age Information Manage-
ment (WAIM) Joint International Conference on Web and Big Data .
Springer Nature Singapore, 2024, pp. 90â€“104.
[183] P . Riley, N. Constant, M. Guo, G. Kumar, D. C. Uthus, and
Z. Parekh, â€œTextsettr: Few-shot text style extraction and tunable
targeted restyling,â€ in Proceedings of the 59th Annual Meeting of the
Association for Computational Linguistics and the 11th International
Joint Conference on Natural Language Processing (Volume 1: Long
Papers) , 2021, pp. 3786â€“3800.
[184] R. MohammadiBaghmolaei and A. Ahmadi, â€œTet: Text emotion
transfer,â€ Knowledge-Based Systems , vol. 262, p. 110236, 2023.
[185] Z. Jin, D. Jin, J. Mueller, N. Matthews, and E. Santus, â€œImat:
Unsupervised text attribute transfer via iterative matching and
translation,â€ in Proceedings of the 2019 Conference on Empirical
Methods in Natural Language Processing and the 9th International
Joint Conference on Natural Language Processing (EMNLP-IJCNLP) ,
2019, pp. 3097â€“3109.
[186] X. Wu, T. Zhang, L. Zang, J. Han, and S. Hu, â€œMask and infill:JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 24
Applying masked language model to sentiment transfer,â€ Aug
2019.
[187] E. Malmi, A. Severyn, and S. Rothe, â€œUnsupervised text style
transfer with padded masked language models,â€ in Proceedings
of the 2020 Conference on Empirical Methods in Natural Language
Processing (EMNLP) , 2020, pp. 8671â€“8680.
[188] Z. Yang, Z. Hu, C. Dyer, E. P . Xing, and T. Berg-Kirkpatrick,
â€œUnsupervised text style transfer using language models as
discriminators,â€ Advances in Neural Information Processing Systems ,
vol. 31, 2018.
[189] T. Shen, T. Lei, R. Barzilay, and T. Jaakkola, â€œStyle transfer
from non-parallel text by cross-alignment,â€ Advances in Neural
Information Processing Systems , vol. 30, 2017.
[190] R. Zhang, Z. Wang, K. Yin, and Z. Huang, â€œEmotional text gen-
eration based on cross-domain sentiment transfer,â€ IEEE Access ,
vol. 7, pp. 100 081â€“100 089, 2019.
[191] Y. Huang, W. Zhu, D. Xiong, Y. Zhang, C. Hu, and F. Xu, â€œCycle-
consistent adversarial autoencoders for unsupervised text style
transfer,â€ in Proceedings of the 28th International Conference on
Computational Linguistics , 2020, pp. 2213â€“2223.
[192] F. Luo, P . Li, P . Yang, J. Zhou, Y. Tan, B. Chang, Z. Sui, and X. Sun,
â€œTowards fine-grained text sentiment transfer,â€ in Proceedings
of the 57th Annual Meeting of the Association for Computational
Linguistics , 2019, pp. 2013â€“2022.
[193] Z. Li, G. Chen, R. Shao, D. Jiang, and L. Nie, â€œEnhancing
the emotional generation capability of large language models
via emotional chain-of-thought,â€ arXiv preprint arXiv:2401.06836 ,
2024.
[194] Z. Yang, Z. Ren, W. Yufeng, S. Peng, H. Sun, X. Zhu, and
X. Liao, â€œEnhancing empathetic response generation by aug-
menting llms with small-scale empathetic models,â€ arXiv preprint
arXiv:2402.11801 , 2024.
[195] L. Sun, N. Xu, J. Wei, B. Yu, L. Bu, and Y. Luo, â€œRational sensi-
bility: Llm enhanced empathetic response generation guided by
self-presentation theory,â€ arXiv preprint arXiv:2312.08702 , 2023.
[196] Y.-J. Lee, C.-G. Lim, and H.-J. Choi, â€œDoes gpt-3 generate em-
pathetic dialogues? a novel in-context example selection method
and automatic evaluation metric for empathetic dialogue gen-
eration,â€ in Proceedings of the 29th International Conference on
Computational Linguistics , 2022, pp. 669â€“683.
[197] Y. Chen, X. Xing, J. Lin, H. Zheng, Z. Wang, Q. Liu, and X. Xu,
â€œSoulchat: Improving llmsâ€™ empathy, listening, and comfort abili-
ties through fine-tuning with multi-turn empathy conversations,â€
inFindings of the Association for Computational Linguistics: EMNLP
2023 , 2023, pp. 1170â€“1183.
[198] Z. Song, X. Zheng, L. Liu, M. Xu, and X.-J. Huang, â€œGenerating
responses with a specific emotion in dialog,â€ in Proceedings of the
57th Annual Meeting of the Association for Computational Linguistics ,
2019, pp. 3685â€“3695.
[199] H. Zhou, M. Huang, T. Zhang, X. Zhu, and B. Liu, â€œEmotional
chatting machine: Emotional conversation generation with inter-
nal and external memory,â€ in Proceedings of the AAAI Conference
on Artificial Intelligence , vol. 32, 2018.
[200] X. Kong, B. Li, G. Neubig, E. Hovy, and Y. Yang, â€œAn adversarial
approach to high-quality, sentiment-controlled neural dialogue
generation,â€ arXiv preprint arXiv:1901.07129 , 2019.
[201] M. Li, J. Zhang, X. Lu, and C. Zong, â€œDual-view conditional vari-
ational auto-encoder for emotional dialogue generation,â€ Trans-
actions on Asian and Low-Resource Language Information Processing ,
vol. 21, no. 3, pp. 1â€“18, 2021.
[202] W. Xu, X. Gu, and G. Chen, â€œGenerating emotional controllable
response based on multi-task and dual attention framework,â€
IEEE Access , vol. 7, pp. 93 734â€“93 741, 2019.
[203] N. Asghar, P . Poupart, J. Hoey, X. Jiang, and L. Mou, â€œAffective
neural response generation,â€ in Advances in Information Retrieval:
40th European Conference on IR Research, ECIR 2018 . Springer,
2018, pp. 154â€“166.
[204] P . Colombo, W. Witon, A. Modi, J. Kennedy, and
M. Kapadia, â€œAffect-driven dialog generation,â€ arXiv preprint
arXiv:1904.02793 , 2019.
[205] C. Huang, O. R. Zaiane, A. Trabelsi, and N. Dziri, â€œAutomatic
dialogue generation with expressed emotions,â€ in Proceedings of
the 2018 Conference of the North American Chapter of the Association
for Computational Linguistics: Human Language Technologies , vol. 2,
2018, pp. 49â€“54.
[206] H. Lin and Z. Deng, â€œEmotional dialogue generation based on
transformer and conditional variational autoencoder,â€ in 2022IEEE 21st International Conference on Ubiquitous Computing and
Communications (IUCC/CIT/DSCI/SmartCNS) . IEEE, 2022, pp.
386â€“393.
[207] K. Wang and X. Wan, â€œSentigan: Generating sentimental texts via
mixture adversarial networks,â€ in IJCAI , 2018, pp. 4446â€“4452.
[208] â€”â€”, â€œAutomatic generation of sentimental texts via mixture
adversarial networks,â€ Artificial Intelligence , vol. 275, pp. 540â€“558,
2019.
[209] W. Chen, X. Chen, and X. Sun, â€œEmotional dialog generation via
multiple classifiers based on a generative adversarial network,â€
Virtual Reality & Intelligent Hardware , vol. 3, no. 1, pp. 18â€“32, 2021.
[210] G. Bi, L. Shen, Y. Cao, M. Chen, Y. Xie, Z. Lin, and X. He,
â€œDiffusemp: A diffusion model-based framework with multi-
grained control for empathetic response generation,â€ arXiv
preprint arXiv:2306.01657 , 2023.
[211] J. Chen, Y. Wu, C. Jia, H. Zheng, and G. Huang, â€œCustomizable
text generation via conditional text generative adversarial net-
work,â€ Neurocomputing , vol. 416, pp. 125â€“135, 2020.
[212] E. Reif, D. Ippolito, A. Yuan, A. Coenen, C. Callison-Burch,
and J. Wei, â€œA recipe for arbitrary text style transfer with large
language models,â€ in Proceedings of the 60th Annual Meeting of the
Association for Computational Linguistics (Volume 2: Short Papers) ,
2022, pp. 837â€“848.
[213] X. Yi, Z. Liu, W. Li, and M. Sun, â€œText style transfer via
learning style instance supported latent space,â€ in Proceedings
of the Twenty-Ninth International Conference on International Joint
Conferences on Artificial Intelligence , 2021, pp. 3801â€“3807.
[214] X. Li, G. Chen, C. Lin, and R. Li, â€œDgst: A dual-generator network
for text style transfer,â€ in Proceedings of the 2020 Conference on
Empirical Methods in Natural Language Processing (EMNLP) , 2020,
pp. 7131â€“7136.
[215] A. Sancheti, K. Krishna, B. V . Srinivasan, and A. Natarajan, â€œRe-
inforced rewards framework for text style transfer,â€ in Advances
in Information Retrieval: 42nd European Conference on IR Research,
ECIR 2020, Lisbon, Portugal, April 14â€“17, 2020, Proceedings, Part I ,
vol. 42. Springer, 2020, pp. 545â€“560.
[216] K. Chawla and D. Yang, â€œSemi-supervised formality style trans-
fer using language model discriminator and mutual information
maximization,â€ in Findings of the Association for Computational
Linguistics: EMNLP 2020 , 2020, pp. 2340â€“2354.
[217] Y. Liu, W. Maier, W. Minker, and S. Ultes, â€œEmpathetic dialogue
generation with pre-trained roberta-gpt2 and external knowl-
edge,â€ in Conversational AI for Natural Human-Centric Interaction:
12th International Workshop on Spoken Dialogue System Technology,
IWSDS 2021, Singapore . Springer, 2022, pp. 67â€“81.
[218] Y. Qian, W. Zhang, and T. Liu, â€œHarnessing the power of large
language models for empathetic response generation: Empirical
investigations and improvements,â€ in The 2023 Conference on
Empirical Methods in Natural Language Processing , 2023.
[219] M. Firdaus, H. Chauhan, A. Ekbal, and P . Bhattacharyya,
â€œEmosen: Generating sentiment and emotion controlled re-
sponses in a multimodal dialogue system,â€ IEEE Transactions on
Affective Computing , vol. 13, no. 3, pp. 1555â€“1566, 2020.
[220] N. Majumder, P . Hong, S. Peng, J. Lu, D. Ghosal, A. Gelbukh,
R. Mihalcea, and S. Poria, â€œMime: Mimicking emotions for empa-
thetic response generation,â€ arXiv preprint arXiv:2010.01454 , 2020.
[221] S. Sabour, C. Zheng, and M. Huang, â€œCem: Commonsense-aware
empathetic response generation,â€ in Proceedings of the AAAI Con-
ference on Artificial Intelligence , vol. 36, 2022, pp. 11 229â€“11 237.
[222] J. Li, X. Sun, X. Wei, C. Li, and J. Tao, â€œReinforcement learn-
ing based emotional editing constraint conversation generation,â€
arXiv preprint arXiv:1904.08061 , 2019.
[223] Y. Li and B. Wu, â€œEmotional dialogue generation with generative
adversarial networks,â€ in 2020 IEEE 4th Information Technology,
Networking, Electronic and Automation Control Conference (ITNEC) ,
vol. 1. IEEE, 2020, pp. 868â€“873.
[224] Z. Gu, Q. Zhu, H. He et al. , â€œMulti-level knowledge-enhanced
prompting for empathetic dialogue generation,â€ in Proceedings of
the 27th International Conference on Computer Supported Cooperative
Work in Design (CSCWD) . IEEE, 2024, pp. 3170â€“3175.
[225] X. Chen, C. Yang, M. Lan et al. , â€œCause-aware empathetic
response generation via chain-of-thought fine-tuning,â€ arXiv
preprint arXiv:2408.11599 , 2024.
[226] X. Ji, H. Zhou, K. Wang, W. Wu, C. C. Loy, X. Cao, and F. Xu,
â€œAudio-driven emotional video portraits,â€ in Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition ,
2021, pp. 14 080â€“14 089.JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 25
[227] Z. Guo, Y. Xie, W. Xie, P . Huang, F. Ma, and F. R. Yu, â€œGaussianpu:
A hybrid 2d-3d upsampling framework for enhancing color point
clouds via 3d gaussian splatting,â€ arXiv preprint arXiv:2409.01581 ,
2024.
[228] T. Cornille, F. Wang, and J. Bekker, â€œInteractive multi-level
prosody control for expressive speech synthesis,â€ in ICASSP
2022-2022 IEEE International Conference on Acoustics, Speech and
Signal Processing (ICASSP) . IEEE, 2022, pp. 8312â€“8316.
[229] Y. Wang, R. Skerry-Ryan, D. Stanton, Y. Wu, R. J. Weiss, N. Jaitly,
Z. Yang, Y. Xiao, Z. Chen, S. Bengio et al. , â€œTacotron: Towards end-
to-end speech synthesis,â€ arXiv preprint arXiv:1703.10135 , 2017.
[230] V . Sorin, D. Brin, Y. Barash, E. Konen, A. Charney, G. Nadkarni,
and E. Klang, â€œLarge language models (llms) and empathyâ€”a
systematic review,â€ medRxiv , pp. 2023â€“08, 2023.
[231] T. BaltruÅ¡aitis, C. Ahuja, and L.-P . Morency, â€œMultimodal ma-
chine learning: A survey and taxonomy,â€ IEEE Transactions on
Pattern Analysis and Machine Intelligence , vol. 41, no. 2, pp. 423â€“
443, 2018.
[232] H. Hou, P . Zeng, F. Ma, and F. R. Yu, â€œVisualrwkv: Exploring
recurrent neural networks for visual language models,â€ arXiv
preprint arXiv:2406.13362 , 2024.
[233] S. Nyatsanga, T. Kucherenko, C. Ahuja, G. E. Henter, and M. Neff,
â€œA comprehensive review of data-driven co-speech gesture gen-
eration,â€ Computer Graphics Forum , vol. 42, pp. 569â€“596, 2023.
[234] C. Papoutsi, A. Drigas, and C. Skianis, â€œVirtual and augmented
reality for developing emotional intelligence skills,â€ Int. J. Recent
Contrib. Eng. Sci. IT (IJES) , vol. 9, no. 3, pp. 35â€“53, 2021.
[235] K. Cao, Y. Liu, G. Meng, and Q. Sun, â€œAn overview on edge
computing research,â€ IEEE access , vol. 8, pp. 85 714â€“85 728, 2020.
[236] F. J. Dian, R. Vahidnia, and A. Rahmati, â€œWearables and the in-
ternet of things (iot), applications, opportunities, and challenges:
A survey,â€ IEEE access , vol. 8, pp. 69 200â€“69 211, 2020.
[237] M. Izani, A. Razak, D. Rehad, and M. Rosli, â€œThe impact of
artificial intelligence on animation filmmaking: Tools, trends, and
future implications,â€ in 2024 International Visualization, Informatics
and Technology Conference (IVIT) . IEEE, 2024, pp. 57â€“62.
[238] A. Channa, A. Sharma, M. Singh, P . Malhotra, A. Bajpai, and
P . Whig, â€œOriginal research article revolutionizing filmmaking:
A comparative analysis of conventional and ai-generated film
production in the era of virtual reality,â€ Journal of Autonomous
Intelligence , vol. 7, no. 4, 2024.
[239] H. Sun and Y. He, â€œThe application and user acceptance of aigc
in network audiovisual field: Based on the perspective of social
cognitive theory,â€ in Proceedings of the 7th International Conference
on Computer Science and Application Engineering , 2023, pp. 1â€“5.",1.8376238482978997,"['Fei Ma', 'Yukan Li', 'Yifan Xie', 'Y. He', 'Yi Zhang', 'Hongwei Ren', 'Zhou Liu', 'Wei Yao', 'Fuji Ren', 'Fei Richard Yu', 'Shiguang Ni']",https://www.semanticscholar.org/paper/bd7e210994b7ba821c29a4d60325493646283b20,semanticscholar
