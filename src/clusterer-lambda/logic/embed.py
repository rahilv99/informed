"""
Simple semantic clustering for government documents and news articles
with government documents as cluster anchors
"""

import numpy as np
import pandas as pd
from sklearn.metrics.pairwise import cosine_similarity
from sentence_transformers import SentenceTransformer
import logging
import psycopg2
import boto3
import pickle
import os
import json
import spacy
import hashlib
from datetime import datetime
from io import StringIO
import sys
import os
# Add parent directory to path for imports
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
sys.path.append(os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), '..', 'scraper-lambda'))
#import common.s3 as s3

db_access_url = os.environ.get('DB_ACCESS_URL')


class Clusterer:
    """
    A simple clusterer that groups news articles around government documents
    based on semantic similarity. Each cluster must have at least one government document.
    """
    def __init__(self, similarity_threshold=0.35, merge_threshold=0.8):
        """
        Initialize the clusterer with similarity thresholds.
        
        Parameters:
        - similarity_threshold: Minimum similarity for assigning news to gov docs (0-1)
        - merge_threshold: Threshold for merging similar government documents (0-1)
        """
        self.model = SentenceTransformer('all-MiniLM-L6-v2')
        self.nlp = spacy.load("en_core_web_sm")
        self.similarity_threshold = similarity_threshold
        self.merge_threshold = merge_threshold
    
    def extract_entities(self, input_text):
        """Extract entities and nouns from text"""
        if not isinstance(input_text, str):
            return ""
        
        doc = self.nlp(input_text)
        
        entities = [ent.text.lower() for ent in doc.ents]
        nouns = [token.text.lower() for token in doc if token.pos_ == "NOUN"]
        return " ".join(entities + nouns)
    
    def prepare_documents(self, gov_df, news_df):
        """
        Prepare government documents and news articles for embedding.
        
        Parameters:
        - gov_df: DataFrame containing government documents
        - news_df: DataFrame containing news articles
        
        Returns:
        - gov_texts: List of government document texts for embedding
        - news_texts: List of news article texts for embedding
        """
        gov_texts = []
        news_texts = []
        
        # Process government documents
        for _, row in gov_df.iterrows():
            # Use title and beginning of text
            title = row['title']
            text = row.get('text', '')
            sz = len(text)
            mid = sz // 2
            content = title # + " " + self.extract_entities(text[mid:mid+5000]) # get text somewhere in the middle
            gov_texts.append(content[:200])
        
        # Process news articles
        for _, row in news_df.iterrows():
            # Use title and beginning of text
            text = row['title']
            news_texts.append(text)
        
        return gov_texts, news_texts
    
    def generate_embeddings(self, gov_texts, news_texts):
        """
        Generate embeddings for government documents and news articles.
        
        Parameters:
        - gov_texts: List of government document texts
        - news_texts: List of news article texts
        
        Returns:
        - gov_embeddings: Array of government document embeddings
        - news_embeddings: Array of news article embeddings
        """
        print(f"Generating embeddings for {len(gov_texts)} government documents...")
        gov_embeddings = self.model.encode(gov_texts)
        
        print(f"Generating embeddings for {len(news_texts)} news articles...")
        news_embeddings = self.model.encode(news_texts)
        
        print(f"Generated {len(gov_embeddings)} government document embeddings and {len(news_embeddings)} news article embeddings")
        return gov_embeddings, news_embeddings
    
    def calculate_similarities(self, gov_embeddings, news_embeddings):
        """
        Calculate similarities between government documents and news articles.
        
        Parameters:
        - gov_embeddings: Array of government document embeddings
        - news_embeddings: Array of news article embeddings
        
        Returns:
        - gov_news_sim: Similarity matrix between government documents and news articles
        - gov_gov_sim: Similarity matrix between government documents
        """
        # Calculate similarity between government documents and news articles
        gov_news_sim = cosine_similarity(gov_embeddings, news_embeddings)
        
        # Calculate similarity between government documents
        gov_gov_sim = cosine_similarity(gov_embeddings)
        
        return gov_news_sim, gov_gov_sim
    
    def merge_similar_gov_docs(self, gov_gov_sim):
        """
        Merge similar government documents into clusters, limiting to at most 10 gov docs per cluster.
        
        Parameters:
        - gov_gov_sim: Similarity matrix between government documents
        
        Returns:
        - gov_clusters: Dictionary mapping cluster IDs to lists of government document indices
        """
        n_gov = gov_gov_sim.shape[0]
        
        # Initialize each government document as its own cluster
        gov_clusters = {i: [i] for i in range(n_gov)}
        
        # Create a priority queue of similarities
        similarities = []
        for i in range(n_gov):
            for j in range(i + 1, n_gov):
                sim = gov_gov_sim[i, j]
                if sim >= self.merge_threshold:
                    similarities.append((-sim, i, j))  # Negative for max-heap
        
        # Sort once to create a max-heap
        similarities.sort()  # Will sort by first element (-sim)
        
        # Track which cluster ID each document belongs to
        doc_to_cluster = {i: i for i in range(n_gov)}
        
        while similarities:
            sim, doc1, doc2 = similarities.pop(0)
            sim = -sim  # Convert back to positive
            
            # Get current cluster IDs
            c1_id = doc_to_cluster[doc1]
            c2_id = doc_to_cluster[doc2]
            
            # Skip if documents are already in the same cluster
            if c1_id == c2_id:
                continue

            # Check if merging would exceed the limit
            total_size = len(gov_clusters[c1_id]) + len(gov_clusters[c2_id])
            if total_size > 10:
                continue  # Skip merging if it would exceed 10 docs
            
            # Merge smaller cluster into larger one
            if len(gov_clusters[c1_id]) < len(gov_clusters[c2_id]):
                c1_id, c2_id = c2_id, c1_id
                
            print(f"Merging clusters {c2_id} into {c1_id} with similarity {sim:.4f}")
            
            # Update cluster assignments
            for doc_idx in gov_clusters[c2_id]:
                doc_to_cluster[doc_idx] = c1_id
            
            # Merge clusters
            gov_clusters[c1_id].extend(gov_clusters[c2_id])
            del gov_clusters[c2_id]
        
        print(f"Created {len(gov_clusters)} government document clusters")
        return gov_clusters

    def assign_news_to_gov_clusters(self, gov_clusters, gov_news_sim):
        """
        Assign news articles to government document clusters, with a maximum of 10 news articles per cluster.
        
        Parameters:
        - gov_clusters: Dictionary mapping cluster IDs to lists of government document indices
        - gov_news_sim: Similarity matrix between government documents and news articles
        
        Returns:
        - final_clusters: Dictionary mapping cluster IDs to dictionaries containing gov docs and news articles
        """
        n_news = gov_news_sim.shape[1]
        final_clusters = {}
        
        # Initialize final clusters with government documents
        for cluster_id, gov_indices in gov_clusters.items():
            final_clusters[cluster_id] = {
                'gov_indices': gov_indices,
                'news_indices': [],
                'news_similarities': {}
            }
        
        # For each news article, find the most similar government document clusters
        for news_idx in range(n_news):
            # Store all cluster similarities for this news article
            cluster_similarities = []
            
            for cluster_id, cluster_data in final_clusters.items():
                # Calculate average similarity to all government documents in the cluster
                cluster_sim = 0
                for gov_idx in cluster_data['gov_indices']:
                    cluster_sim += gov_news_sim[gov_idx, news_idx]
                
                avg_sim = cluster_sim / len(cluster_data['gov_indices'])
                
                if avg_sim >= self.similarity_threshold:
                    cluster_similarities.append((cluster_id, avg_sim))
            
            # Try to assign to clusters that are not full
            for cluster_id, sim in cluster_similarities:
                if len(final_clusters[cluster_id]['news_indices']) < 10:
                    final_clusters[cluster_id]['news_indices'].append(news_idx)
                    final_clusters[cluster_id]['news_similarities'][news_idx] = sim
        
        # Count assigned news articles
        total_assigned = sum(len(cluster['news_indices']) for cluster in final_clusters.values())
        print(f"Assigned {total_assigned}/{n_news} news articles to clusters")
        
        return final_clusters
    
    def organize_clusters(self, final_clusters, gov_df, news_df, gov_embeddings):
        """
        Organize clusters for output.
        
        Parameters:
        - final_clusters: Dictionary mapping cluster IDs to dictionaries containing gov docs and news articles
        - gov_df: DataFrame containing government documents
        - news_df: DataFrame containing news articles
        
        Returns:
        - organized_clusters: List of organized cluster dictionaries
        """
        organized_clusters = []
        
        for cluster_id, cluster_data in final_clusters.items():

            cluster_gov_embeddings = gov_embeddings[cluster_data['gov_indices']]
            center_embedding = np.mean(cluster_gov_embeddings, axis=0)

            # Create cluster
            cluster = {
                'id': cluster_id,
                'documents': [],
                'gov_count': len(cluster_data['gov_indices']),
                'news_count': len(cluster_data['news_indices']),
                'total_count': len(cluster_data['gov_indices']) + len(cluster_data['news_indices']),
                'center_embedding': center_embedding.tolist() 
            }
            
            # Add government documents
            for gov_idx in cluster_data['gov_indices']:
                cluster['documents'].append({
                    'source': 'gov',
                    'type': 'primary',
                    'title': gov_df.iloc[gov_idx]['title'],
                    'text': gov_df.iloc[gov_idx].get('full_text', ''),
                    'url': gov_df.iloc[gov_idx].get('url', ''),
                    'keyword': gov_df.iloc[gov_idx].get('keyword', '')
                })
            
            # Add news articles, sorted by similarity
            if cluster_data['news_indices']:
                sorted_news = sorted(
                    [(idx, cluster_data['news_similarities'].get(idx, 0)) 
                     for idx in cluster_data['news_indices']],
                    key=lambda x: x[1],
                    reverse=True
                )
                
                for news_idx, sim in sorted_news[:3]:
                    cluster['documents'].append({
                        'source': news_df.iloc[news_idx].get('source', 'news'),
                        'type': 'news',
                        'title': news_df.iloc[news_idx]['title'],
                        'text': news_df.iloc[news_idx].get('full_text', ''),
                        'url': news_df.iloc[news_idx].get('url', ''),
                        'keyword': news_df.iloc[news_idx].get('keyword', ''),
                        'similarity': float(f"{sim:.4f}")
                    })
            
            organized_clusters.append(cluster)
        
        # Sort clusters by total document count (descending)
        organized_clusters.sort(key=lambda x: x['total_count'], reverse=True)
        
        return organized_clusters
    
    def filter_irrelevant_clusters(self, organized_clusters):
        """
        Filter out irrelevant clusters based on the criteria:
        - Drop clusters with exactly 1 government document AND less than 2 news articles with similarity > 0.4
        
        Parameters:
        - organized_clusters: List of organized cluster dictionaries
        
        Returns:
        - filtered_clusters: List of relevant clusters only
        """
        filtered_clusters = []
        dropped_count = 0
        
        for cluster in organized_clusters:
            # Check if cluster has exactly 1 government document
            if cluster['gov_count'] == 1:
                # Count news articles with similarity > 0.3
                high_similarity_news_count = 0
                for doc in cluster['documents']:
                    if (doc.get('source') == 'news' and 
                        'similarity' in doc and 
                        doc['similarity'] > 0.3):
                        high_similarity_news_count += 1
                
                # Drop cluster if it has < 1 news articles with similarity > 0.3
                if high_similarity_news_count < 1:
                    dropped_count += 1
                    print(f"Dropping cluster {cluster['id']}: 1 gov doc, {high_similarity_news_count} news articles with similarity > 0.3")
                    continue
            
            # Keep the cluster if it doesn't meet the drop criteria
            filtered_clusters.append(cluster)
        
        print(f"Filtered out {dropped_count} irrelevant clusters. Remaining: {len(filtered_clusters)}")
        return filtered_clusters
           
    def find_bills_for_gov_articles(self, filtered_clusters):
        """
        Find related bills for the top 3 government articles using congress scraper.
        
        Parameters:
        - filtered_clusters: List of filtered cluster dictionaries
        
        Returns:
        - bills_data: Dictionary mapping gov article titles to found bills
        """
        try:
            # Check if Congress API key is available
            import os
            congress_api_key = os.environ.get('CONGRESS_API_KEY')
            if not congress_api_key:
                print("CONGRESS_API_KEY not set - skipping bill search")
                print("To enable bill search, set the CONGRESS_API_KEY environment variable")
                return {}
            
            # Import congress scraper functionality
            from logic.congress_scraper import Congress
            
            # Get top 3 government articles from all clusters
            all_gov_articles = []
            for cluster in filtered_clusters:
                for doc in cluster['documents']:
                    if doc.get('source') == 'gov' and doc.get('type') == 'primary':
                        all_gov_articles.append(doc)
            
            # Sort by cluster score and take top 3
            top_gov_articles = all_gov_articles[:3]
            
            if not top_gov_articles:
                print("No government articles found for bill search")
                return {}
            
            print(f"========== FINDING BILLS FOR TOP {len(top_gov_articles)} GOV ARTICLES ===========")
            
            bills_data = {}
            
            for i, gov_article in enumerate(top_gov_articles):
                title = gov_article.get('title', '')
                
                # Extract key terms from title for better search results
                # Remove common words and use key terms
                search_terms = self.extract_search_terms_from_title(title)
                
                print(f"Searching for bills related to: '{title}'")
                print(f"Using search terms: {search_terms}")
                
                try:
                    # Create Congress instance with extracted search terms
                    congress = Congress(search_terms)
                    bills = congress.get_bills()
                    
                    if bills and len(bills) > 0:
                        print(f"Found {len(bills)} bills for '{title}'")
                        bills_data[title] = bills
                        
                        # Print first few bills for debugging
                        for j, bill in enumerate(bills[:3]):
                            print(f"  Bill {j+1}: {bill.get('title', 'No title')}")
                            print(f"    Bill ID: {bill.get('bill_id', 'No ID')}")
                    else:
                        print(f"No bills found for '{title}'")
                        bills_data[title] = []
                        
                except Exception as e:
                    print(f"Error finding bills for '{title}': {e}")
                    bills_data[title] = []
            
            return bills_data
            
        except ImportError as e:
            print(f"Could not import congress scraper: {e}")
            return {}
        except Exception as e:
            print(f"Error in find_bills_for_gov_articles: {e}")
            return {}

    def extract_search_terms_from_title(self, title):
        """
        Extract meaningful search terms from government document titles.
        
        Parameters:
        - title: Government document title
        
        Returns:
        - search_terms: List of key terms for searching
        """
        # Common words to filter out
        stop_words = {
            'act', 'of', 'the', 'and', 'for', 'in', 'to', 'a', 'an', 'is', 'are', 
            'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'do', 'does', 
            'did', 'will', 'would', 'could', 'should', 'may', 'might', 'must', 'can',
            'update', 'bill', 'legislation', '2024', '2023', '2025'
        }
        
        # Split title into words and filter
        words = title.lower().split()
        key_terms = []
        
        for word in words:
            # Remove punctuation and check if it's a meaningful term
            clean_word = ''.join(c for c in word if c.isalnum())
            if (len(clean_word) > 2 and 
                clean_word not in stop_words and 
                not clean_word.isdigit()):
                key_terms.append(clean_word)
        
        # Return top 3 most meaningful terms, or original title if no good terms found
        if key_terms:
            return key_terms[:3]
        else:
            return [title]

    def format_output(self, organized_clusters):
        """
        Format clusters for output with improved scoring algorithm.
        
        Parameters:
        - organized_clusters: List of organized cluster dictionaries
        
        Returns:
        - output: List of formatted cluster metadata
        """
        output = []
        
        # Calculate global statistics for normalization
        all_news_similarities = []
        max_news_count = 0
        max_gov_count = 0
        
        for cluster in organized_clusters:
            max_news_count = max(max_news_count, cluster['news_count'])
            max_gov_count = max(max_gov_count, cluster['gov_count'])
            
            # Collect all news similarities for normalization
            for doc in cluster['documents']:
                if doc.get('source') == 'news' and 'similarity' in doc:
                    all_news_similarities.append(doc['similarity'])
        
        for cluster in organized_clusters:
            # Create a dataframe from the documents
            df = pd.DataFrame(cluster['documents'])
            
            # Extract news articles for similarity analysis
            news_docs = [doc for doc in cluster['documents'] if doc.get('source') == 'news']
            
            # Calculate similarity-based metrics
            similarities = [doc.get('similarity', 0) for doc in news_docs if 'similarity' in doc]
            
            if similarities:
                avg_similarity = np.mean(similarities)
                high_similarity_count = sum(1 for sim in similarities if sim > 0.45)
                very_high_similarity_count = sum(1 for sim in similarities if sim > 0.55)
                
            else:
                avg_similarity = 0
                high_similarity_count = 0
                very_high_similarity_count = 0
            
            # Calculate diversity metrics
            unique_keywords = set()
            for doc in cluster['documents']:
                keyword = doc.get('keyword', '')
                if keyword:
                    unique_keywords.add(keyword.lower())
            
            # Calculate recency bonus (if articles have timestamps)
            
            # Simplified semantic scoring algorithm based on grounded principles
            
            # Core principle: Score should reflect semantic coherence and information value
            # Higher scores for clusters with:
            # 1. Multiple related documents (coherence through volume)
            # 2. Strong semantic connections (high similarity scores)
            # 3. Balanced gov/news representation (comprehensive coverage)
            
            # Base score starts from total document count (information volume)
            base_score = cluster['total_count']
            
            # Semantic quality multiplier based on news article similarities
            if similarities:
                # Reward high average similarity and consistency
                semantic_quality = avg_similarity * 2.0
                
                # Bonus for multiple high-quality connections
                if high_similarity_count >= 2:
                    semantic_quality += 0.5
                if very_high_similarity_count >= 1:
                    semantic_quality += 0.3
                    
                # Consistency bonus: reward clusters where most articles are well-connected
                consistency_ratio = high_similarity_count / len(similarities)
                if consistency_ratio > 0.5:  # More than half are high similarity
                    semantic_quality += 0.4
            else:
                # No news articles - minimal semantic quality
                semantic_quality = 0.2
            
            # Government authority factor (more gov docs = more authoritative)
            gov_factor = 1.0 + (cluster['gov_count'] - 1) * 0.3
            
            # Calculate final score
            final_score = base_score * semantic_quality * gov_factor
            
            # Apply penalties for edge cases
            if cluster['news_count'] == 0:
                final_score *= 0.4  # Penalty for no supporting news
            elif cluster['news_count'] == 1 and avg_similarity < 0.4:
                final_score *= 0.7  # Penalty for single weak connection
            
            key = hashlib.sha256("".join(sorted(doc['title'] for doc in cluster['documents'])).encode('utf-8')).hexdigest()[:60]
            
            # Create metadata
            metadata = {
                'center_embedding': cluster['center_embedding'],
                'score': round(float(final_score), 4),
                'articles': df.to_json(),
                'key': key
            }
            
            output.append(metadata)
        
        # Sort by score (descending) to prioritize highest-scoring clusters
        output.sort(key=lambda x: x['score'], reverse=True)
        
        return output

    def generate_cluster_report(self, output, output_path="tmp/clusters_report.txt"):
        """
        Generate a neat report showing all article titles organized by cluster,
        separating government documents from news articles.
        
        Parameters:
        - output: List of cluster metadata dictionaries containing score and articles JSON
        - output_path: Path for the output text file
        """
        report_lines = []
        report_lines.append("=" * 80 + "\n")
        report_lines.append("CLUSTER REPORT\n")
        report_lines.append("=" * 80 + "\n\n")
        
        total_gov_docs = 0
        total_news_articles = 0
        
        for i, cluster_metadata in enumerate(output):
            # Extract score and parse articles JSON
            score = cluster_metadata.get('score', 0)
            articles_json = cluster_metadata.get('articles', '[]')
            
            try:
                articles_df = pd.read_json(StringIO(articles_json))
                articles = articles_df.to_dict('records') if not articles_df.empty else []
            except Exception as e:
                print(f"Error parsing articles JSON for cluster {i+1}: {e}")
                articles = []
            
            # Count government and news documents
            gov_count = len([doc for doc in articles if doc.get('source') == 'gov'])
            news_count = len([doc for doc in articles if doc.get('source') == 'news'])
            
            total_gov_docs += gov_count
            total_news_articles += news_count
            
            # Cluster header
            report_lines.append(f"CLUSTER {i+1} - Score: {score:.4f}\n")
            report_lines.append(f"Government Documents: {gov_count} | News Articles: {news_count}\n")
            report_lines.append("-" * 80 + "\n")
            
            # Separate government and news documents
            gov_docs = []
            news_docs = []
            
            for doc in articles:
                if doc.get('source') == 'gov':
                    doc_type = doc.get('type', 'unknown')
                    title = doc.get('title', 'No title')
                    keyword = doc.get('keyword', '')
                    gov_docs.append((doc_type, title, keyword))
                elif doc.get('source') == 'news':
                    title = doc.get('title', 'No title')
                    similarity = doc.get('similarity', 'N/A')
                    keyword = doc.get('keyword', '')
                    news_docs.append((title, similarity, keyword))
            
            # Print government documents
            if gov_docs:
                report_lines.append("GOVERNMENT DOCUMENTS:\n")
                for j, (doc_type, title, keyword) in enumerate(gov_docs, 1):
                    type_label = f"[{doc_type.upper()}]" if doc_type else "[GOV]"
                    keyword_text = f" (Keyword: {keyword})" if keyword else ""
                    report_lines.append(f"  {j}. {type_label} {title}{keyword_text}\n")
                report_lines.append("\n")
            
            # Print news articles
            if news_docs:
                report_lines.append("NEWS ARTICLES:\n")
                for j, (title, similarity, keyword) in enumerate(news_docs, 1):
                    sim_text = f" (Similarity: {similarity})" if similarity != 'N/A' else ""
                    keyword_text = f" (Keyword: {keyword})" if keyword else ""
                    report_lines.append(f"  {j}. {title}{sim_text}{keyword_text}\n")
                report_lines.append("\n")
            
            report_lines.append("=" * 80 + "\n\n")
    
        # Summary
        report_lines.append("SUMMARY:\n")
        report_lines.append(f"Total Clusters: {len(output)}\n")
        report_lines.append(f"Total Government Documents: {total_gov_docs}\n")
        report_lines.append(f"Total News Articles: {total_news_articles}\n")
        report_lines.append(f"Total Documents: {total_gov_docs + total_news_articles}\n")
        
        # Write to file
        try:
            # Create directory if it doesn't exist
            os.makedirs(os.path.dirname(output_path), exist_ok=True)
            
            with open(output_path, "w", encoding='utf-8') as f:
                f.writelines(report_lines)
            print(f"Cluster titles report saved to {output_path}")
        except Exception as e:
            print(f"Error writing cluster titles report: {e}")
            # Print to console if file writing fails
            print("".join(report_lines))
        
    def cluster_articles(self, news_df, gov_df):
        """
        Main method to cluster articles and generate report.
        
        Parameters:
        - news_df: DataFrame containing news articles
        - gov_df: DataFrame containing government documents
        
        Returns:
        - output: List of cluster metadata
        """
        try:
            if gov_df.empty:
                print("No government documents to anchor clusters")
                return []
            
            if news_df.empty:
                print("No news articles to cluster")
                # Still create clusters with just government documents
                news_df = pd.DataFrame(columns=['title', 'full_text', 'url', 'keyword'])
            
            # Step 1: Prepare documents
            gov_texts, news_texts = self.prepare_documents(gov_df, news_df)
            
            # Step 2: Generate embeddings
            gov_embeddings, news_embeddings = self.generate_embeddings(gov_texts, news_texts)
            
            # Step 3: Calculate similarities
            gov_news_sim, gov_gov_sim = self.calculate_similarities(gov_embeddings, news_embeddings)
            
            # Step 4: Merge similar government documents
            gov_clusters = self.merge_similar_gov_docs(gov_gov_sim)
            
            # Step 5: Assign news articles to government document clusters
            final_clusters = self.assign_news_to_gov_clusters(gov_clusters, gov_news_sim)
            
            # Step 6: Organize clusters
            organized_clusters = self.organize_clusters(final_clusters, gov_df, news_df, gov_embeddings)
            
            # Step 7: Filter out irrelevant clusters
            filtered_clusters = self.filter_irrelevant_clusters(organized_clusters)
            
            # Step 8: Find bills for top government articles
            bills_data = self.find_bills_for_gov_articles(filtered_clusters)
            
            # Step 9: Format output
            output = self.format_output(filtered_clusters)
            
            # Print summary
            print("========== CLUSTERING SUMMARY ===========")
            print(f"Total government documents: {len(gov_df)}")
            print(f"Total news articles: {len(news_df)}")
            print(f"Total clusters before filtering: {len(organized_clusters)}")
            print(f"Total clusters after filtering: {len(filtered_clusters)}")
            
            gov_in_clusters = sum(cluster['gov_count'] for cluster in filtered_clusters)
            news_in_clusters = sum(cluster['news_count'] for cluster in filtered_clusters)
            
            print(f"Government documents in final clusters: {gov_in_clusters}/{len(gov_df)} ({gov_in_clusters/max(1, len(gov_df))*100:.1f}%)")
            print(f"News articles in final clusters: {news_in_clusters}/{len(news_df)} ({news_in_clusters/max(1, len(news_df))*100:.1f}%)")
            
            print("========== OUTPUT CLUSTERS ===========")
            for i, cluster in enumerate(filtered_clusters[:10]):  # Show top 10 clusters
                print(f"----- Cluster {i+1} -------")
                print(f"Documents: {cluster['total_count']} ({cluster['gov_count']} gov, {cluster['news_count']} news)")
                for j, doc in enumerate(cluster['documents'][:5]):  # Show top 5 documents
                    print(f"{doc['source']}: {doc['title']}")
                    if j >= 4:
                        break
                print()
            
            # Generate cluster titles report (using filtered clusters)
            # self.generate_cluster_report(output)
            
            return output
            
        except Exception as e:
            logging.error(f"Error in clustering: {e}")
            return []


def load_df(type):
    # Initialize S3 client
    s3 = boto3.client('s3')
    astra_bucket_name = os.getenv("ASTRA_BUCKET_NAME")
    try:
        # Load pickled DataFrame from S3
        key = f"{type}/articles.pkl"
        response = s3.get_object(Bucket=astra_bucket_name, Key=key)
        df = pickle.loads(response['Body'].read())
        print(f"Loaded DataFrame with {len(df)} articles")

        return df
    except Exception as e:
        print(f"Error processing file: {str(e)}")
        raise e  
    
# AWS Lambda handler function
def handler(payload):
    """AWS Lambda handler for article clustering"""
    logging.info("Starting simple semantic clustering Lambda function")
    
    news_df = load_df('gnews')
    gov_df = load_df('gov')
    
    clusterer = Clusterer()
    clusters = clusterer.cluster_articles(news_df, gov_df)
    
    if clusters and len(clusters) > 0:

        # save to db
        conn = psycopg2.connect(dsn=db_access_url, client_encoding='utf8')

        try:
            # Delete all existing entries first
            cursor = conn.cursor()
            cursor.execute("DELETE FROM articles")
            conn.commit()  
            print("Deleted all existing articles from db")

            cluster_ids = []
            for i, metadata in enumerate(clusters):

                # Save the email description to S3
                s3.save_metadata(metadata['articles'], metadata['key'])

                vector_str = f"[{','.join(map(str, metadata['center_embedding']))}]"

                cursor = conn.cursor()
                cursor.execute("""
                    INSERT INTO articles (embedding, score, key, date)
                    VALUES (%s, %s, %s, %s)
                    RETURNING id
                """, (vector_str, metadata['score'], metadata['key'], datetime.now()))

                returned_id = cursor.fetchone()[0]
                cluster_ids.append(returned_id)

            conn.commit()
            print(f"Inserted {len(clusters)} clusters into db")

            chunk_size = 10
            sqs = boto3.client('sqs')
            CONTENT_QUEUE_URL = os.getenv("CONTENT_QUEUE_URL")

            for i in range(0, len(cluster_ids), chunk_size):
                chunk = cluster_ids[i:i+chunk_size]
                next_event = {
                    "action": "e_publish",
                    "payload": {
                        "clusters": chunk
                    }
                }
                response = sqs.send_message(
                    QueueUrl=CONTENT_QUEUE_URL,
                    MessageBody=json.dumps(next_event)
                )
                print(f"Sent publishing request to Astra SQS for clusters {chunk}: {response.get('MessageId')}")

        except Exception as e:
            print(f"Error inserting clusters into db: {e}")
            conn.rollback()
        finally:
            conn.close()
    else:
        logging.error("No clusters generated")



if __name__ == "__main__":

    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )

    with open('tmp/gov.pkl', 'rb') as f:
        gov_df = pickle.loads(f.read())
    with open('tmp/gnews.pkl', 'rb') as f:
        news_df = pickle.loads(f.read())

    clusterer = Clusterer()
    clusters = clusterer.cluster_articles(news_df, gov_df)
    
    if clusters and len(clusters) > 0:
        with open('tmp/example_cluster.pkl', 'wb') as f:
            pickle.dump(clusters[0]['articles'], f)
        print(f"Example cluster saved to tmp/example_cluster.pkl")

    if clusters and len(clusters) > 0:

        # save to db
        conn = psycopg2.connect(dsn=db_access_url, client_encoding='utf8')

        try:
            # Delete all existing entries first
            cursor = conn.cursor()
            cursor.execute("DELETE FROM articles")
            conn.commit()  
            print("Deleted all existing articles from db")

            cluster_ids = []
            for i, metadata in enumerate(clusters):

                cursor = conn.cursor()
                cursor.execute("""
                    INSERT INTO articles (embedding, score, key, date)
                    VALUES (%s, %s, %s, %s)
                    RETURNING id
                """, (metadata['center_embedding'], metadata['score'], metadata['key'], datetime.now()))

                returned_id = cursor.fetchone()[0]
                cluster_ids.append(returned_id)


            conn.commit()
            print(f"Inserted {len(clusters)} clusters into db")

            print(f"Cluster ID: {cluster_ids[0]}")

        except Exception as e:
            print(f"Error inserting clusters into db: {e}")
            conn.rollback()
        finally:
            conn.close()
    else:
        logging.error("No clusters generated")
